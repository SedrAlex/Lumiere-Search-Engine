{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Optimized ANTIQUE Dataset Processing and Embedding Generation\n",
        "\n",
        "This notebook implements optimized processing for higher MAP scores:\n",
        "1. **Better Model Selection**: Uses retrieval-optimized models\n",
        "2. **Improved Text Processing**: Preserves semantic information\n",
        "3. **Enhanced Embedding Strategy**: Query-document optimization\n",
        "4. **Memory \u0026 Speed Optimization**: Efficient batch processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "## Step 1: Install Optimized Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install compatible packages for Colab\n",
        "!pip install --upgrade pip\n",
        "!pip install sentence-transformers>=2.2.2\n",
        "!pip install transformers>=4.21.0\n",
        "!pip install torch>=1.13.0\n",
        "!pip install pandas numpy scikit-learn joblib nltk tqdm faiss-cpu beir datasets ir_datasets\n",
        "!pip install huggingface_hub>=0.10.0\n",
        "\n",
        "# Restart runtime after package installation\n",
        "print(\"[INFO] Packages installed! Please restart runtime and run the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_after_restart"
      },
      "source": [
        "## Step 1.5: Import Packages (Run After Restart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import ir_datasets\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import joblib\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import zipfile\n",
        "import tarfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_data"
      },
      "source": [
        "## Step 2: Download and Extract ANTIQUE Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload"
      },
      "outputs": [],
      "source": [
        "print(\"Downloading ANTIQUE dataset directly...\")\n",
        "\n",
        "# Download the ANTIQUE dataset\n",
        "dataset = ir_datasets.load('antique/train')\n",
        "\n",
        "# Create directory\n",
        "os.makedirs('antique_dataset', exist_ok=True)\n",
        "\n",
        "# Save documents\n",
        "print(\"Saving documents...\")\n",
        "docs_data = [{'doc_id': doc.doc_id, 'text': getattr(doc, 'text', '')} for doc in tqdm(dataset.docs_iter(), desc=\"Loading documents\")]\n",
        "docs_df = pd.DataFrame(docs_data)\n",
        "docs_df.to_csv('antique_dataset/documents.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Save queries\n",
        "print(\"Saving queries...\")\n",
        "queries_data = [{'query_id': query.query_id, 'text': query.text} for query in tqdm(dataset.queries_iter(), desc=\"Loading queries\")]\n",
        "queries_df = pd.DataFrame(queries_data)\n",
        "queries_df.to_csv('antique_dataset/queries.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Save qrels\n",
        "print(\"Saving relevance judgments...\")\n",
        "qrels_data = [{'query_id': qrel.query_id, 'doc_id': qrel.doc_id, 'relevance': qrel.relevance} for qrel in tqdm(dataset.qrels_iter(), desc=\"Loading qrels\")]\n",
        "qrels_df = pd.DataFrame(qrels_data)\n",
        "qrels_df.to_csv('antique_dataset/qrels.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"âœ… Downloaded ANTIQUE dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smart_preprocessing"
      },
      "source": [
        "## Step 3: Smart Text Preprocessing (Preserves Semantics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words = stop_words - {'not', 'no', 'nor', 'against', 'up', 'down', 'over', 'under', 'more', 'most', 'very'}\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def smart_clean_text(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.\u0026+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n",
        "    text = re.sub(r'\u003c.*?\u003e', ' ', text)\n",
        "    text = re.sub(r'\\b\\d{4}\\b', ' YEAR ', text)\n",
        "    text = re.sub(r'\\b\\d+\\.\\d+\\b', ' DECIMAL ', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' NUMBER ', text)\n",
        "    text = re.sub(r'[!]{2,}', ' EMPHASIS ', text)\n",
        "    text = re.sub(r'[?]{2,}', ' QUESTION ', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) >= 2 and token not in stop_words and not token.isdigit() and token.isalpha()]\n",
        "    return ' '.join(processed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multi_model_embeddings"
      },
      "source": [
        "## Step 4: Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "embeddings_generation"
      },
      "outputs": [],
      "source": [
        "print(f\"Loading model: multi-qa-MiniLM-L6-cos-v1\")\n",
        "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device)\n",
        "print(f\"Model loaded successfully on {device}\")\n",
        "\n",
        "# Prepare texts for embedding\n",
        "print(\"\\nPreparing texts for embedding...\")\n",
        "doc_texts = docs_df['text'].apply(smart_clean_text).tolist()\n",
        "doc_ids = docs_df['doc_id'].tolist()\n",
        "query_texts = queries_df['text'].apply(smart_clean_text).tolist()\n",
        "query_ids = queries_df['query_id'].tolist()\n",
        "\n",
        "def generate_embeddings_optimized(texts, batch_size=64):\n",
        "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return embeddings\n",
        "\n",
        "doc_embeddings = generate_embeddings_optimized(doc_texts)\n",
        "query_embeddings = generate_embeddings_optimized(query_texts)\n",
        "\n",
        "print(f\"\\nEmbedding generation completed!\")\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "retrieval_evaluation"
      },
      "source": [
        "## Step 5: Retrieval Evaluation \u0026 MAP Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "index.add(doc_embeddings.astype(np.float32))\n",
        "\n",
        "qrels_dict = defaultdict(dict)\n",
        "for _, row in qrels_df.iterrows():\n",
        "    qid = str(row['query_id'])\n",
        "    did = str(row['doc_id'])\n",
        "    rel = int(row['relevance'])\n",
        "    qrels_dict[qid][did] = rel\n",
        "\n",
        "average_precisions = []\n",
        "for i, query_emb in enumerate(query_embeddings):\n",
        "    query_id = str(query_ids[i])\n",
        "    scores, indices = index.search(query_emb.reshape(1, -1).astype(np.float32), 100)\n",
        "    relevant_found = 0\n",
        "    precision_sum = 0\n",
        "    for rank, doc_idx in enumerate(indices[0]):\n",
        "        doc_id = str(doc_ids[doc_idx])\n",
        "        is_relevant = qrels_dict[query_id].get(doc_id, 0) > 0\n",
        "        if is_relevant:\n",
        "            relevant_found += 1\n",
        "            precision_sum += relevant_found / (rank + 1)\n",
        "    avg_precision = precision_sum / relevant_found if relevant_found > 0 else 0.0\n",
        "    average_precisions.append(avg_precision)\n",
        "map_score = np.mean(average_precisions)\n",
        "print(f\"MAP Score: {map_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
