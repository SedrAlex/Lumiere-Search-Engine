{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Quora Dataset Processing and Embedding Generation\n",
        "\n",
        "This notebook processes the Quora dataset by:\n",
        "1. Uploading and extracting the dataset files\n",
        "2. Applying text cleaning methods\n",
        "3. Generating embeddings using paraphrase-mpnet-base-v2\n",
        "4. Saving all processed files and models\n",
        "\n",
        "**Steps Overview:**\n",
        "- Step 1: Install required packages\n",
        "- Step 2: Upload Quora dataset from Downloads folder\n",
        "- Step 3: Extract and save docs, queries, qrels as separate files\n",
        "- Step 4: Apply text cleaning methods\n",
        "- Step 5: Generate embeddings using paraphrase-mpnet-base-v2\n",
        "- Step 6: Save all files and models using joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "## Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install joblib\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import joblib\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import tarfile\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_data"
      },
      "source": [
        "## Step 2: Upload Quora Dataset\n",
        "\n",
        "Upload your Quora dataset file from your Downloads folder. The system will automatically detect the file format and extract accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload"
      },
      "outputs": [],
      "source": [
        "print(\"Please upload your Quora dataset file from your Downloads folder:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "uploaded_file = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {uploaded_file}\")\n",
        "\n",
        "# Extract the file if it's compressed\n",
        "if uploaded_file.endswith('.zip'):\n",
        "    with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall('quora_dataset')\n",
        "    print(\"Zip file extracted successfully!\")\n",
        "elif uploaded_file.endswith('.tar.gz') or uploaded_file.endswith('.tgz'):\n",
        "    with tarfile.open(uploaded_file, 'r:gz') as tar_ref:\n",
        "        tar_ref.extractall('quora_dataset')\n",
        "    print(\"Tar.gz file extracted successfully!\")\n",
        "else:\n",
        "    # Move the file to quora_dataset directory\n",
        "    os.makedirs('quora_dataset', exist_ok=True)\n",
        "    os.rename(uploaded_file, f'quora_dataset/{uploaded_file}')\n",
        "    print(\"File moved to quora_dataset directory!\")\n",
        "\n",
        "# List contents of the extracted directory\n",
        "print(\"\\nContents of quora_dataset directory:\")\n",
        "for root, dirs, files in os.walk('quora_dataset'):\n",
        "    level = root.replace('quora_dataset', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        print(f\"{subindent}{file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extract_files"
      },
      "source": [
        "## Step 3: Extract and Save Docs, Queries, and Qrels as Separate Files\n",
        "\n",
        "This step identifies and loads the three main components of the Quora dataset:\n",
        "- **docs**: The document collection\n",
        "- **queries**: The search queries\n",
        "- **qrels**: The relevance judgments (query-document pairs with relevance scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract"
      },
      "outputs": [],
      "source": [
        "# Function to find files by pattern\n",
        "def find_files_by_pattern(directory, patterns):\n",
        "    found_files = {}\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            for pattern_name, pattern in patterns.items():\n",
        "                if any(p in file.lower() for p in pattern):\n",
        "                    found_files[pattern_name] = file_path\n",
        "                    break\n",
        "    return found_files\n",
        "\n",
        "# Define patterns to search for different file types\n",
        "file_patterns = {\n",
        "    'docs': ['corpus', 'documents', 'docs', 'collection'],\n",
        "    'queries': ['queries', 'query', 'topics'],\n",
        "    'qrels': ['qrels', 'relevance', 'judgments', 'rel']\n",
        "}\n",
        "\n",
        "# Find the files\n",
        "found_files = find_files_by_pattern('quora_dataset', file_patterns)\n",
        "print(\"Found files:\")\n",
        "for file_type, file_path in found_files.items():\n",
        "    print(f\"{file_type}: {file_path}\")\n",
        "\n",
        "# Load the files\n",
        "def load_file(file_path):\n",
        "    \"\"\"Load file based on extension\"\"\"\n",
        "    if file_path.endswith('.tsv'):\n",
        "        return pd.read_csv(file_path, sep='\\t')\n",
        "    elif file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.json') or file_path.endswith('.jsonl'):\n",
        "        return pd.read_json(file_path, lines=True)\n",
        "    else:\n",
        "        # Try to read as tab-separated by default\n",
        "        try:\n",
        "            return pd.read_csv(file_path, sep='\\t')\n",
        "        except:\n",
        "            return pd.read_csv(file_path)\n",
        "\n",
        "# Load each file type\n",
        "datasets = {}\n",
        "for file_type, file_path in found_files.items():\n",
        "    print(f\"\\nLoading {file_type} from {file_path}...\")\n",
        "    datasets[file_type] = load_file(file_path)\n",
        "    print(f\"Shape: {datasets[file_type].shape}\")\n",
        "    print(f\"Columns: {list(datasets[file_type].columns)}\")\n",
        "    print(f\"First few rows:\")\n",
        "    print(datasets[file_type].head())\n",
        "\n",
        "# Save as separate TSV files with quora names\n",
        "print(\"\\nSaving files with Quora naming convention...\")\n",
        "datasets['docs'].to_csv('quora_docs.tsv', sep='\\t', index=False)\n",
        "datasets['queries'].to_csv('quora_queries.tsv', sep='\\t', index=False)\n",
        "datasets['qrels'].to_csv('quora_qrels.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"Files saved successfully:\")\n",
        "print(\"- quora_docs.tsv\")\n",
        "print(\"- quora_queries.tsv\")\n",
        "print(\"- quora_qrels.tsv\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n=== DATASET SUMMARY ===\")\n",
        "print(f\"Documents: {len(datasets['docs'])} entries\")\n",
        "print(f\"Queries: {len(datasets['queries'])} entries\")\n",
        "print(f\"Qrels: {len(datasets['qrels'])} entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "## Step 4: Text Cleaning Methods\n",
        "\n",
        "This step applies comprehensive text cleaning to both documents and queries:\n",
        "- Convert to lowercase\n",
        "- Remove special characters and numbers\n",
        "- Remove extra whitespace\n",
        "- Remove stopwords\n",
        "- Apply lemmatization\n",
        "- Remove very short texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleaning"
      },
      "outputs": [],
      "source": [
        "# Initialize cleaning tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text cleaning function\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to clean\n",
        "    \n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # Remove special characters and numbers, keep only alphabets and spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and apply lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
        "              if token not in stop_words and len(token) > 2]\n",
        "    \n",
        "    # Join tokens back\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    \n",
        "    return cleaned_text\n",
        "\n",
        "# Apply cleaning to documents\n",
        "print(\"Cleaning documents...\")\n",
        "docs_df = datasets['docs'].copy()\n",
        "\n",
        "# Identify text columns in docs (common column names)\n",
        "text_columns_docs = [col for col in docs_df.columns if any(keyword in col.lower() for keyword in ['text', 'content', 'body', 'document', 'passage'])]\n",
        "if not text_columns_docs:\n",
        "    # If no obvious text column, use the last column or column with longest text\n",
        "    text_columns_docs = [docs_df.columns[-1]]  # Assume last column is text\n",
        "\n",
        "print(f\"Document text columns identified: {text_columns_docs}\")\n",
        "\n",
        "# Apply cleaning to each text column\n",
        "for col in text_columns_docs:\n",
        "    print(f\"Cleaning column: {col}\")\n",
        "    tqdm.pandas(desc=f\"Cleaning {col}\")\n",
        "    docs_df[f'{col}_cleaned'] = docs_df[col].progress_apply(clean_text)\n",
        "\n",
        "# Apply cleaning to queries\n",
        "print(\"\\nCleaning queries...\")\n",
        "queries_df = datasets['queries'].copy()\n",
        "\n",
        "# Identify text columns in queries\n",
        "text_columns_queries = [col for col in queries_df.columns if any(keyword in col.lower() for keyword in ['text', 'query', 'question', 'title'])]\n",
        "if not text_columns_queries:\n",
        "    # If no obvious text column, use the last column\n",
        "    text_columns_queries = [queries_df.columns[-1]]  # Assume last column is text\n",
        "\n",
        "print(f\"Query text columns identified: {text_columns_queries}\")\n",
        "\n",
        "# Apply cleaning to each text column\n",
        "for col in text_columns_queries:\n",
        "    print(f\"Cleaning column: {col}\")\n",
        "    tqdm.pandas(desc=f\"Cleaning {col}\")\n",
        "    queries_df[f'{col}_cleaned'] = queries_df[col].progress_apply(clean_text)\n",
        "\n",
        "# Remove entries with very short cleaned text (less than 3 words)\n",
        "print(\"\\nFiltering out very short texts...\")\n",
        "original_docs_count = len(docs_df)\n",
        "original_queries_count = len(queries_df)\n",
        "\n",
        "# Filter docs\n",
        "for col in text_columns_docs:\n",
        "    docs_df = docs_df[docs_df[f'{col}_cleaned'].str.split().str.len() >= 3]\n",
        "\n",
        "# Filter queries\n",
        "for col in text_columns_queries:\n",
        "    queries_df = queries_df[queries_df[f'{col}_cleaned'].str.split().str.len() >= 3]\n",
        "\n",
        "print(f\"Documents: {original_docs_count} -> {len(docs_df)} (removed {original_docs_count - len(docs_df)} short texts)\")\n",
        "print(f\"Queries: {original_queries_count} -> {len(queries_df)} (removed {original_queries_count - len(queries_df)} short texts)\")\n",
        "\n",
        "# Save cleaned datasets\n",
        "print(\"\\nSaving cleaned datasets...\")\n",
        "docs_df.to_csv('quora_docs_cleaned.tsv', sep='\\t', index=False)\n",
        "queries_df.to_csv('quora_queries_cleaned.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"Cleaned files saved:\")\n",
        "print(\"- quora_docs_cleaned.tsv\")\n",
        "print(\"- quora_queries_cleaned.tsv\")\n",
        "\n",
        "# Display cleaning examples\n",
        "print(\"\\n=== CLEANING EXAMPLES ===\")\n",
        "print(\"\\nDocument cleaning examples:\")\n",
        "for col in text_columns_docs:\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    for i in range(min(3, len(docs_df))):\n",
        "        original = docs_df.iloc[i][col]\n",
        "        cleaned = docs_df.iloc[i][f'{col}_cleaned']\n",
        "        print(f\"Original: {original[:100]}...\")\n",
        "        print(f\"Cleaned:  {cleaned[:100]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nQuery cleaning examples:\")\n",
        "for col in text_columns_queries:\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    for i in range(min(3, len(queries_df))):\n",
        "        original = queries_df.iloc[i][col]\n",
        "        cleaned = queries_df.iloc[i][f'{col}_cleaned']\n",
        "        print(f\"Original: {original}\")\n",
        "        print(f\"Cleaned:  {cleaned}\")\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embeddings"
      },
      "source": [
        "## Step 5: Generate Embeddings using paraphrase-mpnet-base-v2\n",
        "\n",
        "This step:\n",
        "1. Loads the pre-trained paraphrase-mpnet-base-v2 model\n",
        "2. Generates embeddings for cleaned documents and queries\n",
        "3. Saves the embeddings and model for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "embeddings_generation"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model\n",
        "print(\"Loading paraphrase-mpnet-base-v2 model...\")\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Prepare texts for embedding\n",
        "print(\"\\nPreparing texts for embedding...\")\n",
        "\n",
        "# For documents, combine all cleaned text columns\n",
        "doc_texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for idx, row in docs_df.iterrows():\n",
        "    combined_text = ' '.join([str(row[f'{col}_cleaned']) for col in text_columns_docs])\n",
        "    doc_texts.append(combined_text)\n",
        "    # Use the first column as ID, or create an ID\n",
        "    doc_id = row[docs_df.columns[0]] if docs_df.columns[0] != text_columns_docs[0] else idx\n",
        "    doc_ids.append(doc_id)\n",
        "\n",
        "# For queries, combine all cleaned text columns\n",
        "query_texts = []\n",
        "query_ids = []\n",
        "\n",
        "for idx, row in queries_df.iterrows():\n",
        "    combined_text = ' '.join([str(row[f'{col}_cleaned']) for col in text_columns_queries])\n",
        "    query_texts.append(combined_text)\n",
        "    # Use the first column as ID, or create an ID\n",
        "    query_id = row[queries_df.columns[0]] if queries_df.columns[0] != text_columns_queries[0] else idx\n",
        "    query_ids.append(query_id)\n",
        "\n",
        "print(f\"Prepared {len(doc_texts)} documents and {len(query_texts)} queries for embedding\")\n",
        "\n",
        "# Generate document embeddings\n",
        "print(\"\\nGenerating document embeddings...\")\n",
        "doc_embeddings = model.encode(doc_texts, \n",
        "                             batch_size=32, \n",
        "                             show_progress_bar=True,\n",
        "                             convert_to_numpy=True)\n",
        "\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
        "\n",
        "# Generate query embeddings\n",
        "print(\"\\nGenerating query embeddings...\")\n",
        "query_embeddings = model.encode(query_texts, \n",
        "                               batch_size=32, \n",
        "                               show_progress_bar=True,\n",
        "                               convert_to_numpy=True)\n",
        "\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
        "\n",
        "# Create embedding dataframes with IDs\n",
        "doc_embeddings_df = pd.DataFrame({\n",
        "    'doc_id': doc_ids,\n",
        "    'text': doc_texts,\n",
        "    'embedding': [emb.tolist() for emb in doc_embeddings]\n",
        "})\n",
        "\n",
        "query_embeddings_df = pd.DataFrame({\n",
        "    'query_id': query_ids,\n",
        "    'text': query_texts,\n",
        "    'embedding': [emb.tolist() for emb in query_embeddings]\n",
        "})\n",
        "\n",
        "print(\"\\nEmbedding generation completed successfully!\")\n",
        "print(f\"Document embeddings: {doc_embeddings_df.shape}\")\n",
        "print(f\"Query embeddings: {query_embeddings_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_files"
      },
      "source": [
        "## Step 6: Save All Files and Models using Joblib\n",
        "\n",
        "This step saves:\n",
        "- The trained model\n",
        "- Document and query embeddings as matrices\n",
        "- All processed datasets\n",
        "- Metadata for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_models"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "print(\"Saving model and embeddings...\")\n",
        "\n",
        "# Save the SentenceTransformer model\n",
        "model.save('quora_paraphrase_mpnet_model')\n",
        "print(\"Model saved to: quora_paraphrase_mpnet_model/\")\n",
        "\n",
        "# Save embeddings as numpy arrays using joblib\n",
        "joblib.dump(doc_embeddings, 'quora_doc_embeddings_matrix.joblib')\n",
        "joblib.dump(query_embeddings, 'quora_query_embeddings_matrix.joblib')\n",
        "print(\"Embedding matrices saved:\")\n",
        "print(\"- quora_doc_embeddings_matrix.joblib\")\n",
        "print(\"- quora_query_embeddings_matrix.joblib\")\n",
        "\n",
        "# Save embedding dataframes\n",
        "joblib.dump(doc_embeddings_df, 'quora_doc_embeddings_df.joblib')\n",
        "joblib.dump(query_embeddings_df, 'quora_query_embeddings_df.joblib')\n",
        "print(\"Embedding dataframes saved:\")\n",
        "print(\"- quora_doc_embeddings_df.joblib\")\n",
        "print(\"- quora_query_embeddings_df.joblib\")\n",
        "\n",
        "# Save processed datasets\n",
        "joblib.dump(docs_df, 'quora_docs_processed.joblib')\n",
        "joblib.dump(queries_df, 'quora_queries_processed.joblib')\n",
        "joblib.dump(datasets['qrels'], 'quora_qrels.joblib')\n",
        "print(\"Processed datasets saved:\")\n",
        "print(\"- quora_docs_processed.joblib\")\n",
        "print(\"- quora_queries_processed.joblib\")\n",
        "print(\"- quora_qrels.joblib\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'model_name': 'paraphrase-mpnet-base-v2',\n",
        "    'embedding_dim': doc_embeddings.shape[1],\n",
        "    'num_documents': len(doc_texts),\n",
        "    'num_queries': len(query_texts),\n",
        "    'doc_text_columns': text_columns_docs,\n",
        "    'query_text_columns': text_columns_queries,\n",
        "    'doc_ids': doc_ids,\n",
        "    'query_ids': query_ids\n",
        "}\n",
        "\n",
        "joblib.dump(metadata, 'quora_metadata.joblib')\n",
        "print(\"Metadata saved: quora_metadata.joblib\")\n",
        "\n",
        "# Create a summary file\n",
        "summary = f\"\"\"\n",
        "=== QUORA DATASET PROCESSING SUMMARY ===\n",
        "\n",
        "Files Generated:\n",
        "1. quora_docs.tsv - Original documents\n",
        "2. quora_queries.tsv - Original queries\n",
        "3. quora_qrels.tsv - Relevance judgments\n",
        "4. quora_docs_cleaned.tsv - Cleaned documents\n",
        "5. quora_queries_cleaned.tsv - Cleaned queries\n",
        "6. quora_paraphrase_mpnet_model/ - Trained model directory\n",
        "7. quora_doc_embeddings_matrix.joblib - Document embeddings matrix\n",
        "8. quora_query_embeddings_matrix.joblib - Query embeddings matrix\n",
        "9. quora_doc_embeddings_df.joblib - Document embeddings with metadata\n",
        "10. quora_query_embeddings_df.joblib - Query embeddings with metadata\n",
        "11. quora_docs_processed.joblib - Processed documents dataframe\n",
        "12. quora_queries_processed.joblib - Processed queries dataframe\n",
        "13. quora_qrels.joblib - Relevance judgments dataframe\n",
        "14. quora_metadata.joblib - Processing metadata\n",
        "\n",
        "Dataset Statistics:\n",
        "- Documents: {len(doc_texts)}\n",
        "- Queries: {len(query_texts)}\n",
        "- Qrels: {len(datasets['qrels'])}\n",
        "- Embedding dimension: {doc_embeddings.shape[1]}\n",
        "- Model: paraphrase-mpnet-base-v2\n",
        "\n",
        "Text Columns Processed:\n",
        "- Document columns: {text_columns_docs}\n",
        "- Query columns: {text_columns_queries}\n",
        "\n",
        "Processing Steps Applied:\n",
        "1. Text cleaning (lowercase, remove special chars, stopwords, lemmatization)\n",
        "2. Filtering (removed texts with <3 words)\n",
        "3. Embedding generation using paraphrase-mpnet-base-v2\n",
        "4. File saving with joblib for efficient loading\n",
        "\n",
        "Next Steps:\n",
        "- Use the evaluation notebook to assess embedding quality\n",
        "- Target MAP score: >= 0.7\n",
        "- All files are ready for download and further processing\n",
        "\"\"\"\n",
        "\n",
        "with open('quora_processing_summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(summary)\n",
        "print(\"\\n=== PROCESSING COMPLETED SUCCESSFULLY ===\")\n",
        "print(\"All files have been saved and are ready for download!\")\n",
        "print(\"\\nTo download files, run the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_files"
      },
      "source": [
        "## Step 7: Download Generated Files\n",
        "\n",
        "Run this cell to download all generated files to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "# Create a zip file with all generated files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Creating zip file with all generated files...\")\n",
        "\n",
        "# List of files to include in the zip\n",
        "files_to_zip = [\n",
        "    'quora_docs.tsv',\n",
        "    'quora_queries.tsv',\n",
        "    'quora_qrels.tsv',\n",
        "    'quora_docs_cleaned.tsv',\n",
        "    'quora_queries_cleaned.tsv',\n",
        "    'quora_doc_embeddings_matrix.joblib',\n",
        "    'quora_query_embeddings_matrix.joblib',\n",
        "    'quora_doc_embeddings_df.joblib',\n",
        "    'quora_query_embeddings_df.joblib',\n",
        "    'quora_docs_processed.joblib',\n",
        "    'quora_queries_processed.joblib',\n",
        "    'quora_qrels.joblib',\n",
        "    'quora_metadata.joblib',\n",
        "    'quora_processing_summary.txt'\n",
        "]\n",
        "\n",
        "# Create zip file\n",
        "with zipfile.ZipFile('quora_processed_files.zip', 'w') as zipf:\n",
        "    for file in files_to_zip:\n",
        "        if os.path.exists(file):\n",
        "            zipf.write(file)\n",
        "            print(f\"Added {file} to zip\")\n",
        "    \n",
        "    # Add model directory\n",
        "    if os.path.exists('quora_paraphrase_mpnet_model'):\n",
        "        for root, dirs, files in os.walk('quora_paraphrase_mpnet_model'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '.')\n",
        "                zipf.write(file_path, arcname)\n",
        "        print(\"Added model directory to zip\")\n",
        "\n",
        "print(\"\\nZip file created: quora_processed_files.zip\")\n",
        "print(\"Downloading...\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download('quora_processed_files.zip')\n",
        "\n",
        "print(\"\\nDownload completed!\")\n",
        "print(\"\\nFiles included in the download:\")\n",
        "for file in files_to_zip:\n",
        "    if os.path.exists(file):\n",
        "        size = os.path.getsize(file) / (1024*1024)  # Size in MB\n",
        "        print(f\"- {file} ({size:.2f} MB)\")\n",
        "\n",
        "print(\"\\nModel directory: quora_paraphrase_mpnet_model/\")\n",
        "print(\"\\nAll files are now available in your Downloads folder!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
