{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# 🦜 BERTweet Embeddings for Antique Dataset\n",
    "\n",
    "This notebook generates document embeddings using the BERTweet model from Hugging Face for the cleaned Antique dataset.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload the `antique_cleaned_for_embeddings.json` file from your local text cleaning service\n",
    "2. The text has been pre-cleaned using the enhanced text cleaning service\n",
    "\n",
    "**Output:**\n",
    "- High-quality BERTweet embeddings for document representation\n",
    "- Embeddings saved in multiple formats for easy download and use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch numpy pandas scikit-learn tqdm\n",
    "!pip install -q datasets accelerate\n",
    "\n",
    "print(\"✅ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Upload and Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.py": {
       "data": "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"
      }
     },
     "output_id": "ad5f5de2-e9f9-4b89-8327-8da5b14b04ba"
    },
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "# Upload the cleaned dataset file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"📁 Please upload your 'antique_cleaned_for_embeddings.json' file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"✅ Uploaded file: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "print(\"📖 Loading cleaned dataset...\")\n",
    "\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"📊 Dataset Information:\")\n",
    "print(f\"   Dataset: {data.get('dataset', 'Unknown')}\")\n",
    "print(f\"   Total documents: {data.get('total_documents', 0):,}\")\n",
    "print(f\"   Export timestamp: {data.get('export_timestamp', 'Unknown')}\")\n",
    "print(f\"   Cleaning method: {data.get('cleaning_method', 'Unknown')}\")\n",
    "\n",
    "# Extract documents\n",
    "documents = data['documents']\n",
    "print(f\"\\n🔍 Sample document:\")\n",
    "print(f\"   ID: {documents[0]['id']}\")\n",
    "print(f\"   Cleaned text: {documents[0]['text'][:200]}...\")\n",
    "print(f\"   Original text: {documents[0]['original_text'][:200]}...\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(documents):,} documents successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Load BERTweet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load BERTweet model and tokenizer\n",
    "print(\"🤖 Loading BERTweet model...\")\n",
    "\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"   Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Load model\n",
    "print(\"   Loading model...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ BERTweet model loaded successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Embedding dimension: {model.config.hidden_size}\")\n",
    "print(f\"   Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Text Preprocessing for BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_for_bertweet(text, max_length=256):\n",
    "    \"\"\"\n",
    "    Preprocess text specifically for BERTweet model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning for BERTweet\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Truncate if too long (leave room for special tokens)\n",
    "    words = text.split()\n",
    "    if len(words) > max_length - 10:  # Reserve space for [CLS], [SEP], etc.\n",
    "        text = ' '.join(words[:max_length - 10])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = documents[0]['text']\n",
    "processed_text = preprocess_text_for_bertweet(sample_text)\n",
    "\n",
    "print(\"🔄 Text preprocessing example:\")\n",
    "print(f\"   Original: {sample_text[:150]}...\")\n",
    "print(f\"   Processed: {processed_text[:150]}...\")\n",
    "print(f\"   Length: {len(processed_text.split())} words\")\n",
    "\n",
    "print(\"✅ Text preprocessing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embeddings"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts, batch_size=16, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate BERTweet embeddings for a batch of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Batch size for processing\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Preprocess texts\n",
    "        processed_texts = [preprocess_text_for_bertweet(text, max_length) for text in batch_texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            processed_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use [CLS] token embedding (first token)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"✅ Embedding generation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_all"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all documents\n",
    "print(\"🚀 Starting embedding generation...\")\n",
    "print(f\"   Processing {len(documents):,} documents\")\n",
    "print(f\"   Using device: {device}\")\n",
    "\n",
    "# Extract texts\n",
    "texts = [doc['text'] for doc in documents]\n",
    "doc_ids = [doc['id'] for doc in documents]\n",
    "\n",
    "# Set batch size based on available memory\n",
    "batch_size = 32 if torch.cuda.is_available() else 8\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Generate embeddings\n",
    "start_time = datetime.now()\n",
    "embeddings = generate_embeddings_batch(texts, batch_size=batch_size)\n",
    "end_time = datetime.now()\n",
    "\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n✅ Embedding generation completed!\")\n",
    "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"   Average time per document: {processing_time/len(documents):.4f} seconds\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n📊 Embedding Statistics:\")\n",
    "print(f\"   Mean: {embeddings.mean():.6f}\")\n",
    "print(f\"   Std: {embeddings.std():.6f}\")\n",
    "print(f\"   Min: {embeddings.min():.6f}\")\n",
    "print(f\"   Max: {embeddings.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Quality Check and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_check"
   },
   "outputs": [],
   "source": [
    "# Quality check: Test similarity between related documents\n",
    "print(\"🔍 Quality Check: Testing embedding similarities...\")\n",
    "\n",
    "# Sample a few documents for similarity testing\n",
    "sample_indices = np.random.choice(len(embeddings), min(10, len(embeddings)), replace=False)\n",
    "sample_embeddings = embeddings[sample_indices]\n",
    "sample_texts = [texts[i] for i in sample_indices]\n",
    "sample_ids = [doc_ids[i] for i in sample_indices]\n",
    "\n",
    "# Calculate pairwise similarities\n",
    "similarities = cosine_similarity(sample_embeddings)\n",
    "\n",
    "print(f\"\\n📊 Similarity Matrix for {len(sample_indices)} sample documents:\")\n",
    "print(f\"   Average similarity: {similarities.mean():.4f}\")\n",
    "print(f\"   Std similarity: {similarities.std():.4f}\")\n",
    "\n",
    "# Find most similar pairs (excluding self-similarity)\n",
    "similarities_no_diag = similarities.copy()\n",
    "np.fill_diagonal(similarities_no_diag, -1)\n",
    "\n",
    "max_sim_idx = np.unravel_index(np.argmax(similarities_no_diag), similarities_no_diag.shape)\n",
    "max_similarity = similarities_no_diag[max_sim_idx]\n",
    "\n",
    "print(f\"\\n🔗 Most similar document pair:\")\n",
    "print(f\"   Similarity: {max_similarity:.4f}\")\n",
    "print(f\"   Doc 1 ({sample_ids[max_sim_idx[0]]}): {sample_texts[max_sim_idx[0]][:100]}...\")\n",
    "print(f\"   Doc 2 ({sample_ids[max_sim_idx[1]]}): {sample_texts[max_sim_idx[1]][:100]}...\")\n",
    "\n",
    "print(\"\\n✅ Quality check completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_embeddings"
   },
   "outputs": [],
   "source": [
    "# Prepare data for saving\n",
    "print(\"💾 Preparing data for saving...\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'embedding_dimension': embeddings.shape[1],\n",
    "    'total_documents': len(documents),\n",
    "    'generation_timestamp': datetime.now().isoformat(),\n",
    "    'processing_time_seconds': processing_time,\n",
    "    'device_used': str(device),\n",
    "    'batch_size': batch_size,\n",
    "    'max_sequence_length': 256,\n",
    "    'dataset_info': {\n",
    "        'dataset': data.get('dataset', 'antique'),\n",
    "        'cleaning_method': data.get('cleaning_method', 'enhanced'),\n",
    "        'export_timestamp': data.get('export_timestamp')\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"✅ Metadata prepared\")\n",
    "print(f\"   Model: {metadata['model_name']}\")\n",
    "print(f\"   Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "print(f\"   Total documents: {metadata['total_documents']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_numpy"
   },
   "outputs": [],
   "source": [
    "# Save as NumPy format\n",
    "print(\"💾 Saving embeddings in NumPy format...\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('antique_bertweet_embeddings.npy', embeddings)\n",
    "\n",
    "# Save document IDs\n",
    "np.save('antique_bertweet_doc_ids.npy', np.array(doc_ids))\n",
    "\n",
    "# Save metadata\n",
    "with open('antique_bertweet_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✅ NumPy format saved:\")\n",
    "print(\"   - antique_bertweet_embeddings.npy\")\n",
    "print(\"   - antique_bertweet_doc_ids.npy\")\n",
    "print(\"   - antique_bertweet_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_pickle"
   },
   "outputs": [],
   "source": [
    "# Save as Pickle format (includes everything in one file)\n",
    "print(\"💾 Saving complete dataset in Pickle format...\")\n",
    "\n",
    "complete_data = {\n",
    "    'embeddings': embeddings,\n",
    "    'doc_ids': doc_ids,\n",
    "    'texts': texts,\n",
    "    'metadata': metadata,\n",
    "    'original_documents': documents[:1000]  # Save first 1000 for reference\n",
    "}\n",
    "\n",
    "with open('antique_bertweet_complete.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_data, f)\n",
    "\n",
    "print(\"✅ Pickle format saved:\")\n",
    "print(\"   - antique_bertweet_complete.pkl\")\n",
    "print(\"   - Contains embeddings, IDs, texts, and metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_csv"
   },
   "outputs": [],
   "source": [
    "# Save as CSV format for easy inspection\n",
    "print(\"💾 Saving document mapping in CSV format...\")\n",
    "\n",
    "# Create DataFrame with document info\n",
    "df = pd.DataFrame({\n",
    "    'doc_id': doc_ids,\n",
    "    'text': texts,\n",
    "    'text_length': [len(text) for text in texts],\n",
    "    'word_count': [len(text.split()) for text in texts]\n",
    "})\n",
    "\n",
    "# Add embedding norms for quality check\n",
    "df['embedding_norm'] = np.linalg.norm(embeddings, axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('antique_bertweet_documents.csv', index=False)\n",
    "\n",
    "print(\"✅ CSV format saved:\")\n",
    "print(\"   - antique_bertweet_documents.csv\")\n",
    "print(f\"   - {len(df):,} document records\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\n📊 Document Statistics:\")\n",
    "print(f\"   Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"   Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Average embedding norm: {df['embedding_norm'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download all generated files\n",
    "print(\"📥 Downloading generated files...\")\n",
    "\n",
    "# List of files to download\n",
    "files_to_download = [\n",
    "    'antique_bertweet_embeddings.npy',\n",
    "    'antique_bertweet_doc_ids.npy',\n",
    "    'antique_bertweet_metadata.json',\n",
    "    'antique_bertweet_complete.pkl',\n",
    "    'antique_bertweet_documents.csv'\n",
    "]\n",
    "\n",
    "# Check file sizes\n",
    "print(\"📋 File Information:\")\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"   {filename}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"   {filename}: File not found\")\n",
    "\n",
    "print(\"\\n🚀 Ready to download! Run the next cell to download files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Download files one by one\n",
    "from google.colab import files\n",
    "\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"📥 Downloading {filename}...\")\n",
    "        files.download(filename)\n",
    "    else:\n",
    "        print(f\"❌ {filename} not found\")\n",
    "\n",
    "print(\"\\n✅ All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"🎉 BERTWEET EMBEDDING GENERATION COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"📊 Summary:\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Documents processed: {len(documents):,}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Total processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"   Average time per document: {processing_time/len(documents):.4f} seconds\")\n",
    "print(f\"   Device used: {device}\")\n",
    "print()\n",
    "print(f\"📁 Generated Files:\")\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"   ✅ {filename} ({size_mb:.2f} MB)\")\n",
    "print()\n",
    "print(f\"📋 Next Steps:\")\n",
    "print(f\"   1. Download all generated files to your local machine\")\n",
    "print(f\"   2. Load the embeddings in your search engine backend\")\n",
    "print(f\"   3. Use embeddings for document retrieval and similarity search\")\n",
    "print(f\"   4. Evaluate retrieval performance using your evaluation metrics\")\n",
    "print()\n",
    "print(f\"💡 Usage Examples:\")\n",
    "print(f\"   # Load embeddings in Python:\")\n",
    "print(f\"   embeddings = np.load('antique_bertweet_embeddings.npy')\")\n",
    "print(f\"   doc_ids = np.load('antique_bertweet_doc_ids.npy')\")\n",
    "print(f\"   \")\n",
    "print(f\"   # Or load complete dataset:\")\n",
    "print(f\"   with open('antique_bertweet_complete.pkl', 'rb') as f:\")\n",
    "print(f\"       data = pickle.load(f)\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ EMBEDDING GENERATION SUCCESSFUL!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "print(\"🧹 Cleaning up memory...\")\n",
    "\n",
    "# Delete large variables\n",
    "del embeddings\n",
    "del model\n",
    "del tokenizer\n",
    "del documents\n",
    "del texts\n",
    "del data\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "print(\"✅ Memory cleanup completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
