{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# ü¶ú BERTweet Embeddings for Antique Dataset\n",
    "\n",
    "This notebook generates document embeddings using the BERTweet model from Hugging Face for the cleaned Antique dataset.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload the `antique_cleaned_for_embeddings.json` file from your local text cleaning service\n",
    "2. The text has been pre-cleaned using the enhanced text cleaning service\n",
    "\n",
    "**Output:**\n",
    "- High-quality BERTweet embeddings for document representation\n",
    "- Embeddings saved in multiple formats for easy download and use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch numpy pandas scikit-learn tqdm\n",
    "!pip install -q datasets accelerate\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Upload and Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.py": {
       "data": "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"
      }
     },
     "output_id": "ad5f5de2-e9f9-4b89-8327-8da5b14b04ba"
    },
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "# Upload the cleaned dataset file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Please upload your 'antique_cleaned_for_embeddings.json' file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"‚úÖ Uploaded file: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "print(\"üìñ Loading cleaned dataset...\")\n",
    "\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"üìä Dataset Information:\")\n",
    "print(f\"   Dataset: {data.get('dataset', 'Unknown')}\")\n",
    "print(f\"   Total documents: {data.get('total_documents', 0):,}\")\n",
    "print(f\"   Export timestamp: {data.get('export_timestamp', 'Unknown')}\")\n",
    "print(f\"   Cleaning method: {data.get('cleaning_method', 'Unknown')}\")\n",
    "\n",
    "# Extract documents\n",
    "documents = data['documents']\n",
    "print(f\"\\nüîç Sample document:\")\n",
    "print(f\"   ID: {documents[0]['id']}\")\n",
    "print(f\"   Cleaned text: {documents[0]['text'][:200]}...\")\n",
    "print(f\"   Original text: {documents[0]['original_text'][:200]}...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(documents):,} documents successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Load BERTweet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load BERTweet model and tokenizer\n",
    "print(\"ü§ñ Loading BERTweet model...\")\n",
    "\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"   Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Load model\n",
    "print(\"   Loading model...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ BERTweet model loaded successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Embedding dimension: {model.config.hidden_size}\")\n",
    "print(f\"   Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Text Preprocessing for BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_for_bertweet(text, max_length=256):\n",
    "    \"\"\"\n",
    "    Preprocess text specifically for BERTweet model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning for BERTweet\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Truncate if too long (leave room for special tokens)\n",
    "    words = text.split()\n",
    "    if len(words) > max_length - 10:  # Reserve space for [CLS], [SEP], etc.\n",
    "        text = ' '.join(words[:max_length - 10])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = documents[0]['text']\n",
    "processed_text = preprocess_text_for_bertweet(sample_text)\n",
    "\n",
    "print(\"üîÑ Text preprocessing example:\")\n",
    "print(f\"   Original: {sample_text[:150]}...\")\n",
    "print(f\"   Processed: {processed_text[:150]}...\")\n",
    "print(f\"   Length: {len(processed_text.split())} words\")\n",
    "\n",
    "print(\"‚úÖ Text preprocessing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embeddings"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts, batch_size=16, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate BERTweet embeddings for a batch of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Batch size for processing\n",
    "        max_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Preprocess texts\n",
    "        processed_texts = [preprocess_text_for_bertweet(text, max_length) for text in batch_texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            processed_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use [CLS] token embedding (first token)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"‚úÖ Embedding generation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_all"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all documents\n",
    "print(\"üöÄ Starting embedding generation...\")\n",
    "print(f\"   Processing {len(documents):,} documents\")\n",
    "print(f\"   Using device: {device}\")\n",
    "\n",
    "# Extract texts\n",
    "texts = [doc['text'] for doc in documents]\n",
    "doc_ids = [doc['id'] for doc in documents]\n",
    "\n",
    "# Set batch size based on available memory\n",
    "batch_size = 32 if torch.cuda.is_available() else 8\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Generate embeddings\n",
    "start_time = datetime.now()\n",
    "embeddings = generate_embeddings_batch(texts, batch_size=batch_size)\n",
    "end_time = datetime.now()\n",
    "\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n‚úÖ Embedding generation completed!\")\n",
    "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"   Average time per document: {processing_time/len(documents):.4f} seconds\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Embedding Statistics:\")\n",
    "print(f\"   Mean: {embeddings.mean():.6f}\")\n",
    "print(f\"   Std: {embeddings.std():.6f}\")\n",
    "print(f\"   Min: {embeddings.min():.6f}\")\n",
    "print(f\"   Max: {embeddings.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Quality Check and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_check"
   },
   "outputs": [],
   "source": [
    "# Quality check: Test similarity between related documents\n",
    "print(\"üîç Quality Check: Testing embedding similarities...\")\n",
    "\n",
    "# Sample a few documents for similarity testing\n",
    "sample_indices = np.random.choice(len(embeddings), min(10, len(embeddings)), replace=False)\n",
    "sample_embeddings = embeddings[sample_indices]\n",
    "sample_texts = [texts[i] for i in sample_indices]\n",
    "sample_ids = [doc_ids[i] for i in sample_indices]\n",
    "\n",
    "# Calculate pairwise similarities\n",
    "similarities = cosine_similarity(sample_embeddings)\n",
    "\n",
    "print(f\"\\nüìä Similarity Matrix for {len(sample_indices)} sample documents:\")\n",
    "print(f\"   Average similarity: {similarities.mean():.4f}\")\n",
    "print(f\"   Std similarity: {similarities.std():.4f}\")\n",
    "\n",
    "# Find most similar pairs (excluding self-similarity)\n",
    "similarities_no_diag = similarities.copy()\n",
    "np.fill_diagonal(similarities_no_diag, -1)\n",
    "\n",
    "max_sim_idx = np.unravel_index(np.argmax(similarities_no_diag), similarities_no_diag.shape)\n",
    "max_similarity = similarities_no_diag[max_sim_idx]\n",
    "\n",
    "print(f\"\\nüîó Most similar document pair:\")\n",
    "print(f\"   Similarity: {max_similarity:.4f}\")\n",
    "print(f\"   Doc 1 ({sample_ids[max_sim_idx[0]]}): {sample_texts[max_sim_idx[0]][:100]}...\")\n",
    "print(f\"   Doc 2 ({sample_ids[max_sim_idx[1]]}): {sample_texts[max_sim_idx[1]][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Quality check completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_embeddings"
   },
   "outputs": [],
   "source": [
    "# Prepare data for saving\n",
    "print(\"üíæ Preparing data for saving...\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'embedding_dimension': embeddings.shape[1],\n",
    "    'total_documents': len(documents),\n",
    "    'generation_timestamp': datetime.now().isoformat(),\n",
    "    'processing_time_seconds': processing_time,\n",
    "    'device_used': str(device),\n",
    "    'batch_size': batch_size,\n",
    "    'max_sequence_length': 256,\n",
    "    'dataset_info': {\n",
    "        'dataset': data.get('dataset', 'antique'),\n",
    "        'cleaning_method': data.get('cleaning_method', 'enhanced'),\n",
    "        'export_timestamp': data.get('export_timestamp')\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Metadata prepared\")\n",
    "print(f\"   Model: {metadata['model_name']}\")\n",
    "print(f\"   Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "print(f\"   Total documents: {metadata['total_documents']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_numpy"
   },
   "outputs": [],
   "source": [
    "# Save as NumPy format\n",
    "print(\"üíæ Saving embeddings in NumPy format...\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('antique_bertweet_embeddings.npy', embeddings)\n",
    "\n",
    "# Save document IDs\n",
    "np.save('antique_bertweet_doc_ids.npy', np.array(doc_ids))\n",
    "\n",
    "# Save metadata\n",
    "with open('antique_bertweet_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ NumPy format saved:\")\n",
    "print(\"   - antique_bertweet_embeddings.npy\")\n",
    "print(\"   - antique_bertweet_doc_ids.npy\")\n",
    "print(\"   - antique_bertweet_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_pickle"
   },
   "outputs": [],
   "source": [
    "# Save as Pickle format (includes everything in one file)\n",
    "print(\"üíæ Saving complete dataset in Pickle format...\")\n",
    "\n",
    "complete_data = {\n",
    "    'embeddings': embeddings,\n",
    "    'doc_ids': doc_ids,\n",
    "    'texts': texts,\n",
    "    'metadata': metadata,\n",
    "    'original_documents': documents[:1000]  # Save first 1000 for reference\n",
    "}\n",
    "\n",
    "with open('antique_bertweet_complete.pkl', 'wb') as f:\n",
    "    pickle.dump(complete_data, f)\n",
    "\n",
    "print(\"‚úÖ Pickle format saved:\")\n",
    "print(\"   - antique_bertweet_complete.pkl\")\n",
    "print(\"   - Contains embeddings, IDs, texts, and metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_csv"
   },
   "outputs": [],
   "source": [
    "# Save as CSV format for easy inspection\n",
    "print(\"üíæ Saving document mapping in CSV format...\")\n",
    "\n",
    "# Create DataFrame with document info\n",
    "df = pd.DataFrame({\n",
    "    'doc_id': doc_ids,\n",
    "    'text': texts,\n",
    "    'text_length': [len(text) for text in texts],\n",
    "    'word_count': [len(text.split()) for text in texts]\n",
    "})\n",
    "\n",
    "# Add embedding norms for quality check\n",
    "df['embedding_norm'] = np.linalg.norm(embeddings, axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('antique_bertweet_documents.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ CSV format saved:\")\n",
    "print(\"   - antique_bertweet_documents.csv\")\n",
    "print(f\"   - {len(df):,} document records\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nüìä Document Statistics:\")\n",
    "print(f\"   Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"   Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Average embedding norm: {df['embedding_norm'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download all generated files\n",
    "print(\"üì• Downloading generated files...\")\n",
    "\n",
    "# List of files to download\n",
    "files_to_download = [\n",
    "    'antique_bertweet_embeddings.npy',\n",
    "    'antique_bertweet_doc_ids.npy',\n",
    "    'antique_bertweet_metadata.json',\n",
    "    'antique_bertweet_complete.pkl',\n",
    "    'antique_bertweet_documents.csv'\n",
    "]\n",
    "\n",
    "# Check file sizes\n",
    "print(\"üìã File Information:\")\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"   {filename}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"   {filename}: File not found\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to download! Run the next cell to download files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Download files one by one\n",
    "from google.colab import files\n",
    "\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"üì• Downloading {filename}...\")\n",
    "        files.download(filename)\n",
    "    else:\n",
    "        print(f\"‚ùå {filename} not found\")\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"üéâ BERTWEET EMBEDDING GENERATION COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Documents processed: {len(documents):,}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Total processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"   Average time per document: {processing_time/len(documents):.4f} seconds\")\n",
    "print(f\"   Device used: {device}\")\n",
    "print()\n",
    "print(f\"üìÅ Generated Files:\")\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"   ‚úÖ {filename} ({size_mb:.2f} MB)\")\n",
    "print()\n",
    "print(f\"üìã Next Steps:\")\n",
    "print(f\"   1. Download all generated files to your local machine\")\n",
    "print(f\"   2. Load the embeddings in your search engine backend\")\n",
    "print(f\"   3. Use embeddings for document retrieval and similarity search\")\n",
    "print(f\"   4. Evaluate retrieval performance using your evaluation metrics\")\n",
    "print()\n",
    "print(f\"üí° Usage Examples:\")\n",
    "print(f\"   # Load embeddings in Python:\")\n",
    "print(f\"   embeddings = np.load('antique_bertweet_embeddings.npy')\")\n",
    "print(f\"   doc_ids = np.load('antique_bertweet_doc_ids.npy')\")\n",
    "print(f\"   \")\n",
    "print(f\"   # Or load complete dataset:\")\n",
    "print(f\"   with open('antique_bertweet_complete.pkl', 'rb') as f:\")\n",
    "print(f\"       data = pickle.load(f)\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ EMBEDDING GENERATION SUCCESSFUL!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "print(\"üßπ Cleaning up memory...\")\n",
    "\n",
    "# Delete large variables\n",
    "del embeddings\n",
    "del model\n",
    "del tokenizer\n",
    "del documents\n",
    "del texts\n",
    "del data\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "print(\"‚úÖ Memory cleanup completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
