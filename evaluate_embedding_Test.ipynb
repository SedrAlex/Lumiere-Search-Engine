{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO85zVLdYmwn",
        "outputId": "e3f1e72a-df0f-497b-a8c1-b346fdbcb879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.11/dist-packages (0.5.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.13.4)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.6.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (5.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (6.0.2)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.4.4)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (3.4.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (18.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
            "Requirement already satisfied: ir-measures in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.5 in /usr/local/lib/python3.11/dist-packages (from ir-measures) (0.5.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "üî• CUDA available: False\n",
            "‚úÖ Unified text cleaner initialized (same as training notebook)!\n",
            "üìÇ Loading embedding model and data...\n",
            "üöÄ Loading SentenceTransformer model...\n",
            "‚úÖ Model loaded on cpu\n",
            "üìä Loading embeddings matrix...\n",
            "‚úÖ Embeddings matrix loaded with shape\n",
            "üìÑ Loading document metadata...\n",
            "‚úÖ Metadata loaded with 403,194 documents\n",
            "‚úÖ Document order aligned: 403,194\n",
            "‚úÖ Perfect alignment verified!\n",
            "üìö Loading Antique test set...\n",
            "Loading test queries...\n",
            "Loading test qrels...\n",
            "‚úÖ Test set loaded successfully!\n",
            "   üîç Test queries: 200\n",
            "   üéØ Qrels: 200\n",
            "üß™ Evaluating embedding model on test set (cosine similarity)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:25<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluation Results:\n",
            "üîç Queries evaluated: 200\n",
            "‚è±Ô∏è  Average query time: 723.51 ms\n",
            "üìä MAP@100: 0.1489\n",
            "üìä P@10: 0.3405\n",
            "üìä P@100: 0.1057\n",
            "üìä R@100: 0.3299\n",
            "üìä nDCG@100: 0.3396\n",
            "‚úÖ Evaluation results saved to antique_embedding_evaluation.json\n",
            "\n",
            "üîç Example Query Analysis:\n",
            "Query ID: 3990512\n",
            "Query Text: 'how can we get concentration onsomething?'\n",
            "Cleaned Query: 'how can we get concentr onsomething?'\n",
            "\n",
            "üéØ Relevant Documents (36):\n",
            "  - 3990512_1 (relevance=4)\n",
            "  - 332116_1 (relevance=4)\n",
            "  - 3270641_1 (relevance=4)\n",
            "  - 3980621_2 (relevance=4)\n",
            "  - 3990512_0 (relevance=3)\n",
            "  - 1900286_7 (relevance=3)\n",
            "  - 3154563_3 (relevance=3)\n",
            "  - 2800339_2 (relevance=3)\n",
            "  - 311770_2 (relevance=3)\n",
            "  - 4087614_7 (relevance=3)\n",
            "  - 667270_1 (relevance=3)\n",
            "  - 2036065_1 (relevance=2)\n",
            "  - 3265991_12 (relevance=2)\n",
            "  - 1772359_2 (relevance=2)\n",
            "  - 3579785_0 (relevance=2)\n",
            "  - 3239083_1 (relevance=2)\n",
            "  - 3974525_0 (relevance=2)\n",
            "  - 1173778_3 (relevance=2)\n",
            "  - 4359089_0 (relevance=2)\n",
            "  - 1173778_0 (relevance=2)\n",
            "  - 3916430_1 (relevance=2)\n",
            "  - 1860702_3 (relevance=2)\n",
            "  - 3149758_0 (relevance=1)\n",
            "  - 3990512_2 (relevance=1)\n",
            "  - 1949736_5 (relevance=1)\n",
            "  - 1949736_1 (relevance=1)\n",
            "  - 3253458_0 (relevance=1)\n",
            "  - 3749223_0 (relevance=1)\n",
            "  - 248974_2 (relevance=1)\n",
            "  - 1182425_0 (relevance=1)\n",
            "  - 4452969_1 (relevance=1)\n",
            "  - 3538410_2 (relevance=1)\n",
            "  - 55529_0 (relevance=1)\n",
            "  - 1860702_0 (relevance=1)\n",
            "  - 1860702_1 (relevance=1)\n",
            "  - 3656872_7 (relevance=1)\n",
            "\n",
            "üîç Top 10 Results:\n",
            "1. 2771268_1 (score=0.6110) ‚ùå rel=0\n",
            "   Find at least one positive thing and concentrate on that....\n",
            "\n",
            "2. 971642_3 (score=0.5976) ‚ùå rel=0\n",
            "   concentrate...\n",
            "\n",
            "3. 3265991_12 (score=0.5976) ‚úÖ rel=2\n",
            "   concentration...\n",
            "\n",
            "4. 2036065_1 (score=0.5976) ‚úÖ rel=2\n",
            "   just by concentration...\n",
            "\n",
            "5. 1253368_5 (score=0.5730) ‚ùå rel=0\n",
            "   TO CONCENTRATE OUR MIND....\n",
            "\n",
            "6. 1314452_4 (score=0.5648) ‚ùå rel=0\n",
            "   concentrate on your work not fun...\n",
            "\n",
            "7. 2085569_9 (score=0.5576) ‚ùå rel=0\n",
            "   I think that you should concentrate on how to use a computer and spell correctly first, then try to ...\n",
            "\n",
            "8. 1781658_4 (score=0.5411) ‚ùå rel=0\n",
            "   concentrate on other things and do more than sit around, doing nothing encourages you to do it. but ...\n",
            "\n",
            "9. 3347488_0 (score=0.5274) ‚ùå rel=0\n",
            "   When you invent something, you create something.  When you discover something, you find or observe s...\n",
            "\n",
            "10. 3077638_1 (score=0.5274) ‚ùå rel=0\n",
            "   Learn to concentrate.. First you really must want to focus and concentrate .. It helps if you are sm...\n",
            "\n",
            "\n",
            "‚úÖ Evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Evaluation Notebook for Antique Embedding Model (No FAISS)\n",
        "# Calculates MAP (Mean Average Precision) on Antique Test Set\n",
        "# Uses cosine similarity directly with embeddings\n",
        "# ===================================================================\n",
        "\n",
        "# STEP 1: Install Required Libraries\n",
        "!pip install sentence-transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install ir-datasets joblib requests scipy tqdm nltk numpy pandas scikit-learn\n",
        "!pip install ir-measures\n",
        "\n",
        "# STEP 2: Import Libraries\n",
        "import ir_datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# STEP 3: Load Unified Text Cleaning Service (SAME AS TRAINING)\n",
        "class UnifiedTextCleaningService:\n",
        "    \"\"\"Identical to training notebook to ensure consistency\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        from nltk.stem import PorterStemmer\n",
        "        from nltk.corpus import stopwords\n",
        "        import nltk\n",
        "\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text: str,\n",
        "                   remove_stopwords: bool = True,\n",
        "                   apply_stemming: bool = True,\n",
        "                   apply_lemmatization: bool = False) -> str:\n",
        "        \"\"\"Identical cleaning to training notebook\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Basic cleaning\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?;:()-]', '', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = [word.strip() for word in text.split() if word.strip()]\n",
        "\n",
        "        # Remove stopwords\n",
        "        if remove_stopwords and self.stop_words:\n",
        "            tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        # Stemming\n",
        "        if apply_stemming and self.stemmer:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "text_cleaner = UnifiedTextCleaningService()\n",
        "print(\"‚úÖ Unified text cleaner initialized (same as training notebook)!\"),\n",
        "\n",
        "# STEP 4: Load Model and Embeddings\n",
        "def load_embedding_model_and_data(model_path: str,\n",
        "                                 embeddings_path: str,\n",
        "                                 metadata_path: str):\n",
        "    \"\"\"Load all components needed for evaluation\"\"\"\n",
        "    print(\"üìÇ Loading embedding model and data...\")\n",
        "    # Load SentenceTransformer model\n",
        "    print(\"üöÄ Loading SentenceTransformer model...\")\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device)\n",
        "    print(f\"‚úÖ Model loaded on {device}\")\n",
        "    # Load embeddings matrix\n",
        "    print(\"üìä Loading embeddings matrix...\")\n",
        "    embeddings_matrix = joblib.load(\"/content/drive/MyDrive/downloads/antique_embeddings_matrix.joblib\")\n",
        "    print(f\"‚úÖ Embeddings matrix loaded with shape\")\n",
        "\n",
        "    # Load document metadata\n",
        "    print(\"üìÑ Loading document metadata...\")\n",
        "    metadata = joblib.load(\"/content/drive/MyDrive/downloads/antique_embedding_document_metadata.joblib\")\n",
        "    documents = metadata['documents']\n",
        "    document_order = metadata['document_order']\n",
        "    docid_to_index = {doc_id: idx for idx, doc_id in enumerate(document_order)}\n",
        "\n",
        "    cleaned_texts = metadata['cleaned_texts']\n",
        "\n",
        "    print(f\"‚úÖ Metadata loaded with {len(documents):,} documents\")\n",
        "    print(f\"‚úÖ Document order aligned: {len(document_order):,}\")\n",
        "\n",
        "    # Verify alignment\n",
        "    assert len(documents) == len(document_order) == len(cleaned_texts) == embeddings_matrix.shape[0]\n",
        "    print(\"‚úÖ Perfect alignment verified!\")\n",
        "\n",
        "    # Create docid to index mapping\n",
        "    docid_to_index = {doc_id: idx for idx, doc_id in enumerate(document_order)}\n",
        "\n",
        "    return model, embeddings_matrix, documents, document_order, docid_to_index\n",
        "\n",
        "# Update these paths to match your saved files\n",
        "MODEL_PATH = \"/content/gdrive/My Drive/downloads/antique_embedding_model\"\n",
        "EMBEDDINGS_PATH = \"/content/gdrive/My Drive/downloads/antique_embeddings_matrix.joblib\"\n",
        "METADATA_PATH = \"/content/gdrive/My Drive/downloads/antique_embedding_document_metadata.joblib\"\n",
        "\n",
        "# Load the model and data\n",
        "model, embeddings_matrix, documents, document_order, docid_to_index = load_embedding_model_and_data(\n",
        "    MODEL_PATH, EMBEDDINGS_PATH, METADATA_PATH\n",
        ")\n",
        "\n",
        "# STEP 5: Load Antique Test Set\n",
        "def load_antique_test_set():\n",
        "    \"\"\"Load test queries and qrels from Antique dataset\"\"\"\n",
        "    print(\"üìö Loading Antique test set...\")\n",
        "\n",
        "    # Load test dataset\n",
        "    dataset = ir_datasets.load('antique/test')\n",
        "\n",
        "    # Extract test queries and qrels\n",
        "    test_queries = []\n",
        "    test_qrels = {}\n",
        "\n",
        "    print(\"Loading test queries...\")\n",
        "    for query in dataset.queries_iter():\n",
        "        test_queries.append({\n",
        "            'query_id': query.query_id,\n",
        "            'text': query.text\n",
        "        })\n",
        "\n",
        "    print(\"Loading test qrels...\")\n",
        "    for qrel in dataset.qrels_iter():\n",
        "        if qrel.query_id not in test_qrels:\n",
        "            test_qrels[qrel.query_id] = {}\n",
        "        test_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "\n",
        "    print(f\"‚úÖ Test set loaded successfully!\")\n",
        "    print(f\"   üîç Test queries: {len(test_queries):,}\")\n",
        "    print(f\"   üéØ Qrels: {len(test_qrels):,}\")\n",
        "\n",
        "    return test_queries, test_qrels\n",
        "\n",
        "test_queries, test_qrels = load_antique_test_set()\n",
        "\n",
        "# STEP 6: Run Evaluation with Cosine Similarity\n",
        "def evaluate_with_cosine_similarity(model, embeddings_matrix, documents, document_order,\n",
        "                                  docid_to_index, test_queries, test_qrels, top_k=100):\n",
        "    \"\"\"Evaluate embedding model using cosine similarity\"\"\"\n",
        "\n",
        "    print(\"üß™ Evaluating embedding model on test set (cosine similarity)...\")\n",
        "\n",
        "    # Process each query and collect results\n",
        "    run_results = []\n",
        "    query_times = []\n",
        "\n",
        "    for query in tqdm(test_queries, desc=\"Processing queries\"):\n",
        "        query_id = query['query_id']\n",
        "        query_text = query['text']\n",
        "\n",
        "        # Skip queries without relevant documents\n",
        "        if query_id not in test_qrels:\n",
        "            continue\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Clean query with same method as training (less aggressive for queries)\n",
        "        cleaned_query = text_cleaner.clean_text(\n",
        "            query_text,\n",
        "            remove_stopwords=False,  # Keep stopwords for queries\n",
        "            apply_stemming=True,\n",
        "            apply_lemmatization=False\n",
        "        )\n",
        "\n",
        "        # Generate query embedding\n",
        "        with torch.no_grad():\n",
        "            query_embedding = model.encode(\n",
        "                [cleaned_query],\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True\n",
        "            )\n",
        "\n",
        "        # Calculate cosine similarity with all documents\n",
        "        similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]\n",
        "\n",
        "        # Get top-k results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        # Convert to run format for ir_measures\n",
        "        for rank, idx in enumerate(top_indices, 1):\n",
        "            doc_id = document_order[idx]\n",
        "            score = similarities[idx]\n",
        "            run_results.append({\n",
        "                'query_id': query_id,\n",
        "                'doc_id': doc_id,\n",
        "                'score': float(score),\n",
        "                'rank': rank\n",
        "            })\n",
        "\n",
        "        query_times.append(time.time() - start_time)\n",
        "\n",
        "    # Convert to pandas DataFrame for ir_measures\n",
        "    run_df = pd.DataFrame(run_results)\n",
        "\n",
        "    # Convert qrels to proper format\n",
        "    qrel_list = []\n",
        "    for query_id, docs in test_qrels.items():\n",
        "        for doc_id, rel in docs.items():\n",
        "            qrel_list.append({\n",
        "                'query_id': query_id,\n",
        "                'doc_id': doc_id,\n",
        "                'relevance': rel\n",
        "            })\n",
        "    qrels_df = pd.DataFrame(qrel_list)\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"\\nüìä Evaluation Results:\")\n",
        "    metrics = ir_measures.calc_aggregate(\n",
        "        [AP@100, P@10, P@100, R@100, nDCG@100],\n",
        "        qrels_df,\n",
        "        run_df\n",
        "    )\n",
        "\n",
        "    avg_query_time = np.mean(query_times) * 1000  # in milliseconds\n",
        "\n",
        "    # Print results\n",
        "    print(f\"üîç Queries evaluated: {len([q for q in test_queries if q['query_id'] in test_qrels]):,}\")\n",
        "    print(f\"‚è±Ô∏è  Average query time: {avg_query_time:.2f} ms\")\n",
        "    print(f\"üìä MAP@100: {metrics[AP@100]:.4f}\")\n",
        "    print(f\"üìä P@10: {metrics[P@10]:.4f}\")\n",
        "    print(f\"üìä P@100: {metrics[P@100]:.4f}\")\n",
        "    print(f\"üìä R@100: {metrics[R@100]:.4f}\")\n",
        "    print(f\"üìä nDCG@100: {metrics[nDCG@100]:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Run evaluation\n",
        "metrics = evaluate_with_cosine_similarity(\n",
        "    model, embeddings_matrix, documents, document_order,\n",
        "    docid_to_index, test_queries, test_qrels\n",
        ")\n",
        "\n",
        "# STEP 7: Save Evaluation Results\n",
        "def save_evaluation_results(metrics, output_path=\"antique_embedding_evaluation.json\"):\n",
        "    \"\"\"Save evaluation results to JSON file\"\"\"\n",
        "    results = {\n",
        "        'model': 'all-MiniLM-L6-v2',\n",
        "        'dataset': 'antique/test',\n",
        "        'metrics': {\n",
        "            'MAP@100': float(metrics[AP@100]),\n",
        "            'P@10': float(metrics[P@10]),\n",
        "            'P@100': float(metrics[P@100]),\n",
        "            'R@100': float(metrics[R@100]),\n",
        "            'nDCG@100': float(metrics[nDCG@100])\n",
        "        },\n",
        "        'configuration': {\n",
        "            'preprocessing': 'unified_text_cleaning',\n",
        "            'query_cleaning': {\n",
        "                'remove_stopwords': False,\n",
        "                'apply_stemming': True,\n",
        "                'apply_lemmatization': False\n",
        "            },\n",
        "            'evaluation_method': 'cosine_similarity',\n",
        "            'evaluation_top_k': 100,\n",
        "            'normalized_embeddings': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Evaluation results saved to {output_path}\")\n",
        "\n",
        "save_evaluation_results(metrics)\n",
        "\n",
        "# STEP 8: Example Query with Detailed Results\n",
        "def show_example_query(model, embeddings_matrix, documents, document_order, test_queries, test_qrels):\n",
        "    \"\"\"Show detailed results for an example query\"\"\"\n",
        "    print(\"\\nüîç Example Query Analysis:\")\n",
        "\n",
        "    # Find a query with several relevant documents\n",
        "    example_query = None\n",
        "    for query in test_queries:\n",
        "        if query['query_id'] in test_qrels and len(test_qrels[query['query_id']]) >= 3:\n",
        "            example_query = query\n",
        "            break\n",
        "\n",
        "    if not example_query:\n",
        "        print(\"No suitable example query found\")\n",
        "        return\n",
        "\n",
        "    query_id = example_query['query_id']\n",
        "    query_text = example_query['text']\n",
        "\n",
        "    print(f\"Query ID: {query_id}\")\n",
        "    print(f\"Query Text: '{query_text}'\")\n",
        "\n",
        "    # Clean query\n",
        "    cleaned_query = text_cleaner.clean_text(\n",
        "        query_text,\n",
        "        remove_stopwords=False,\n",
        "        apply_stemming=True,\n",
        "        apply_lemmatization=False\n",
        "    )\n",
        "    print(f\"Cleaned Query: '{cleaned_query}'\")\n",
        "\n",
        "    # Get relevant documents from qrels\n",
        "    relevant_docs = test_qrels[query_id]\n",
        "    print(f\"\\nüéØ Relevant Documents ({len(relevant_docs)}):\")\n",
        "    for doc_id, rel in sorted(relevant_docs.items(), key=lambda x: -x[1]):\n",
        "        print(f\"  - {doc_id} (relevance={rel})\")\n",
        "\n",
        "    # Generate embedding and calculate similarities\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model.encode(\n",
        "            [cleaned_query],\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]\n",
        "    top_indices = np.argsort(similarities)[::-1][:10]  # Top 10\n",
        "\n",
        "    print(\"\\nüîç Top 10 Results:\")\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        doc_id = document_order[idx]\n",
        "        is_relevant = doc_id in relevant_docs\n",
        "        rel_score = relevant_docs.get(doc_id, 0)\n",
        "        similarity = similarities[idx]\n",
        "        doc_text = documents[idx]['text'][:100] + \"...\"\n",
        "\n",
        "        print(f\"{rank}. {doc_id} (score={similarity:.4f}) {'‚úÖ' if is_relevant else '‚ùå'} rel={rel_score}\")\n",
        "        print(f\"   {doc_text}\\n\")\n",
        "\n",
        "show_example_query(model, embeddings_matrix, documents, document_order, test_queries, test_qrels)\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation completed!\")"
      ]
    }
  ]
}