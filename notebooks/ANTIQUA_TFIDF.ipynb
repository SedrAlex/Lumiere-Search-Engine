{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install nltk scikit-learn pandas numpy joblib contractions beautifulsoup4 inflect\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.lemma import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import inflect\n",
    "import joblib\n",
    "from google.colab import files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing Class\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = nltk.tokenize.word_tokenize\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.inflect_engine = inflect.engine()\n",
    "\n",
    "    def clean_text(self, text, words_to_remove):\n",
    "        words = text.split()\n",
    "        cleaned_words = [word for word in words if word not in words_to_remove]\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "        return cleaned_text\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            if word.replace('.', '', 1).isdigit():\n",
    "                converted_words.append(word)\n",
    "            else:\n",
    "                if word.isdigit():\n",
    "                    try:\n",
    "                        num = int(word)\n",
    "                        if num <= 999999999999999:\n",
    "                            converted_word = self.inflect_engine.number_to_words(word)\n",
    "                            converted_words.append(converted_word)\n",
    "                        else:\n",
    "                            converted_words.append('[Number Out of Range]')\n",
    "                    except:\n",
    "                        converted_words.append('[Number Out of Range]')\n",
    "                else:\n",
    "                    converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        try:\n",
    "            if '<' in text and '>' in text:\n",
    "                return BeautifulSoup(text, 'html.parser').get_text()\n",
    "            else:\n",
    "                return text\n",
    "        except:\n",
    "            logging.warning('MarkupResemblesLocatorWarning: The input looks more like a filename than markup.')\n",
    "            return text\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def remove_special_characters_and_emojis(self, text):\n",
    "        return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "    def replace_synonyms(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        synonym_words = [self.get_synonym(word) for word in words]\n",
    "        return ' '.join(synonym_words)\n",
    "\n",
    "    def get_synonym(self, word):\n",
    "        synonyms = nltk.corpus.wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            return synonyms[0].lemmas()[0].name()\n",
    "        return word\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f'NOT_{word}')\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_non_english_words(self, text):\n",
    "        words = self.tokenizer(text)\n",
    "        english_words = [word for word in words if wordnet.synsets(word)]\n",
    "        return ' '.join(english_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text processor\n",
    "processor = TextProcessor()\n",
    "\n",
    "# Custom text processing function\n",
    "def processed_text(text):\n",
    "    if text is None:\n",
    "        return text\n",
    "    text = processor.cleaned_text(text)\n",
    "    text = processor.normalization_example(text)\n",
    "    text = processor.stemming_example(text)\n",
    "    text = processor.lemmatization_example(text)\n",
    "    text = processor.remove_stopwords(text)\n",
    "    text = processor.number_to_words(text)\n",
    "    text = processor.remove_punctuation(text)\n",
    "    text = processor.expand_contractions(text)\n",
    "    text = processor.normalize_unicode(text)\n",
    "    text = processor.handle_negations(text)\n",
    "    text = processor.remove_urls(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Google Drive\n",
    "# Update this path to match your dataset location\n",
    "dataset_path = '/content/drive/MyDrive/antique_dataset/antique_dataset.tsv'\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text columns (adjust based on your dataset structure)\n",
    "# Common column names for text data\n",
    "text_columns = []\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['text', 'content', 'description', 'title', 'body', 'document']):\n",
    "        text_columns.append(col)\n",
    "\n",
    "print(f'Identified text columns: {text_columns}')\n",
    "\n",
    "# If no text columns found automatically, specify manually\n",
    "if not text_columns:\n",
    "    # Manually specify your text column name here\n",
    "    text_columns = ['text']  # Replace with your actual column name\n",
    "    print(f'Using manual text columns: {text_columns}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text columns if multiple exist\n",
    "if len(text_columns) > 1:\n",
    "    df['combined_text'] = df[text_columns].fillna('').astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "    text_column = 'combined_text'\n",
    "else:\n",
    "    text_column = text_columns[0]\n",
    "\n",
    "# Remove null values\n",
    "documents = df[text_column].dropna().astype(str).tolist()\n",
    "print(f'Number of documents: {len(documents)}')\n",
    "print(f'Sample document: {documents[0][:200]}...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom text processing to all documents\n",
    "print('Processing documents with custom cleaning method...')\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Processed {i}/{len(documents)} documents')\n",
    "    \n",
    "    processed_doc = processed_text(doc)\n",
    "    processed_documents.append(processed_doc)\n",
    "\n",
    "print(f'Text processing complete. Sample processed document: {processed_documents[0][:200]}...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom tokenizer that applies our text processing\n",
    "def custom_tokenizer(text):\n",
    "    \"\"\"Custom tokenizer using our processed_text function\"\"\"\n",
    "    # Apply our custom text processing\n",
    "    processed = processed_text(text)\n",
    "    # Tokenize the processed text\n",
    "    tokens = processor.tokenizer(processed) if processed else []\n",
    "    return tokens\n",
    "\n",
    "# Create TF-IDF vectorizer with custom tokenizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    preprocessor=False,  # Disable built-in preprocessing\n",
    "    tokenizer=custom_tokenizer,  # Use our custom tokenizer\n",
    "    token_pattern=None,  # Disable default token pattern\n",
    "    lowercase=False,     # We already handle lowercasing in custom processing\n",
    "    stop_words=None,     # We already handle stopwords in custom processing\n",
    "    max_features=10000,  # Limit vocabulary size\n",
    "    min_df=2,           # Minimum document frequency\n",
    "    max_df=0.95,        # Maximum document frequency\n",
    "    ngram_range=(1, 2)   # Include unigrams and bigrams\n",
    ")\n",
    "\n",
    "print('Fitting TF-IDF vectorizer with custom tokenizer...')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)  # Use original documents, not processed ones\n",
    "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
    "print(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inverted index from TF-IDF matrix\n",
    "print('Building inverted index...')\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create inverted index\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "# Convert sparse matrix to coordinate format for efficient iteration\n",
    "coo_matrix = tfidf_matrix.tocoo()\n",
    "\n",
    "for doc_idx, term_idx in zip(coo_matrix.row, coo_matrix.col):\n",
    "    term = feature_names[term_idx]\n",
    "    tfidf_score = coo_matrix.data[coo_matrix.row == doc_idx][coo_matrix.col[coo_matrix.row == doc_idx] == term_idx][0]\n",
    "    inverted_index[term].append((doc_idx, tfidf_score))\n",
    "\n",
    "# Sort document lists by TF-IDF score (descending)\n",
    "for term in inverted_index:\n",
    "    inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f'Inverted index created with {len(inverted_index)} terms')\n",
    "print(f'Sample term: {list(inverted_index.keys())[0]} -> {inverted_index[list(inverted_index.keys())[0]][:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document similarity matrix\n",
    "print('Calculating document similarity matrix...')\n",
    "\n",
    "# Calculate cosine similarity between all documents\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "print(f'Similarity matrix shape: {similarity_matrix.shape}')\n",
    "\n",
    "# Calculate statistics\n",
    "mean_similarity = np.mean(similarity_matrix)\n",
    "max_similarity = np.max(similarity_matrix)\n",
    "min_similarity = np.min(similarity_matrix)\n",
    "\n",
    "print(f'Mean similarity: {mean_similarity:.4f}')\n",
    "print(f'Max similarity: {max_similarity:.4f}')\n",
    "print(f'Min similarity: {min_similarity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAP (Mean Average Precision) for IR evaluation\n",
    "print('Calculating MAP score...')\n",
    "\n",
    "def calculate_map(similarity_matrix, threshold=0.4):\n",
    "    \"\"\"Calculate MAP score for document retrieval\"\"\"\n",
    "    map_scores = []\n",
    "    n_docs = similarity_matrix.shape[0]\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        # Get similarity scores for document i (excluding self)\n",
    "        similarities = similarity_matrix[i].copy()\n",
    "        similarities[i] = 0  # Exclude self-similarity\n",
    "        \n",
    "        # Sort documents by similarity (descending)\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_similarities = similarities[sorted_indices]\n",
    "        \n",
    "        # Calculate precision at each relevant document\n",
    "        relevant_docs = sorted_similarities >= threshold\n",
    "        if np.sum(relevant_docs) == 0:\n",
    "            map_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precisions = []\n",
    "        num_relevant = 0\n",
    "        \n",
    "        for j, is_relevant in enumerate(relevant_docs):\n",
    "            if is_relevant:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / (j + 1)\n",
    "                precisions.append(precision)\n",
    "        \n",
    "        if precisions:\n",
    "            average_precision = np.mean(precisions)\n",
    "            map_scores.append(average_precision)\n",
    "        else:\n",
    "            map_scores.append(0.0)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "# Calculate MAP with threshold 0.4\n",
    "map_score = calculate_map(similarity_matrix, threshold=0.4)\n",
    "print(f'MAP score (threshold=0.4): {map_score:.4f}')\n",
    "\n",
    "# Try different thresholds if MAP is too low\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "print('\\nMAP scores at different thresholds:')\n",
    "for thresh in thresholds:\n",
    "    map_val = calculate_map(similarity_matrix, threshold=thresh)\n",
    "    print(f'Threshold {thresh}: MAP = {map_val:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix for additional analysis\n",
    "print('Creating document-term matrix...')\n",
    "\n",
    "# Convert to dense matrix for easier manipulation (only if not too large)\n",
    "if tfidf_matrix.shape[0] * tfidf_matrix.shape[1] < 10000000:  # Limit to avoid memory issues\n",
    "    doc_term_matrix = tfidf_matrix.toarray()\n",
    "    print(f'Document-term matrix shape: {doc_term_matrix.shape}')\n",
    "else:\n",
    "    doc_term_matrix = tfidf_matrix  # Keep as sparse matrix\n",
    "    print(f'Document-term matrix shape (sparse): {doc_term_matrix.shape}')\n",
    "\n",
    "# Calculate term frequencies\n",
    "term_frequencies = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "term_freq_dict = dict(zip(feature_names, term_frequencies))\n",
    "\n",
    "# Get top terms\n",
    "top_terms = sorted(term_freq_dict.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "print('\\nTop 20 terms by frequency:')\n",
    "for term, freq in top_terms:\n",
    "    print(f'{term}: {freq:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all components using joblib\n",
    "print('Saving all components...')\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "print('TF-IDF vectorizer saved')\n",
    "\n",
    "# Save TF-IDF matrix\n",
    "joblib.dump(tfidf_matrix, 'tfidf_matrix.joblib')\n",
    "print('TF-IDF matrix saved')\n",
    "\n",
    "# Save similarity matrix\n",
    "joblib.dump(similarity_matrix, 'similarity_matrix.joblib')\n",
    "print('Similarity matrix saved')\n",
    "\n",
    "# Save inverted index\n",
    "joblib.dump(dict(inverted_index), 'inverted_index.joblib')\n",
    "print('Inverted index saved')\n",
    "\n",
    "# Save document-term matrix\n",
    "joblib.dump(doc_term_matrix, 'doc_term_matrix.joblib')\n",
    "print('Document-term matrix saved')\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(feature_names, 'feature_names.joblib')\n",
    "print('Feature names saved')\n",
    "\n",
    "# Save processed documents\n",
    "joblib.dump(processed_documents, 'processed_documents.joblib')\n",
    "print('Processed documents saved')\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics = {\n",
    "    'map_score': map_score,\n",
    "    'mean_similarity': mean_similarity,\n",
    "    'max_similarity': max_similarity,\n",
    "    'min_similarity': min_similarity,\n",
    "    'vocab_size': len(tfidf_vectorizer.vocabulary_),\n",
    "    'num_documents': len(documents),\n",
    "    'matrix_shape': tfidf_matrix.shape\n",
    "}\n",
    "joblib.dump(metrics, 'ir_metrics.joblib')\n",
    "print('IR metrics saved')\n",
    "\n",
    "print('All components saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "print('Downloading files...')\n",
    "\n",
    "files_to_download = [\n",
    "    'tfidf_vectorizer.joblib',\n",
    "    'tfidf_matrix.joblib',\n",
    "    'similarity_matrix.joblib',\n",
    "    'inverted_index.joblib',\n",
    "    'doc_term_matrix.joblib',\n",
    "    'feature_names.joblib',\n",
    "    'processed_documents.joblib',\n",
    "    'ir_metrics.joblib'\n",
    "]\n",
    "\n",
    "for file_name in files_to_download:\n",
    "    try:\n",
    "        files.download(file_name)\n",
    "        print(f'Downloaded: {file_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading {file_name}: {e}')\n",
    "\n",
    "print('Download complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and validation\n",
    "print('=== SUMMARY ==>')\n",
    "print(f'Documents processed: {len(documents)}')\n",
    "print(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}')\n",
    "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
    "print(f'Inverted index terms: {len(inverted_index)}')\n",
    "print(f'MAP score: {map_score:.4f}')\n",
    "print(f'Mean document similarity: {mean_similarity:.4f}')\n",
    "\n",
    "if map_score >= 0.4:\n",
    "    print('✅ MAP score requirement met (>= 0.4)')\n",
    "else:\n",
    "    print('❌ MAP score below 0.4. Consider adjusting parameters.')\n",
    "\n",
    "print('\\n=== USAGE EXAMPLE ==>')\n",
    "print('To load the saved components:')\n",
    "print('import joblib')\n",
    "print('vectorizer = joblib.load(\"tfidf_vectorizer.joblib\")')\n",
    "print('matrix = joblib.load(\"tfidf_matrix.joblib\")')\n",
    "print('inverted_index = joblib.load(\"inverted_index.joblib\")')\n",
    "print('similarity_matrix = joblib.load(\"similarity_matrix.joblib\")')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
