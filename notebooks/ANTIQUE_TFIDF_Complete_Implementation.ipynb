{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ANTIQUE Dataset TF-IDF Implementation with Advanced Text Cleaning\n",
        "\n",
        "This notebook implements TF-IDF vectorization on the ANTIQUE dataset with:\n",
        "- **Advanced custom text cleaning tailored for ANTIQUE medical queries**\n",
        "- **Custom tokenization with medical term preservation**\n",
        "- **Inverted index construction**\n",
        "- **Model persistence using joblib**\n",
        "- **Evaluation using MAP metric (target: â‰¥ 0.2)**\n",
        "\n",
        "## Dataset Structure\n",
        "- Documents: ANTIQUE medical documents from Yahoo Answers\n",
        "- Queries: Medical questions and health-related queries\n",
        "- Relevance judgments: Manual relevance annotations\n",
        "\n",
        "## Key Optimizations for ANTIQUE\n",
        "- Medical terminology preservation\n",
        "- Health-specific text preprocessing\n",
        "- Handling of symptom and treatment descriptions\n",
        "- Optimized n-gram features for medical content matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d91ce0-87de-432d-dd8f-3aba4517b2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "!pip install nltk scikit-learn pandas numpy joblib tqdm gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8528163-21af-4f1e-886e-216d179b2b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import joblib\n",
        "import nltk\n",
        "import warnings\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "    print('Running in local environment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## 2. Data Loading and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "load_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871eb167-f112-4b3b-d44b-281dc1e4bcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found: /content/drive/MyDrive/downloads/documents.tsv\n",
            "âœ“ Found: /content/drive/MyDrive/downloads/queries.tsv\n",
            "âœ“ Found: /content/drive/MyDrive/downloads/qrels.tsv\n",
            "Loaded 403666 documents\n",
            "Loaded 2426 queries\n",
            "Loaded 27422 relevance judgments\n"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "if COLAB_ENV:\n",
        "    DATA_PATH = '/content/drive/MyDrive/downloads'\n",
        "else:\n",
        "    DATA_PATH = '/Users/raafatmhanna/Desktop/custom-search-engine/backend/data'\n",
        "\n",
        "DOCS_FILE = os.path.join(DATA_PATH, 'documents.tsv')\n",
        "QUERIES_FILE = os.path.join(DATA_PATH, 'queries.tsv')\n",
        "QRELS_FILE = os.path.join(DATA_PATH, 'qrels.tsv')\n",
        "\n",
        "# Verify files exist\n",
        "files_to_check = [DOCS_FILE, QUERIES_FILE, QRELS_FILE]\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f'âœ“ Found: {file_path}')\n",
        "    else:\n",
        "        print(f'âœ— Missing: {file_path}')\n",
        "\n",
        "# Load datasets\n",
        "docs_df = pd.read_csv(DOCS_FILE, sep='\t')\n",
        "queries_df = pd.read_csv(QUERIES_FILE, sep='\t')\n",
        "qrels_df = pd.read_csv(QRELS_FILE, sep='\t')\n",
        "\n",
        "print(f'Loaded {len(docs_df)} documents')\n",
        "print(f'Loaded {len(queries_df)} queries')\n",
        "print(f'Loaded {len(qrels_df)} relevance judgments')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "## 3. Advanced Text Cleaning for ANTIQUE Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "text_cleaning_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae8cb1a-437b-4059-8b64-d689af8d0983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: What causes severe swelling and pain in the knees?\n",
            "Cleaned: what caus sever swell pain knee\n",
            "\n",
            "Original: I have chronic aching in my joints. What's the best remedy?\n",
            "Cleaned: chronic ach joint what best remedi\n"
          ]
        }
      ],
      "source": [
        "# Enhanced OptimizedAntiqueTextCleaner with better preprocessing\n",
        "class EnhancedAntiqueTextCleaner:\n",
        "    \"\"\"\n",
        "    Enhanced text cleaning class for ANTIQUE dataset with optimized MAP performance.\n",
        "    Key improvements:\n",
        "    1. Minimal stopwords - only core filler words\n",
        "    2. Enhanced medical synonyms dictionary\n",
        "    3. Better contraction handling\n",
        "    4. POS-aware lemmatization\n",
        "    5. Medical phrase preservation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Minimal stopwords - preserve meaningful medical and query terms\n",
        "        self.stop_words = {\n",
        "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
        "            'of', 'with', 'by', 'from', 'as', 'into', 'through', 'during',\n",
        "            'before', 'after', 'above', 'below', 'up', 'down', 'out', 'off',\n",
        "            'over', 'under', 'again', 'further', 'then', 'once'\n",
        "        }\n",
        "\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "        # Comprehensive contractions dictionary\n",
        "        self.contractions = {\n",
        "            \"don't\": \"do not\", \"can't\": \"cannot\", \"won't\": \"will not\",\n",
        "            \"n't\": \" not\", \"'re\": \" are\", \"'ve\": \" have\",\n",
        "            \"'ll\": \" will\", \"'d\": \" would\", \"'m\": \" am\",\n",
        "            \"what's\": \"what is\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "            \"it's\": \"it is\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
        "            \"doesn't\": \"does not\", \"isn't\": \"is not\", \"wasn't\": \"was not\",\n",
        "            \"weren't\": \"were not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
        "            \"hadn't\": \"had not\", \"wouldn't\": \"would not\", \"shouldn't\": \"should not\",\n",
        "            \"couldn't\": \"could not\", \"mustn't\": \"must not\", \"who's\": \"who is\",\n",
        "            \"where's\": \"where is\", \"when's\": \"when is\", \"why's\": \"why is\",\n",
        "            \"how's\": \"how is\", \"here's\": \"here is\", \"let's\": \"let us\"\n",
        "        }\n",
        "\n",
        "        # Enhanced medical synonyms and variations\n",
        "        self.medical_synonyms = {\n",
        "            # Pain-related terms\n",
        "            'ache': 'pain', 'aching': 'pain', 'hurt': 'pain', 'hurting': 'pain',\n",
        "            'sore': 'pain', 'tender': 'pain', 'discomfort': 'pain', 'soreness': 'pain',\n",
        "            'throbbing': 'pain', 'stinging': 'pain', 'burning': 'pain',\n",
        "            \n",
        "            # Disease/condition terms\n",
        "            'illness': 'disease', 'sickness': 'disease', 'ailment': 'disease',\n",
        "            'disorder': 'condition', 'syndrome': 'condition', 'malady': 'disease',\n",
        "            \n",
        "            # Treatment terms\n",
        "            'remedy': 'treatment', 'cure': 'treatment', 'therapy': 'treatment',\n",
        "            'medication': 'medicine', 'drug': 'medicine', 'pill': 'medicine',\n",
        "            'prescription': 'medicine',\n",
        "            \n",
        "            # Medical professional terms\n",
        "            'physician': 'doctor', 'doc': 'doctor', 'medic': 'doctor',\n",
        "            'surgeon': 'doctor', 'specialist': 'doctor',\n",
        "            \n",
        "            # Symptom terms\n",
        "            'swelling': 'inflammation', 'redness': 'inflammation',\n",
        "            'infection': 'inflammation', 'irritation': 'inflammation',\n",
        "            \n",
        "            # Body part variations\n",
        "            'tummy': 'stomach', 'belly': 'stomach', 'gut': 'stomach',\n",
        "            'knee': 'joint', 'ankle': 'joint', 'wrist': 'joint',\n",
        "            'elbow': 'joint', 'shoulder': 'joint', 'hip': 'joint'\n",
        "        }\n",
        "\n",
        "        # Medical phrases that should be kept together\n",
        "        self.medical_phrases = {\n",
        "            'high blood pressure': 'hypertension',\n",
        "            'blood pressure': 'hypertension',\n",
        "            'heart attack': 'cardiac_event',\n",
        "            'chest pain': 'chest_pain',\n",
        "            'back pain': 'back_pain',\n",
        "            'stomach pain': 'stomach_pain',\n",
        "            'head ache': 'headache',\n",
        "            'sore throat': 'sore_throat',\n",
        "            'runny nose': 'runny_nose'\n",
        "        }\n",
        "\n",
        "    def get_wordnet_pos(self, word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        \"\"\"Expand contractions in text\"\"\"\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = re.sub(re.escape(contraction), expansion, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def normalize_medical_phrases(self, text):\n",
        "        \"\"\"Normalize common medical phrases\"\"\"\n",
        "        for phrase, normalized in self.medical_phrases.items():\n",
        "            text = re.sub(r'\\b' + re.escape(phrase) + r'\\b', normalized, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def normalize_medical_terms(self, text):\n",
        "        \"\"\"Normalize medical terms to improve matching\"\"\"\n",
        "        words = text.split()\n",
        "        normalized_words = []\n",
        "        for word in words:\n",
        "            if word.lower() in self.medical_synonyms:\n",
        "                normalized_words.append(self.medical_synonyms[word.lower()])\n",
        "            else:\n",
        "                normalized_words.append(word)\n",
        "        return ' '.join(normalized_words)\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Expand contractions\n",
        "        text = self.expand_contractions(text)\n",
        "\n",
        "        # Normalize medical phrases first\n",
        "        text = self.normalize_medical_phrases(text)\n",
        "\n",
        "        # Remove URLs, emails, and other web artifacts\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Keep only letters, numbers and spaces\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize\n",
        "        words = text.split()\n",
        "\n",
        "        # Filter out very short words and pure numbers\n",
        "        words = [word for word in words if len(word) >= 2 and not word.isdigit()]\n",
        "\n",
        "        # Process words with POS-aware lemmatization\n",
        "        processed_words = []\n",
        "        for word in words:\n",
        "            if word not in self.stop_words:\n",
        "                # POS-aware lemmatization\n",
        "                pos = self.get_wordnet_pos(word)\n",
        "                lemma = self.lemmatizer.lemmatize(word, pos)\n",
        "                # Apply stemming for better matching\n",
        "                stemmed = self.stemmer.stem(lemma)\n",
        "                processed_words.append(stemmed)\n",
        "\n",
        "        # Join and normalize medical terms\n",
        "        cleaned_text = ' '.join(processed_words)\n",
        "        cleaned_text = self.normalize_medical_terms(cleaned_text)\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "\n",
        "# Optimized TF-IDF parameters for better medical query relevance\n",
        "def create_optimized_tfidf_vectorizer():\n",
        "    \"\"\"Create TF-IDF vectorizer with optimized parameters for ANTIQUE dataset\"\"\"\n",
        "    return TfidfVectorizer(\n",
        "        tokenizer=lambda text: text.split(),  # Simple tokenizer\n",
        "        lowercase=False,  # Already handled in preprocessing\n",
        "        stop_words=None,  # Already handled in preprocessing\n",
        "        max_features=200000,  # Increased for better coverage\n",
        "        ngram_range=(1, 2),  # Bigrams for medical phrases\n",
        "        max_df=0.4,  # More aggressive filtering of common terms\n",
        "        min_df=2,    # Keep rare but potentially important terms\n",
        "        smooth_idf=True,\n",
        "        sublinear_tf=True,  # Log-scaling for TF\n",
        "        norm='l2'\n",
        "    )\n",
        "\n",
        "\n",
        "# Enhanced query expansion with medical focus\n",
        "def expand_query_medical(query, top_n=3):\n",
        "    \"\"\"Medical-focused query expansion using WordNet synonyms\"\"\"\n",
        "    words = query.split()\n",
        "    expanded_terms = set(words)  # Start with original words\n",
        "    \n",
        "    # Medical term mappings for common concepts\n",
        "    medical_expansions = {\n",
        "        'pain': ['ache', 'hurt', 'sore', 'discomfort'],\n",
        "        'treatment': ['remedy', 'cure', 'therapy', 'medication'],\n",
        "        'disease': ['illness', 'condition', 'disorder', 'syndrome'],\n",
        "        'doctor': ['physician', 'specialist', 'medic'],\n",
        "        'medicine': ['drug', 'medication', 'pill', 'prescription'],\n",
        "        'inflammation': ['swelling', 'redness', 'infection']\n",
        "    }\n",
        "    \n",
        "    for word in words:\n",
        "        # Add medical expansions if available\n",
        "        if word in medical_expansions:\n",
        "            expanded_terms.update(medical_expansions[word][:top_n])\n",
        "        \n",
        "        # Add WordNet synonyms\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemma_names():\n",
        "                if '_' not in lemma and len(lemma) > 2:\n",
        "                    synonyms.add(lemma.lower())\n",
        "        \n",
        "        # Add top synonyms (shorter ones first for precision)\n",
        "        sorted_synonyms = sorted(synonyms, key=len)[:top_n]\n",
        "        expanded_terms.update(sorted_synonyms)\n",
        "    \n",
        "    # Filter out very short or redundant terms\n",
        "    filtered_terms = [term for term in expanded_terms if len(term) > 2]\n",
        "    return ' '.join(filtered_terms)\n",
        "\n",
        "# Initialize enhanced text cleaner with better medical term processing\n",
        "text_cleaner = EnhancedAntiqueTextCleaner()\n",
        "\n",
        "# Test the cleaner\n",
        "test_text = \"What causes severe swelling and pain in the knees?\"\n",
        "cleaned_text = text_cleaner.clean_text(test_text)\n",
        "print(f'Original: {test_text}')\n",
        "print(f'Cleaned: {cleaned_text}')\n",
        "\n",
        "# Test with medical terms\n",
        "test_text2 = \"I have chronic aching in my joints. What's the best remedy?\"\n",
        "cleaned_text2 = text_cleaner.clean_text(test_text2)\n",
        "print(f'\\nOriginal: {test_text2}')\n",
        "print(f'Cleaned: {cleaned_text2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preprocessing"
      },
      "source": [
        "## 4. Data Preprocessing and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "preprocess_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8976cfce-b2fb-4f81-f4d5-0a226f22165b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403666/403666 [05:37<00:00, 1197.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning: 402025 documents remaining\n",
            "Preprocessing queries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2426/2426 [00:00<00:00, 4360.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning: 2426 queries remaining\n",
            "\n",
            "Example cleaned documents:\n",
            "Doc 1: small group politician believ strongli fact saddam hussien remain power after first gulf war signal ...\n",
            "Doc 2: becaus there lot oil iraq...\n",
            "Doc 3: tempt say invad iraq becaus lot oil not countri deep econom problem captur other countri oil actual ...\n",
            "\n",
            "Example cleaned queries:\n",
            "Query 1: what caus sever swell pain knee\n",
            "Query 2: whi not put parachut underneath airplan seat\n",
            "Query 3: how clean alloy cylind head\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocess documents with progress bar\n",
        "print('Preprocessing documents...')\n",
        "tqdm.pandas(desc='Cleaning documents')\n",
        "docs_df['cleaned_text'] = docs_df['text'].progress_apply(text_cleaner.clean_text)\n",
        "docs_df = docs_df[docs_df['cleaned_text'].str.len() > 0]\n",
        "print(f'After cleaning: {len(docs_df)} documents remaining')\n",
        "\n",
        "# Preprocess queries\n",
        "print('Preprocessing queries...')\n",
        "tqdm.pandas(desc='Cleaning queries')\n",
        "queries_df['cleaned_query'] = queries_df['text'].progress_apply(text_cleaner.clean_text)\n",
        "queries_df = queries_df[queries_df['cleaned_query'].str.len() > 0]\n",
        "print(f'After cleaning: {len(queries_df)} queries remaining')\n",
        "\n",
        "# Show some examples\n",
        "print('\\nExample cleaned documents:')\n",
        "for i in range(min(3, len(docs_df))):\n",
        "    print(f'Doc {i+1}: {docs_df.iloc[i][\"cleaned_text\"][:100]}...')\n",
        "\n",
        "print('\\nExample cleaned queries:')\n",
        "for i in range(min(3, len(queries_df))):\n",
        "    print(f'Query {i+1}: {queries_df.iloc[i][\"cleaned_query\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ukblSOHwf6"
      },
      "source": [
        "## 5. Inverted Index Construction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_vectorization"
      },
      "source": [
        "## 5. TF-IDF Vectorization with Custom Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA3TquMEHwf6"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Create inverted index\n",
        "def create_inverted_index(X, feature_names):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, doc in enumerate(X):\n",
        "        for word in doc.indices:\n",
        "            term = feature_names[word]\n",
        "            inverted_index[term].append(doc_id)\n",
        "    return inverted_index\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "inverted_index = create_inverted_index(tfidf_matrix, feature_names)\n",
        "print(list(inverted_index.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "create_tfidf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420fb35d-d8ea-4b37-ca94-4c5318c7d540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating optimized TF-IDF vectorizer...\n",
            "Vectorizing documents...\n",
            "TF-IDF matrix created with shape: (402025, 150000)\n",
            "Vocabulary size: 150000\n",
            "Applying smart query expansion...\n",
            "Query expansion completed\n"
          ]
        }
      ],
      "source": [
        "# Create optimized TF-IDF vectorizer for ANTIQUE dataset\n",
        "print('Creating optimized TF-IDF vectorizer...')\n",
        "\n",
        "# Simple tokenizer for faster processing\n",
        "def simple_tokenizer(text):\n",
        "    return text.split()\n",
"\n",
"# Use optimized TF-IDF parameters for better medical query relevance\n",
        "tfidf_vectorizer = create_optimized_tfidf_vectorizer()\n",
        "\n",
        "# Alternative: manual configuration with optimized parameters\n",
        "# tfidf_vectorizer = TfidfVectorizer(\n",
        "#     tokenizer=simple_tokenizer,\n",
        "#     lowercase=False,  # Already handled in preprocessing\n",
        "#     stop_words=None,  # Already handled in preprocessing\n",
        "#     max_features=200000,  # Increased for better coverage\n",
        "#     ngram_range=(1, 2),  # Bigrams for medical phrases\n",
        "#     max_df=0.4,      # More aggressive filtering of common terms\n",
        "#     min_df=2,         # Keep rare but potentially important terms\n",
        "#     smooth_idf=True,\n",
        "#     sublinear_tf=True,  # Log-scaling for TF\n",
        "#     norm='l2'         # Keep L2 normalization\n",
        "# )\n",
        "\n",
        "# Fit and transform documents\n",
        "print('Vectorizing documents...')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(docs_df['cleaned_text'])\n",
        "print(f'TF-IDF matrix created with shape: {tfidf_matrix.shape}')\n",
        "print(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}')\n",
        "\n",
        "# Advanced Query Expansion\n",
        "def expand_query_smart(query, top_n=5):\n",
        "    \"\"\"Smart query expansion using WordNet synonyms\"\"\"\n",
        "    words = query.split()\n",
        "    expanded_terms = set(words)  # Start with original words\n",
        "\n",
        "    for word in words:\n",
        "        # Get synonyms from WordNet\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemma_names():\n",
        "                if '_' not in lemma and len(lemma) > 2:\n",
        "                    synonyms.add(lemma.lower())\n",
        "\n",
        "        # Add top synonyms (shorter ones first for better precision)\n",
        "        sorted_synonyms = sorted(synonyms, key=len)[:top_n]\n",
        "        expanded_terms.update(sorted_synonyms)\n",
        "\n",
        "    # Remove very short or redundant terms\n",
        "    filtered_terms = [term for term in expanded_terms if len(term) > 2]\n",
        "    return ' '.join(filtered_terms)\n",
        "\n",
        "# Apply medical-focused query expansion\n",
        "print('Applying medical-focused query expansion...')\n",
        "queries_df['expanded_query'] = queries_df['cleaned_query'].apply(expand_query_medical)\n",
        "print('Medical query expansion completed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MElD1-8NHwf7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "build_inverted_index",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64ae3dd-694a-4eb3-9de3-7636fdcdceb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimized search structures...\n",
            "Inverted index built with 150000 terms\n"
          ]
        }
      ],
      "source": [
        "def build_optimized_inverted_index(tfidf_matrix, feature_names, doc_ids):\n",
        "    \"\"\"Build an optimized inverted index for fast retrieval\"\"\"\n",
        "    inverted_index = defaultdict(list)\n",
        "    coo_matrix = tfidf_matrix.tocoo()\n",
        "\n",
        "    # Build term -> [(doc_id, score)] mapping\n",
        "    for doc_idx, term_idx, score in zip(coo_matrix.row, coo_matrix.col, coo_matrix.data):\n",
        "        if score > 0:  # Only store non-zero scores\n",
        "            term = feature_names[term_idx]\n",
        "            doc_id = doc_ids[doc_idx]\n",
        "            inverted_index[term].append((doc_id, score))\n",
        "\n",
        "    # Sort each term's document list by score (descending)\n",
        "    for term in inverted_index:\n",
        "        inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return dict(inverted_index)\n",
        "\n",
        "# Build optimized structures\n",
        "print('Building optimized search structures...')\n",
        "doc_ids = docs_df['doc_id'].tolist()\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "inverted_index = build_optimized_inverted_index(tfidf_matrix, feature_names, doc_ids)\n",
        "print(f'Inverted index built with {len(inverted_index)} terms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 7. Model Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cce557e-4621-428b-d98f-e390f5dd9e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test search for: \"what causes knee pain\"\n",
            "Cleaned query: \"what caus knee pain\"\n",
            "Found 5 results\n",
            "  1. Doc 389820_27: 0.8369\n",
            "  2. Doc 3195865_12: 0.5278\n",
            "  3. Doc 3786595_4: 0.4287\n",
            "Search function is working correctly!\n"
          ]
        }
      ],
      "source": [
        "# Simple search function for testing\n",
        "def simple_tfidf_search(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=10):\n",
        "    \"\"\"Simple TF-IDF search for testing purposes\"\"\"\n",
        "    if not query_text or not query_text.strip():\n",
        "        return []\n",
        "\n",
        "    # Transform query\n",
        "    query_vector = tfidf_vectorizer.transform([query_text])\n",
        "\n",
        "    # Calculate similarity\n",
        "    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Get top results\n",
        "    if top_k < len(doc_ids):\n",
        "        top_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
        "        top_indices = top_indices[np.argsort(-scores[top_indices])]\n",
        "    else:\n",
        "        top_indices = np.argsort(-scores)\n",
        "\n",
        "    results = [(doc_ids[i], scores[i]) for i in top_indices if scores[i] > 0]\n",
        "    return results\n",
        "\n",
        "# Test the search function\n",
        "test_query = 'what causes knee pain'\n",
        "cleaned_test_query = text_cleaner.clean_text(test_query)\n",
        "test_results = simple_tfidf_search(\n",
        "    cleaned_test_query,\n",
        "    tfidf_vectorizer,\n",
        "    tfidf_matrix,\n",
        "    doc_ids,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(f'Test search for: \"{test_query}\"')\n",
        "print(f'Cleaned query: \"{cleaned_test_query}\"')\n",
        "print(f'Found {len(test_results)} results')\n",
        "\n",
        "for i, (doc_id, score) in enumerate(test_results[:3]):\n",
        "    print(f'  {i+1}. Doc {doc_id}: {score:.4f}')\n",
        "\n",
        "print('Search function is working correctly!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_evaluation"
      },
      "source": [
        "## 8. Save Models and Prepare for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492deaea-29f8-415d-a31c-ef023ff60ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing optimized search function...\n",
            "Test optimized search results for: \"What causes knee pain and swelling?\"\n",
            "Cleaned query: \"what caus knee pain swell\"\n",
            "Found 10 results\n",
            "  1. Doc ID: 389820_27, Score: 0.6451\n",
            "  2. Doc ID: 3195865_12, Score: 0.4068\n",
            "  3. Doc ID: 513354_2, Score: 0.3491\n",
            "  4. Doc ID: 3786595_4, Score: 0.3304\n",
            "  5. Doc ID: 1658637_4, Score: 0.3092\n",
            "\n",
            "âœ“ Models are ready for evaluation!\n",
            "\n",
            "ðŸ“‹ Next steps:\n",
            "  1. Run the separate evaluation notebook: ANTIQUE_TF-IDF_GPU_Evaluation.ipynb\n",
            "  2. The evaluation notebook will load these saved models\n",
            "  3. GPU-accelerated evaluation will be performed\n",
            "  4. Comprehensive results will be generated\n"
          ]
        }
      ],
      "source": [
        "# Define optimized search function for evaluation notebook\n",
        "def optimized_tfidf_search(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_df, top_k=1000, use_expansion=True, use_rocchio=True):\n",
        "    \"\"\"\n",
        "    Optimized TF-IDF search with advanced query expansion and Rocchio feedback.\n",
        "    This function is saved with the models for use in the evaluation notebook.\n",
        "    \"\"\"\n",
        "    if not query_text or not query_text.strip():\n",
        "        return []\n",
        "\n",
        "    original_query = query_text.strip()\n",
        "\n",
        "    # Initial TF-IDF search\n",
        "    query_vector = tfidf_vectorizer.transform([original_query])\n",
        "    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Query expansion with synonyms\n",
        "    if use_expansion and np.max(scores) > 0:\n",
        "        # Expand with WordNet synonyms\n",
        "        expanded_query = expand_query_smart(original_query, top_n=3)\n",
        "        if expanded_query != original_query:\n",
        "            expanded_vector = tfidf_vectorizer.transform([expanded_query])\n",
        "            expanded_scores = cosine_similarity(expanded_vector, tfidf_matrix).flatten()\n",
        "            # Combine original and expanded scores\n",
        "            scores = 0.7 * scores + 0.3 * expanded_scores\n",
        "\n",
        "    # Get top results\n",
        "    if top_k < len(doc_ids):\n",
        "        top_doc_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
        "        top_doc_indices = top_doc_indices[np.argsort(-scores[top_doc_indices])]\n",
        "    else:\n",
        "        top_doc_indices = np.argsort(-scores)\n",
        "\n",
        "    results = [(doc_ids[i], scores[i]) for i in top_doc_indices if scores[i] > 0]\n",
        "    return results\n",
        "\n",
        "# Test the optimized search function\n",
        "print('Testing optimized search function...')\n",
        "test_query = 'What causes knee pain and swelling?'\n",
        "cleaned_test_query = text_cleaner.clean_text(test_query)\n",
        "test_results = optimized_tfidf_search(\n",
        "    cleaned_test_query,\n",
        "    tfidf_vectorizer=tfidf_vectorizer,\n",
        "    tfidf_matrix=tfidf_matrix,\n",
        "    doc_ids=doc_ids,\n",
        "    docs_df=docs_df,\n",
        "    top_k=10\n",
        ")\n",
        "\n",
        "print(f'Test optimized search results for: \"{test_query}\"')\n",
        "print(f'Cleaned query: \"{cleaned_test_query}\"')\n",
        "print(f'Found {len(test_results)} results')\n",
        "\n",
        "for i, (doc_id, score) in enumerate(test_results[:5]):\n",
        "    print(f'  {i+1}. Doc ID: {doc_id}, Score: {score:.4f}')\n",
        "\n",
        "print('\\nâœ“ Models are ready for evaluation!')\n",
        "print('\\nðŸ“‹ Next steps:')\n",
        "print('  1. Run the separate evaluation notebook: ANTIQUE_TF-IDF_GPU_Evaluation.ipynb')\n",
        "print('  2. The evaluation notebook will load these saved models')\n",
        "print('  3. GPU-accelerated evaluation will be performed')\n",
        "print('  4. Comprehensive results will be generated')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_models"
      },
      "source": [
        "## 8. Save Models and Prepare for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_models_joblib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a9adeb-b524-4cfd-b159-19830fc52580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving optimized models...\n"
          ]
        }
      ],
      "source": [
        "# Define output directory\n",
        "if COLAB_ENV:\n",
        "    output_dir = '/content/drive/MyDrive/tfidf-optimized'\n",
        "else:\n",
        "    output_dir = '/Users/raafatmhanna/Desktop/custom-search-engine/backend/models/tfidf-optimized'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save optimized models and components\n",
        "print('Saving optimized models and components...')\n",
        "\n",
        "# Core TF-IDF components\n",
        "joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
        "joblib.dump(tfidf_matrix, os.path.join(output_dir, 'tfidf_matrix.joblib'))\n",
        "joblib.dump(doc_ids, os.path.join(output_dir, 'doc_ids.joblib'))\n",
        "joblib.dump(inverted_index, os.path.join(output_dir, 'inverted_index.joblib'))\n",
        "\n",
        "# Save text cleaner for consistency\n",
        "joblib.dump(text_cleaner, os.path.join(output_dir, 'text_cleaner.joblib'))\n",
        "\n",
        "# Save preprocessing information\n",
        "preprocessing_info = {\n",
        "    'num_documents': len(docs_df),\n",
        "    'num_queries': len(queries_df),\n",
        "    'vocabulary_size': len(tfidf_vectorizer.vocabulary_),\n",
        "    'tfidf_matrix_shape': tfidf_matrix.shape,\n",
        "    'max_features': tfidf_vectorizer.max_features,\n",
        "    'ngram_range': tfidf_vectorizer.ngram_range,\n",
        "    'max_df': tfidf_vectorizer.max_df,\n",
        "    'min_df': tfidf_vectorizer.min_df\n",
        "}\n",
        "joblib.dump(preprocessing_info, os.path.join(output_dir, 'preprocessing_info.joblib'))\n",
        "\n",
        "# Save cleaned dataframes for evaluation\n",
        "docs_df_minimal = docs_df[['doc_id', 'cleaned_text']].copy()\n",
        "queries_df_minimal = queries_df[['query_id', 'text', 'cleaned_query']].copy()\n",
        "\n",
        "joblib.dump(docs_df_minimal, os.path.join(output_dir, 'docs_df_cleaned.joblib'))\n",
        "joblib.dump(queries_df_minimal, os.path.join(output_dir, 'queries_df_cleaned.joblib'))\n",
        "\n",
        "# Save query expansion function\n",
        "import pickle\n",
        "with open(os.path.join(output_dir, 'expand_query_smart.pkl'), 'wb') as f:\n",
        "    pickle.dump(expand_query_smart, f)\n",
        "\n",
        "# Create model info file\n",
        "model_info = {\n",
        "    'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'model_type': 'TF-IDF with Query Expansion',\n",
        "    'target_dataset': 'ANTIQUE',\n",
        "    'target_map': 0.4,\n",
        "    'preprocessing': {\n",
        "        'text_cleaning': 'OptimizedAntiqueTextCleaner',\n",
        "        'stopwords': 'minimal medical-aware',\n",
        "        'stemming': 'Porter Stemmer',\n",
        "        'lemmatization': 'WordNet Lemmatizer'\n",
        "    },\n",
        "    'tfidf_params': {\n",
        "        'max_features': 150000,\n",
        "        'ngram_range': (1, 2),\n",
        "        'max_df': 0.8,\n",
        "        'min_df': 2,\n",
        "        'norm': 'l2',\n",
        "        'smooth_idf': True,\n",
        "        'sublinear_tf': False\n",
        "    },\n",
        "    'features': [\n",
        "        'Query expansion with WordNet synonyms',\n",
        "        'Medical term normalization',\n",
        "        'Domain-specific preprocessing',\n",
        "        'Inverted index for fast retrieval'\n",
        "    ]\n",
        "}\n",
        "\n",
        "joblib.dump(model_info, os.path.join(output_dir, 'model_info.joblib'))\n",
        "\n",
        "print(f'âœ“ Models saved to: {output_dir}')\n",
        "print('\\nFiles saved:')\n",
        "saved_files = os.listdir(output_dir)\n",
        "for file in sorted(saved_files):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
        "    print(f'  - {file} ({file_size:.1f} MB)')\n",
        "\n",
        "print('\\nðŸŽ¯ Training Complete!')\n",
        "print('=' * 50)\n",
        "print('âœ“ TF-IDF vectorizer trained and saved')\n",
        "print('âœ“ Document corpus processed and indexed')\n",
        "print('âœ“ Query expansion system ready')\n",
        "print('âœ“ All components saved for evaluation')\n",
        "print('\\nðŸ“‹ Next Steps:')\n",
        "print('  1. Open: ANTIQUE_TF-IDF_GPU_Evaluation.ipynb')\n",
        "print('  2. Enable GPU runtime in Colab')\n",
        "print('  3. Run the evaluation notebook')\n",
        "print('  4. Review comprehensive results')\n",
        "\n",
        "print(f'Model directory: {output_dir}')\n",
        "print(f'Total files: {len(saved_files)}')\n",
        "print(f'Ready for GPU-accelerated evaluation! ðŸš€')\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b55118",
        "outputId": "adf844e0-0536-45f0-c1af-14a1a3c5ba8d"
      },
      "source": [
        "def search_documents_enhanced(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_df, top_k=1000, use_feedback=True):\n",
        "    \"\"\"\n",
        "    Enhanced search with pseudo-relevance feedback and multiple ranking strategies.\n",
        "    \"\"\"\n",
        "    if not query_text:\n",
        "        return []\n",
        "\n",
        "    # Original query search\n",
        "    query_tfidf = tfidf_vectorizer.transform([query_text])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "    # Pseudo-relevance feedback\n",
        "    if use_feedback:\n",
        "        # Get top 5 documents for feedback\n",
        "        top_feedback_indices = np.argsort(-cosine_similarities)[:5]\n",
        "        feedback_docs = []\n",
        "        for idx in top_feedback_indices:\n",
        "            doc_id = doc_ids[idx]\n",
        "            doc_text = docs_df[docs_df['doc_id'] == doc_id]['cleaned_text'].iloc[0]\n",
        "            feedback_docs.append(doc_text)\n",
        "\n",
        "        # Expand query with feedback terms\n",
        "        feedback_text = ' '.join(feedback_docs)\n",
        "        expanded_query = query_text + ' ' + feedback_text\n",
        "\n",
        "        # Re-search with expanded query\n",
        "        expanded_query_tfidf = tfidf_vectorizer.transform([expanded_query])\n",
        "        expanded_similarities = cosine_similarity(expanded_query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "        # Combine original and expanded similarities\n",
        "        final_similarities = 0.7 * cosine_similarities + 0.3 * expanded_similarities\n",
        "    else:\n",
        "        final_similarities = cosine_similarities\n",
        "\n",
        "    # Get top results\n",
        "    if top_k < len(doc_ids):\n",
        "        top_doc_indices = np.argpartition(final_similarities, -top_k)[-top_k:]\n",
        "        top_doc_indices = top_doc_indices[np.argsort(-final_similarities[top_doc_indices])]\n",
        "    else:\n",
        "        top_doc_indices = np.argsort(-final_similarities)\n",
        "\n",
        "    results = [(doc_ids[i], final_similarities[i]) for i in top_doc_indices]\n",
        "    return results\n",
        "\n",
        "# Backward compatibility function\n",
        "def search_documents(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=1000):\n",
        "    return search_documents_enhanced(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_df, top_k, use_feedback=True)\n",
        "\n",
        "# Test enhanced search\n",
        "test_query = \"what are the causes of severe knee pain\"\n",
        "test_results = search_documents_enhanced(\n",
        "    text_cleaner.clean_text(test_query),\n",
        "    tfidf_vectorizer=tfidf_vectorizer,\n",
        "    tfidf_matrix=tfidf_matrix,\n",
        "    doc_ids=doc_ids,\n",
        "    docs_df=docs_df,\n",
        "    top_k=10\n",
        ")\n",
        "print(f\"Enhanced search results for query '{test_query}':\")\n",
        "for doc_id, score in test_results:\n",
        "    print(f\"  Doc ID: {doc_id}, Score: {score:.4f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced search results for query 'what are the causes of severe knee pain':\n",
            "  Doc ID: 389820_27, Score: 0.6975\n",
            "  Doc ID: 1658637_4, Score: 0.4953\n",
            "  Doc ID: 3195865_12, Score: 0.4694\n",
            "  Doc ID: 3786595_4, Score: 0.3968\n",
            "  Doc ID: 2988484_2, Score: 0.3089\n",
            "  Doc ID: 2105586_5, Score: 0.2972\n",
            "  Doc ID: 3963590_4, Score: 0.2965\n",
            "  Doc ID: 437018_9, Score: 0.2743\n",
            "  Doc ID: 3485210_1, Score: 0.2715\n",
            "  Doc ID: 2105586_3, Score: 0.2614\n",
            "Enhanced search results for query 'what are the causes of severe knee pain':\n",
            "  Doc ID: 389820_27, Score: 0.6975\n",
            "  Doc ID: 1658637_4, Score: 0.4953\n",
            "  Doc ID: 3195865_12, Score: 0.4694\n",
            "  Doc ID: 3786595_4, Score: 0.3968\n",
            "  Doc ID: 2988484_2, Score: 0.3089\n",
            "  Doc ID: 2105586_5, Score: 0.2972\n",
            "  Doc ID: 3963590_4, Score: 0.2965\n",
            "  Doc ID: 437018_9, Score: 0.2743\n",
            "  Doc ID: 3485210_1, Score: 0.2715\n",
            "  Doc ID: 2105586_3, Score: 0.2614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ef1046",
        "outputId": "6ef3b10d-647c-40f7-a01b-35c5a380dabf"
      },
      "source": [
        "def calculate_average_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculates the Average Precision (AP) for a single query.\n",
        "\n",
        "    Args:\n",
        "        retrieved_docs (list): A list of retrieved document IDs in ranked order.\n",
        "        relevant_docs (set): A set of relevant document IDs for the query.\n",
        "\n",
        "    Returns:\n",
        "        float: The Average Precision (AP) for the query.\n",
        "    \"\"\"\n",
        "    if not relevant_docs:\n",
        "        return 0.0\n",
        "\n",
        "    hits = 0\n",
        "    sum_precisions = 0.0\n",
        "    for i, doc_id in enumerate(retrieved_docs):\n",
        "        if doc_id in relevant_docs:\n",
        "            hits += 1\n",
        "            precision_at_k = hits / (i + 1)\n",
        "            sum_precisions += precision_at_k\n",
        "\n",
        "    return sum_precisions / len(relevant_docs)\n",
        "\n",
        "# Add a test case for calculate_average_precision\n",
        "test_retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']\n",
        "test_relevant = {'doc1', 'doc3', 'doc5'}\n",
        "test_ap = calculate_average_precision(test_retrieved, test_relevant)\n",
        "print(f\"Test AP: {test_ap:.4f}\")\n",
        "\n",
        "test_retrieved_2 = ['doc1', 'doc2', 'doc4']\n",
        "test_relevant_2 = {'doc3', 'doc5'}\n",
        "test_ap_2 = calculate_average_precision(test_retrieved_2, test_relevant_2)\n",
        "print(f\"Test AP 2: {test_ap_2:.4f}\")\n",
        "\n",
        "test_retrieved_3 = []\n",
        "test_relevant_3 = {'doc1', 'doc2'}\n",
        "test_ap_3 = calculate_average_precision(test_retrieved_3, test_relevant_3)\n",
        "print(f\"Test AP 3: {test_ap_3:.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AP: 0.7556\n",
            "Test AP 2: 0.0000\n",
            "Test AP 3: 0.0000\n",
            "Test AP: 0.7556\n",
            "Test AP 2: 0.0000\n",
            "Test AP 3: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comprehensive_evaluation"
      },
      "source": [
        "## 10. Comprehensive Evaluation - MAP, MPR, and Precision\n",
        "\n",
        "This section performs a comprehensive evaluation of the TF-IDF system without using the inverted index.\n",
        "We'll calculate Mean Average Precision (MAP), Mean Precision at Recall (MPR), and Precision at various cut-offs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "evaluation_metrics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e25946a-5e6c-487e-de87-bcca4af6d004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Evaluation metrics functions defined\n",
            "âœ“ Evaluation metrics functions defined\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def calculate_precision_at_k(retrieved_docs, relevant_docs, k):\n",
        "    \"\"\"Calculate precision at k\"\"\"\n",
        "    if k == 0 or len(retrieved_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_retrieved = 0\n",
        "    for i, doc_id in enumerate(retrieved_docs[:k]):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "\n",
        "    return relevant_retrieved / min(k, len(retrieved_docs))\n",
        "\n",
        "def calculate_recall_at_k(retrieved_docs, relevant_docs, k):\n",
        "    \"\"\"Calculate recall at k\"\"\"\n",
        "    if len(relevant_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_retrieved = 0\n",
        "    for i, doc_id in enumerate(retrieved_docs[:k]):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "\n",
        "    return relevant_retrieved / len(relevant_docs)\n",
        "\n",
        "def calculate_average_precision_improved(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision (AP) for a single query.\n",
        "    Improved version with better handling of edge cases.\n",
        "    \"\"\"\n",
        "    if not relevant_docs or len(relevant_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    if not retrieved_docs or len(retrieved_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_retrieved = 0\n",
        "    sum_precisions = 0.0\n",
        "\n",
        "    for i, doc_id in enumerate(retrieved_docs):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "            precision_at_i = relevant_retrieved / (i + 1)\n",
        "            sum_precisions += precision_at_i\n",
        "\n",
        "    if relevant_retrieved == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return sum_precisions / len(relevant_docs)\n",
        "\n",
        "def calculate_mean_precision_at_recall(retrieved_docs, relevant_docs, recall_levels=None):\n",
        "    \"\"\"\n",
        "    Calculate Mean Precision at Recall (MPR) for standard recall levels.\n",
        "    \"\"\"\n",
        "    if recall_levels is None:\n",
        "        recall_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\n",
        "    if not relevant_docs or len(relevant_docs) == 0:\n",
        "        return {level: 0.0 for level in recall_levels}\n",
        "\n",
        "    if not retrieved_docs or len(retrieved_docs) == 0:\n",
        "        return {level: 0.0 for level in recall_levels}\n",
        "\n",
        "    # Calculate precision and recall at each position\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    relevant_retrieved = 0\n",
        "\n",
        "    for i, doc_id in enumerate(retrieved_docs):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "\n",
        "        precision = relevant_retrieved / (i + 1)\n",
        "        recall = relevant_retrieved / len(relevant_docs)\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    # Interpolate precision at recall levels\n",
        "    precision_at_recall = {}\n",
        "    for recall_level in recall_levels:\n",
        "        max_precision = 0.0\n",
        "        for i, recall in enumerate(recalls):\n",
        "            if recall >= recall_level:\n",
        "                max_precision = max(max_precision, precisions[i])\n",
        "        precision_at_recall[recall_level] = max_precision\n",
        "\n",
        "    return precision_at_recall\n",
        "\n",
        "print('âœ“ Evaluation metrics functions defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "prepare_evaluation_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b4af50-badb-44c0-92f4-89bc9550e138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing evaluation data...\n",
            "Total queries: 2426\n",
            "Queries with relevance judgments: 2426\n",
            "Total relevance judgments: 27422\n",
            "Sampling 100 queries for evaluation\n",
            "Evaluation data prepared successfully!\n",
            "Preparing evaluation data...\n",
            "Total queries: 2426\n",
            "Queries with relevance judgments: 2426\n",
            "Total relevance judgments: 27422\n",
            "Sampling 100 queries for evaluation\n",
            "Evaluation data prepared successfully!\n"
          ]
        }
      ],
      "source": [
        "# Prepare evaluation data\n",
        "print('Preparing evaluation data...')\n",
        "\n",
        "# Create relevance judgments dictionary\n",
        "relevance_judgments = defaultdict(set)\n",
        "for _, row in qrels_df.iterrows():\n",
        "    query_id = row['query_id']\n",
        "    doc_id = row['doc_id']\n",
        "    relevance = row['relevance']\n",
        "\n",
        "    # Consider relevance >= 1 as relevant (adjust threshold as needed)\n",
        "    if relevance >= 1:\n",
        "        relevance_judgments[query_id].add(doc_id)\n",
        "\n",
        "# Get queries that have relevance judgments\n",
        "evaluated_queries = set(relevance_judgments.keys())\n",
        "queries_with_judgments = queries_df[queries_df['query_id'].isin(evaluated_queries)].copy()\n",
        "\n",
        "print(f'Total queries: {len(queries_df)}')\n",
        "print(f'Queries with relevance judgments: {len(queries_with_judgments)}')\n",
        "print(f'Total relevance judgments: {len(qrels_df)}')\n",
        "# print(f'Unique relevant documents: {len(set(qrels_df['doc_id'].values))}')\n",
        "\n",
        "# Sample queries for evaluation (use all if manageable, otherwise sample)\n",
        "max_eval_queries = 100  # Limit for demonstration - increase as needed\n",
        "if len(queries_with_judgments) > max_eval_queries:\n",
        "    eval_queries = queries_with_judgments.sample(n=max_eval_queries, random_state=42)\n",
        "    print(f'Sampling {max_eval_queries} queries for evaluation')\n",
        "else:\n",
        "    eval_queries = queries_with_judgments\n",
        "    print(f'Using all {len(eval_queries)} queries for evaluation')\n",
        "\n",
        "print('Evaluation data prepared successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "run_comprehensive_evaluation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261be0c4-2a9b-470b-ceff-d96b3d2ce26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting comprehensive evaluation...\n",
            "============================================================\n",
            "Evaluating 100 queries...\n",
            "Progress: 10/100 (10.0%) - Elapsed: 16.0s, Remaining: 143.8s, Current MAP: 0.0298\n",
            "Progress: 20/100 (20.0%) - Elapsed: 38.2s, Remaining: 152.7s, Current MAP: 0.0209\n",
            "Progress: 30/100 (30.0%) - Elapsed: 50.5s, Remaining: 117.9s, Current MAP: 0.0393\n",
            "Progress: 40/100 (40.0%) - Elapsed: 62.9s, Remaining: 94.4s, Current MAP: 0.0517\n",
            "Progress: 50/100 (50.0%) - Elapsed: 75.4s, Remaining: 75.4s, Current MAP: 0.0574\n",
            "Progress: 60/100 (60.0%) - Elapsed: 87.8s, Remaining: 58.6s, Current MAP: 0.0510\n",
            "Progress: 70/100 (70.0%) - Elapsed: 99.9s, Remaining: 42.8s, Current MAP: 0.0458\n",
            "Progress: 80/100 (80.0%) - Elapsed: 111.9s, Remaining: 28.0s, Current MAP: 0.0412\n",
            "Progress: 90/100 (90.0%) - Elapsed: 124.5s, Remaining: 13.8s, Current MAP: 0.0405\n",
            "Progress: 100/100 (100.0%) - Elapsed: 138.2s, Remaining: 0.0s, Current MAP: 0.0427\n",
            "Evaluation completed in 138.20 seconds\n",
            "Successfully evaluated 100 queries\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive evaluation\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import numpy as np  # Import numpy\n",
        "from sklearn.metrics.pairwise import cosine_similarity # Import cosine_similarity\n",
        "\n",
        "\n",
        "def search_documents_enhanced(query_text, tfidf_vectorizer, tfidf_matrix, doc_ids, docs_df, top_k=1000, use_feedback=True):\n",
        "    \"\"\"\n",
        "    Enhanced search with pseudo-relevance feedback and multiple ranking strategies.\n",
        "    \"\"\"\n",
        "    if not query_text:\n",
        "        return []\n",
        "\n",
        "    # Original query search\n",
        "    query_tfidf = tfidf_vectorizer.transform([query_text])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "    # Pseudo-relevance feedback\n",
        "    if use_feedback:\n",
        "        # Get top 5 documents for feedback\n",
        "        top_feedback_indices = np.argsort(-cosine_similarities)[:5]\n",
        "        feedback_docs = []\n",
        "        for idx in top_feedback_indices:\n",
        "            doc_id = doc_ids[idx]\n",
        "            # Ensure doc_id exists in docs_df before accessing 'cleaned_text'\n",
        "            doc_row = docs_df[docs_df['doc_id'] == doc_id]\n",
        "            if not doc_row.empty:\n",
        "                doc_text = doc_row['cleaned_text'].iloc[0]\n",
        "                feedback_docs.append(doc_text)\n",
        "            # else:\n",
        "                # Handle case where doc_id is not found if necessary\n",
        "\n",
        "        # Expand query with feedback terms\n",
        "        feedback_text = ' '.join(feedback_docs)\n",
        "        expanded_query = query_text + ' ' + feedback_text\n",
        "\n",
        "        # Re-search with expanded query\n",
        "        expanded_query_tfidf = tfidf_vectorizer.transform([expanded_query])\n",
        "        expanded_similarities = cosine_similarity(expanded_query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "        # Combine original and expanded similarities\n",
        "        final_similarities = 0.7 * cosine_similarities + 0.3 * expanded_similarities\n",
        "    else:\n",
        "        final_similarities = cosine_similarities\n",
        "\n",
        "    # Get top results\n",
        "    if top_k < len(doc_ids):\n",
        "        top_doc_indices = np.argpartition(final_similarities, -top_k)[-top_k:]\n",
        "        top_doc_indices = top_doc_indices[np.argsort(-final_similarities[top_doc_indices])]\n",
        "    else:\n",
        "        top_doc_indices = np.argsort(-final_similarities)\n",
        "\n",
        "    results = [(doc_ids[i], final_similarities[i]) for i in top_doc_indices]\n",
        "    return results\n",
        "\n",
        "\n",
        "print('ðŸš€ Starting comprehensive evaluation...')\n",
        "print('=' * 60)\n",
        "\n",
        "# Initialize result storage\n",
        "evaluation_results = {\n",
        "    'average_precisions': [],\n",
        "    'precision_at_k': {k: [] for k in [1, 3, 5, 10, 20, 50, 100]},\n",
        "    'recall_at_k': {k: [] for k in [1, 3, 5, 10, 20, 50, 100]},\n",
        "    'precision_at_recall': {level: [] for level in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
        "    'query_details': []\n",
        "}\n",
        "\n",
        "# Track evaluation progress\n",
        "start_time = time.time()\n",
        "processed_queries = 0\n",
        "total_queries = len(eval_queries)\n",
        "\n",
        "print(f'Evaluating {total_queries} queries...')\n",
        "\n",
        "# Process each query\n",
        "for idx, (_, query_row) in enumerate(eval_queries.iterrows()):\n",
        "    query_id = query_row['query_id']\n",
        "    query_text = query_row['text']\n",
        "    cleaned_query = query_row['cleaned_query']\n",
        "\n",
        "    # Get relevant documents for this query\n",
        "    relevant_docs = relevance_judgments[query_id]\n",
        "\n",
        "    if len(relevant_docs) == 0:\n",
        "        continue\n",
        "\n",
        "    # Search using direct TF-IDF (no inverted index)\n",
        "    search_results = search_documents_enhanced(\n",
        "        cleaned_query,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        doc_ids=doc_ids,\n",
        "        docs_df=docs_df,\n",
        "        top_k=1000,  # Get top 1000 documents\n",
        "        use_feedback=True\n",
        "    )\n",
        "\n",
        "    # Extract document IDs from search results\n",
        "    retrieved_docs = [doc_id for doc_id, score in search_results if score > 0]\n",
        "\n",
        "    if len(retrieved_docs) == 0:\n",
        "        continue\n",
        "\n",
        "    # Calculate metrics\n",
        "    # 1. Average Precision\n",
        "    ap = calculate_average_precision_improved(retrieved_docs, relevant_docs)\n",
        "    evaluation_results['average_precisions'].append(ap)\n",
        "\n",
        "    # 2. Precision at K\n",
        "    for k in evaluation_results['precision_at_k'].keys():\n",
        "        prec_k = calculate_precision_at_k(retrieved_docs, relevant_docs, k)\n",
        "        evaluation_results['precision_at_k'][k].append(prec_k)\n",
        "\n",
        "    # 3. Recall at K\n",
        "    for k in evaluation_results['recall_at_k'].keys():\n",
        "        rec_k = calculate_recall_at_k(retrieved_docs, relevant_docs, k)\n",
        "        evaluation_results['recall_at_k'][k].append(rec_k)\n",
        "\n",
        "    # 4. Precision at Recall levels\n",
        "    prec_at_recall = calculate_mean_precision_at_recall(retrieved_docs, relevant_docs)\n",
        "    for level in evaluation_results['precision_at_recall'].keys():\n",
        "        evaluation_results['precision_at_recall'][level].append(prec_at_recall[level])\n",
        "\n",
        "    # Store query details\n",
        "    evaluation_results['query_details'].append({\n",
        "        'query_id': query_id,\n",
        "        'query_text': query_text,\n",
        "        'cleaned_query': cleaned_query,\n",
        "        'num_relevant': len(relevant_docs),\n",
        "        'num_retrieved': len(retrieved_docs),\n",
        "        'average_precision': ap,\n",
        "        'precision_at_10': calculate_precision_at_k(retrieved_docs, relevant_docs, 10),\n",
        "        'recall_at_10': calculate_recall_at_k(retrieved_docs, relevant_docs, 10)\n",
        "    })\n",
        "\n",
        "    processed_queries += 1\n",
        "\n",
        "    # Progress update\n",
        "    if processed_queries % 10 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        avg_time = elapsed / processed_queries\n",
        "        remaining = (total_queries - processed_queries) * avg_time\n",
        "        # Calculate current MAP only if there are average precisions recorded\n",
        "        current_map = np.mean(evaluation_results[\"average_precisions\"]) if evaluation_results[\"average_precisions\"] else 0.0\n",
        "        print(f'Progress: {processed_queries}/{total_queries} ({processed_queries/total_queries*100:.1f}%) - '\n",
        "              f'Elapsed: {elapsed:.1f}s, Remaining: {remaining:.1f}s, Current MAP: {current_map:.4f}')\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f'Evaluation completed in {total_time:.2f} seconds')\n",
        "print(f'Successfully evaluated {processed_queries} queries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "display_evaluation_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d181638-679b-49f3-f269-c6bfc8e67c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ“Š COMPREHENSIVE EVALUATION RESULTS\n",
            "================================================================================\n",
            "ðŸŽ¯ MEAN AVERAGE PRECISION (MAP): 0.0427\n",
            "âŒ Target not reached. MAP 0.0427 < 0.3\n",
            "   Need improvement of 0.2573 points\n",
            "ðŸ“ˆ PRECISION AT K:\n",
            "   P@ 1: 0.1300\n",
            "   P@ 3: 0.0800\n",
            "   P@ 5: 0.0600\n",
            "   P@10: 0.0490\n",
            "   P@20: 0.0390\n",
            "   P@50: 0.0248\n",
            "   P@100: 0.0175\n",
            "ðŸ“‰ RECALL AT K:\n",
            "   R@ 1: 0.0160\n",
            "   R@ 3: 0.0245\n",
            "   R@ 5: 0.0280\n",
            "   R@10: 0.0463\n",
            "   R@20: 0.0737\n",
            "   R@50: 0.1255\n",
            "   R@100: 0.1878\n",
            "ðŸ”„ MEAN PRECISION AT RECALL LEVELS (MPR):\n",
            "   MPR@0.1: 0.1373\n",
            "   MPR@0.2: 0.0834\n",
            "   MPR@0.3: 0.0565\n",
            "   MPR@0.4: 0.0419\n",
            "   MPR@0.5: 0.0333\n",
            "   MPR@0.6: 0.0103\n",
            "   MPR@0.7: 0.0061\n",
            "   MPR@0.8: 0.0033\n",
            "   MPR@0.9: 0.0019\n",
            "   MPR@1.0: 0.0015\n",
            "OVERALL MEAN PRECISION AT RECALL (MPR): 0.0376\n",
            "ðŸ“Š ADDITIONAL STATISTICS:\n",
            "   Queries evaluated: 100\n",
            "   Average relevant docs per query: 10.4\n",
            "   MAP Standard Deviation: 0.0958\n",
            "   MAP Min: 0.0000\n",
            "   MAP Max: 0.5400\n",
            "   MAP Median: 0.0060\n",
            "ðŸ† PERFORMANCE BREAKDOWN:\n",
            "   High Performance (AP >= 0.5): 2 queries (2.0%)\n",
            "   Medium Performance (0.2 <= AP < 0.5): 5 queries (5.0%)\n",
            "   Low Performance (AP < 0.2): 93 queries (93.0%)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate and display comprehensive results\n",
        "print('' + '=' * 80)\n",
        "print('ðŸ“Š COMPREHENSIVE EVALUATION RESULTS')\n",
        "print('=' * 80)\n",
        "\n",
        "# Mean Average Precision (MAP)\n",
        "map_score = np.mean(evaluation_results['average_precisions'])\n",
        "print(f'ðŸŽ¯ MEAN AVERAGE PRECISION (MAP): {map_score:.4f}')\n",
        "\n",
        "# Check if MAP target is achieved\n",
        "target_map = 0.3\n",
        "if map_score >= target_map:\n",
        "    print(f'âœ… TARGET ACHIEVED! MAP {map_score:.4f} >= {target_map}')\n",
        "else:\n",
        "    print(f'âŒ Target not reached. MAP {map_score:.4f} < {target_map}')\n",
        "    print(f'   Need improvement of {target_map - map_score:.4f} points')\n",
        "\n",
        "# Precision at K\n",
        "print('ðŸ“ˆ PRECISION AT K:')\n",
        "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
        "    if evaluation_results['precision_at_k'][k]:\n",
        "        prec_k = np.mean(evaluation_results['precision_at_k'][k])\n",
        "        print(f'   P@{k:2d}: {prec_k:.4f}')\n",
        "\n",
        "# Recall at K\n",
        "print('ðŸ“‰ RECALL AT K:')\n",
        "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
        "    if evaluation_results['recall_at_k'][k]:\n",
        "        rec_k = np.mean(evaluation_results['recall_at_k'][k])\n",
        "        print(f'   R@{k:2d}: {rec_k:.4f}')\n",
        "\n",
        "# Mean Precision at Recall (MPR)\n",
        "print('ðŸ”„ MEAN PRECISION AT RECALL LEVELS (MPR):')\n",
        "mpr_values = []\n",
        "for level in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "    if evaluation_results['precision_at_recall'][level]:\n",
        "        mpr_level = np.mean(evaluation_results['precision_at_recall'][level])\n",
        "        mpr_values.append(mpr_level)\n",
        "        print(f'   MPR@{level:.1f}: {mpr_level:.4f}')\n",
        "\n",
        "# Overall MPR\n",
        "if mpr_values:\n",
        "    overall_mpr = np.mean(mpr_values)\n",
        "    print(f'OVERALL MEAN PRECISION AT RECALL (MPR): {overall_mpr:.4f}')\n",
        "\n",
        "# Additional Statistics\n",
        "print('ðŸ“Š ADDITIONAL STATISTICS:')\n",
        "print(f'   Queries evaluated: {len(evaluation_results[\"average_precisions\"])}')\n",
        "print(f'   Average relevant docs per query: {np.mean([len(relevance_judgments[q]) for q in eval_queries[\"query_id\"]]):,.1f}')\n",
        "print(f'   MAP Standard Deviation: {np.std(evaluation_results[\"average_precisions\"]):.4f}')\n",
        "print(f'   MAP Min: {np.min(evaluation_results[\"average_precisions\"]):.4f}')\n",
        "print(f'   MAP Max: {np.max(evaluation_results[\"average_precisions\"]):.4f}')\n",
        "print(f'   MAP Median: {np.median(evaluation_results[\"average_precisions\"]):.4f}')\n",
        "\n",
        "# Performance breakdown\n",
        "high_perf_queries = [ap for ap in evaluation_results['average_precisions'] if ap >= 0.5]\n",
        "med_perf_queries = [ap for ap in evaluation_results['average_precisions'] if 0.2 <= ap < 0.5]\n",
        "low_perf_queries = [ap for ap in evaluation_results['average_precisions'] if ap < 0.2]\n",
        "\n",
        "print('ðŸ† PERFORMANCE BREAKDOWN:')\n",
        "print(f'   High Performance (AP >= 0.5): {len(high_perf_queries)} queries ({len(high_perf_queries)/len(evaluation_results[\"average_precisions\"])*100:.1f}%)')\n",
        "print(f'   Medium Performance (0.2 <= AP < 0.5): {len(med_perf_queries)} queries ({len(med_perf_queries)/len(evaluation_results[\"average_precisions\"])*100:.1f}%)')\n",
        "print(f'   Low Performance (AP < 0.2): {len(low_perf_queries)} queries ({len(low_perf_queries)/len(evaluation_results[\"average_precisions\"])*100:.1f}%)')\n",
        "\n",
        "print('=' * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "analyze_top_queries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1054b714-1a83-4d28-8891-5ca413bf0145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” QUERY ANALYSIS:\n",
            "==================================================\n",
            "ðŸ† TOP 5 PERFORMING QUERIES:\n",
            "1. Query ID: 2620160, AP: 0.5400\n",
            "   Text: \"how do you adjust the shore hardness of PVC tubing?...\"\n",
            "   Relevant docs: 2, Retrieved: 1000, P@10: 0.1000\n",
            "2. Query ID: 1530148, AP: 0.5068\n",
            "   Text: \"what is persepolis?...\"\n",
            "   Relevant docs: 8, Retrieved: 1000, P@10: 0.4000\n",
            "3. Query ID: 1548552, AP: 0.3422\n",
            "   Text: \"why do yanks call a chocolate bar a candy bar?...\"\n",
            "   Relevant docs: 26, Retrieved: 1000, P@10: 0.8000\n",
            "4. Query ID: 3541329, AP: 0.3078\n",
            "   Text: \"What is \"marbling\" in terms of cooking a steak?...\"\n",
            "   Relevant docs: 15, Retrieved: 1000, P@10: 0.4000\n",
            "5. Query ID: 3171095, AP: 0.2785\n",
            "   Text: \"Why when someone is deaf and blind did they also call them dumb?...\"\n",
            "   Relevant docs: 7, Retrieved: 1000, P@10: 0.2000\n",
            "âŒ BOTTOM 5 PERFORMING QUERIES:\n",
            "1. Query ID: 3780692, AP: 0.0000\n",
            "   Text: \"how do you send pictures to the \"PSP\"?...\"\n",
            "   Relevant docs: 2, Retrieved: 1000, P@10: 0.0000\n",
            "2. Query ID: 3013959, AP: 0.0000\n",
            "   Text: \"how can i get online account information on my debit visa credit card?...\"\n",
            "   Relevant docs: 2, Retrieved: 1000, P@10: 0.0000\n",
            "3. Query ID: 498337, AP: 0.0000\n",
            "   Text: \"Is there anything i can feed to a baby duck besides chick starter?...\"\n",
            "   Relevant docs: 6, Retrieved: 1000, P@10: 0.0000\n",
            "4. Query ID: 916180, AP: 0.0000\n",
            "   Text: \"What is my cat doing?...\"\n",
            "   Relevant docs: 18, Retrieved: 1000, P@10: 0.0000\n",
            "5. Query ID: 808428, AP: 0.0000\n",
            "   Text: \"how will u celebrate ure quincenera?...\"\n",
            "   Relevant docs: 2, Retrieved: 1000, P@10: 0.0000\n",
            "âš ï¸  QUERIES WITH ZERO AP: 16 (16.0%)\n",
            "ðŸ“‹ EVALUATION PROCESS SUMMARY:\n",
            "========================================\n",
            "âœ… Evaluation completed without using inverted index\n",
            "âœ… Direct TF-IDF cosine similarity search used\n",
            "âœ… Pseudo-relevance feedback applied\n",
            "âœ… Query expansion with synonyms enabled\n",
            "âœ… Comprehensive metrics calculated:\n",
            "   - Mean Average Precision (MAP)\n",
            "   - Precision at K (P@K)\n",
            "   - Recall at K (R@K)\n",
            "   - Mean Precision at Recall (MPR)\n",
            "ðŸ”§ IMPROVEMENT NEEDED:\n",
            "   Current MAP: 0.0427\n",
            "   Target MAP: 0.3000\n",
            "   Gap: -0.2573\n",
            "SUGGESTED IMPROVEMENTS:\n",
            "   1. Fine-tune query expansion parameters\n",
            "   2. Adjust TF-IDF parameters (ngram_range, max_df, min_df)\n",
            "   3. Implement BM25 scoring instead of TF-IDF\n",
            "   4. Add more sophisticated text preprocessing\n",
            "   5. Use semantic embeddings (BERT, etc.)\n",
            "================================================================================\n",
            "ðŸ EVALUATION COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Analyze top and bottom performing queries\n",
        "print('ðŸ” QUERY ANALYSIS:')\n",
        "print('=' * 50)\n",
        "\n",
        "# Sort queries by performance\n",
        "query_performance = sorted(evaluation_results['query_details'], key=lambda x: x['average_precision'], reverse=True)\n",
        "\n",
        "# Top 5 performing queries\n",
        "print('ðŸ† TOP 5 PERFORMING QUERIES:')\n",
        "for i, query_info in enumerate(query_performance[:5]):\n",
        "    print(f'{i+1}. Query ID: {query_info[\"query_id\"]}, AP: {query_info[\"average_precision\"]:.4f}')\n",
        "    print(f'   Text: \"{query_info[\"query_text\"][:100]}...\"')\n",
        "    print(f'   Relevant docs: {query_info[\"num_relevant\"]}, Retrieved: {query_info[\"num_retrieved\"]}, P@10: {query_info[\"precision_at_10\"]:.4f}')\n",
        "\n",
        "# Bottom 5 performing queries\n",
        "print('âŒ BOTTOM 5 PERFORMING QUERIES:')\n",
        "for i, query_info in enumerate(query_performance[-5:]):\n",
        "    print(f'{i+1}. Query ID: {query_info[\"query_id\"]}, AP: {query_info[\"average_precision\"]:.4f}')\n",
        "    print(f'   Text: \"{query_info[\"query_text\"][:100]}...\"')\n",
        "    print(f'   Relevant docs: {query_info[\"num_relevant\"]}, Retrieved: {query_info[\"num_retrieved\"]}, P@10: {query_info[\"precision_at_10\"]:.4f}')\n",
        "\n",
        "# Queries with zero AP\n",
        "zero_ap_queries = [q for q in query_performance if q['average_precision'] == 0.0]\n",
        "print(f'âš ï¸  QUERIES WITH ZERO AP: {len(zero_ap_queries)} ({len(zero_ap_queries)/len(query_performance)*100:.1f}%)')\n",
        "\n",
        "# Analysis summary\n",
        "print('ðŸ“‹ EVALUATION PROCESS SUMMARY:')\n",
        "print('=' * 40)\n",
        "print('âœ… Evaluation completed without using inverted index')\n",
        "print('âœ… Direct TF-IDF cosine similarity search used')\n",
        "print('âœ… Pseudo-relevance feedback applied')\n",
        "print('âœ… Query expansion with synonyms enabled')\n",
        "print('âœ… Comprehensive metrics calculated:')\n",
        "print('   - Mean Average Precision (MAP)')\n",
        "print('   - Precision at K (P@K)')\n",
        "print('   - Recall at K (R@K)')\n",
        "print('   - Mean Precision at Recall (MPR)')\n",
        "\n",
        "if map_score >= target_map:\n",
        "    print('ðŸŽ‰ SUCCESS: Target MAP achieved!')\n",
        "    print(f'   Current MAP: {map_score:.4f}')\n",
        "    print(f'   Target MAP: {target_map:.4f}')\n",
        "    print(f'   Improvement: +{map_score - target_map:.4f}')\n",
        "else:\n",
        "    print('ðŸ”§ IMPROVEMENT NEEDED:')\n",
        "    print(f'   Current MAP: {map_score:.4f}')\n",
        "    print(f'   Target MAP: {target_map:.4f}')\n",
        "    print(f'   Gap: -{target_map - map_score:.4f}')\n",
        "    print('SUGGESTED IMPROVEMENTS:')\n",
        "    print('   1. Fine-tune query expansion parameters')\n",
        "    print('   2. Adjust TF-IDF parameters (ngram_range, max_df, min_df)')\n",
        "    print('   3. Implement BM25 scoring instead of TF-IDF')\n",
        "    print('   4. Add more sophisticated text preprocessing')\n",
        "    print('   5. Use semantic embeddings (BERT, etc.)')\n",
        "\n",
        "print('=' * 80)\n",
        "print('ðŸ EVALUATION COMPLETE')\n",
        "print('=' * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a05e2a8d"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "evaluation"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}