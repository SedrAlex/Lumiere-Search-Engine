{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgYnDjCa8vP5",
        "outputId": "62221655-8074-46ba-e5fc-24923f09de42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect) (4.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install nltk scikit-learn pandas numpy joblib contractions beautifulsoup4 inflect\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2ESaam18vP8"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer # Corrected import statement\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "import contractions\n",
        "import inflect\n",
        "import joblib\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQLCLu538vP-",
        "outputId": "528096ae-a432-40ab-c108-46d3c3dff636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Yio4w28vP_"
      },
      "outputs": [],
      "source": [
        "# Text Processing Class\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = nltk.tokenize\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.inflect_engine = inflect.engine()\n",
        "\n",
        "    def clean_text(self, text, words_to_remove):\n",
        "        words = text.split()\n",
        "        cleaned_words = [word for word in words if word not in words_to_remove]\n",
        "        cleaned_text = ' '.join(cleaned_words)\n",
        "        return cleaned_text\n",
        "\n",
        "    def number_to_words(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        converted_words = []\n",
        "        for word in words:\n",
        "            if word.replace('.', '', 1).isdigit():\n",
        "                converted_words.append(word)\n",
        "            else:\n",
        "                if word.isdigit():\n",
        "                    try:\n",
        "                        num = int(word)\n",
        "                        if num <= 999999999999999:\n",
        "                            converted_word = self.inflect_engine.number_to_words(word)\n",
        "                            converted_words.append(converted_word)\n",
        "                        else:\n",
        "                            converted_words.append('[Number Out of Range]')\n",
        "                    except:\n",
        "                        converted_words.append('[Number Out of Range]')\n",
        "                else:\n",
        "                    converted_words.append(word)\n",
        "        return ' '.join(converted_words)\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        try:\n",
        "            if '<' in text and '>' in text:\n",
        "                return BeautifulSoup(text, 'html.parser').get_text()\n",
        "            else:\n",
        "                return text\n",
        "        except:\n",
        "            logging.warning('MarkupResemblesLocatorWarning: The input looks more like a filename than markup.')\n",
        "            return text\n",
        "\n",
        "    def normalize_unicode(self, text):\n",
        "        return unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        return contractions.fix(text)\n",
        "\n",
        "    def cleaned_text(self, text):\n",
        "        text = re.sub(r'\\W', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text\n",
        "\n",
        "    def normalization_example(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def stemming_example(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
        "        return ' '.join(stemmed_words)\n",
        "\n",
        "    def lemmatization_example(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(lemmatized_words)\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    def remove_urls(self, text):\n",
        "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    def remove_special_characters_and_emojis(self, text):\n",
        "        return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
        "\n",
        "    def replace_synonyms(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        synonym_words = [self.get_synonym(word) for word in words]\n",
        "        return ' '.join(synonym_words)\n",
        "\n",
        "    def get_synonym(self, word):\n",
        "        synonyms = nltk.corpus.wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            return synonyms[0].lemmas()[0].name()\n",
        "        return word\n",
        "\n",
        "    def handle_negations(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        negated_text = []\n",
        "        negate = False\n",
        "        for word in words:\n",
        "            if word.lower() in ['not', \"n't\"]:\n",
        "                negate = True\n",
        "            elif negate:\n",
        "                negated_text.append(f'NOT_{word}')\n",
        "                negate = False\n",
        "            else:\n",
        "                negated_text.append(word)\n",
        "        return ' '.join(negated_text)\n",
        "\n",
        "    def remove_non_english_words(self, text):\n",
        "        words = self.tokenizer.word_tokenize(text)\n",
        "        english_words = [word for word in words if wordnet.synsets(word)]\n",
        "        return ' '.join(english_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxjFVOzz8vQA"
      },
      "outputs": [],
      "source": [
        "# Initialize text processor\n",
        "processor = TextProcessor()\n",
        "\n",
        "# Custom text processing function\n",
        "def processed_text(text):\n",
        "    if text is None:\n",
        "        return text\n",
        "    text = processor.cleaned_text(text)\n",
        "    text = processor.normalization_example(text)\n",
        "    text = processor.stemming_example(text)\n",
        "    text = processor.lemmatization_example(text)\n",
        "    text = processor.remove_stopwords(text)\n",
        "    text = processor.number_to_words(text)\n",
        "    text = processor.remove_punctuation(text)\n",
        "    text = processor.expand_contractions(text)\n",
        "    text = processor.normalize_unicode(text)\n",
        "    text = processor.handle_negations(text)\n",
        "    text = processor.remove_urls(text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oudamSqT8vQA",
        "outputId": "020933e8-98f8-4afb-8f4f-3eb9a38ea081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading documents...\n",
            "Documents loaded: 522771\n",
            "Sample document: doc_id    doc_id\n",
            "text        text\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Loading queries...\n",
            "Queries loaded: 5001\n",
            "Sample query: query_id    query_id\n",
            "text            text\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Loading relevance judgments...\n",
            "Relevance judgments loaded: 7627\n",
            "Sample qrel: query_id      query_id\n",
            "Q0              doc_id\n",
            "doc_id       relevance\n",
            "relevance          NaN\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Load Quora dataset files from Google Drive\n",
        "docs_path = '/content/drive/MyDrive/downloads/docs.tsv'\n",
        "queries_path = '/content/drive/MyDrive/downloads/queries.tsv'\n",
        "qrels_path = '/content/drive/MyDrive/downloads/qrels.tsv'\n",
        "\n",
        "# Load documents\n",
        "print('Loading documents...')\n",
        "docs_df = pd.read_csv(docs_path, sep='\\t', header=None, names=['doc_id', 'text'])\n",
        "print(f'Documents loaded: {len(docs_df)}')\n",
        "print(f'Sample document: {docs_df.iloc[0]}')\n",
        "\n",
        "# Load queries\n",
        "print('\\nLoading queries...')\n",
        "queries_df = pd.read_csv(queries_path, sep='\\t', header=None, names=['query_id', 'text'])\n",
        "print(f'Queries loaded: {len(queries_df)}')\n",
        "print(f'Sample query: {queries_df.iloc[0]}')\n",
        "\n",
        "# Load relevance judgments\n",
        "print('\\nLoading relevance judgments...')\n",
        "qrels_df = pd.read_csv(qrels_path, sep='\\t', header=None, names=['query_id', 'Q0', 'doc_id', 'relevance'])\n",
        "print(f'Relevance judgments loaded: {len(qrels_df)}')\n",
        "print(f'Sample qrel: {qrels_df.iloc[0]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AIR2ACb8vQB",
        "outputId": "50b51dba-176f-43d4-ec3a-bef74feb80b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing documents for TF-IDF processing...\n",
            "Number of documents to process: 522769\n",
            "Sample document text: text...\n",
            "Document mapping created: 522769 documents\n"
          ]
        }
      ],
      "source": [
        "# Prepare documents for processing\n",
        "print('Preparing documents for TF-IDF processing...')\n",
        "\n",
        "# Remove null values and ensure text is string\n",
        "docs_df = docs_df.dropna(subset=['text'])\n",
        "docs_df['text'] = docs_df['text'].astype(str)\n",
        "\n",
        "# Extract documents and their IDs\n",
        "documents = docs_df['text'].tolist()\n",
        "doc_ids = docs_df['doc_id'].tolist()\n",
        "\n",
        "print(f'Number of documents to process: {len(documents)}')\n",
        "print(f'Sample document text: {documents[0][:200]}...')\n",
        "\n",
        "# Create document ID to index mapping\n",
        "doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
        "index_to_doc_id = {idx: doc_id for idx, doc_id in enumerate(doc_ids)}\n",
        "print(f'Document mapping created: {len(doc_id_to_index)} documents')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDyE6Rpx8vQB",
        "outputId": "5b327f4d-05c3-4565-f47e-ca7b7d398d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing documents with custom cleaning method...\n",
            "Processed 0/522769 documents\n",
            "Processed 10000/522769 documents\n",
            "Processed 20000/522769 documents\n",
            "Processed 30000/522769 documents\n",
            "Processed 40000/522769 documents\n",
            "Processed 50000/522769 documents\n",
            "Processed 60000/522769 documents\n",
            "Processed 70000/522769 documents\n",
            "Processed 80000/522769 documents\n",
            "Processed 90000/522769 documents\n",
            "Processed 100000/522769 documents\n",
            "Processed 110000/522769 documents\n",
            "Processed 120000/522769 documents\n",
            "Processed 130000/522769 documents\n",
            "Processed 140000/522769 documents\n",
            "Processed 150000/522769 documents\n",
            "Processed 160000/522769 documents\n",
            "Processed 170000/522769 documents\n",
            "Processed 180000/522769 documents\n",
            "Processed 190000/522769 documents\n",
            "Processed 200000/522769 documents\n",
            "Processed 210000/522769 documents\n",
            "Processed 220000/522769 documents\n",
            "Processed 230000/522769 documents\n",
            "Processed 240000/522769 documents\n",
            "Processed 250000/522769 documents\n",
            "Processed 260000/522769 documents\n",
            "Processed 270000/522769 documents\n",
            "Processed 280000/522769 documents\n",
            "Processed 290000/522769 documents\n",
            "Processed 300000/522769 documents\n",
            "Processed 310000/522769 documents\n",
            "Processed 320000/522769 documents\n",
            "Processed 330000/522769 documents\n",
            "Processed 340000/522769 documents\n",
            "Processed 350000/522769 documents\n",
            "Processed 360000/522769 documents\n",
            "Processed 370000/522769 documents\n",
            "Processed 380000/522769 documents\n",
            "Processed 390000/522769 documents\n",
            "Processed 400000/522769 documents\n",
            "Processed 410000/522769 documents\n",
            "Processed 420000/522769 documents\n",
            "Processed 430000/522769 documents\n",
            "Processed 440000/522769 documents\n",
            "Processed 450000/522769 documents\n",
            "Processed 460000/522769 documents\n",
            "Processed 470000/522769 documents\n",
            "Processed 480000/522769 documents\n",
            "Processed 490000/522769 documents\n",
            "Processed 500000/522769 documents\n",
            "Processed 510000/522769 documents\n",
            "Processed 520000/522769 documents\n",
            "Text processing complete. Sample processed document: text...\n"
          ]
        }
      ],
      "source": [
        "# Apply custom text processing to all documents\n",
        "print('Processing documents with custom cleaning method...')\n",
        "processed_documents = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    if i % 10000 == 0:\n",
        "        print(f'Processed {i}/{len(documents)} documents')\n",
        "\n",
        "    processed_doc = processed_text(doc)\n",
        "    processed_documents.append(processed_doc)\n",
        "\n",
        "print(f'Text processing complete. Sample processed document: {processed_documents[0][:200]}...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8V03NQy8vQD",
        "outputId": "68634dda-fc6b-4027-ef2b-5d675aabe1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TF-IDF vectorizer with custom tokenizer...\n",
            "Fitting TF-IDF vectorizer with custom tokenizer...\n",
            "TF-IDF matrix shape: (522769, 10000)\n",
            "Vocabulary size: 10000\n",
            "Matrix sparsity: 0.9994\n"
          ]
        }
      ],
      "source": [
        "# Create custom tokenizer that applies our text processing\n",
        "def custom_tokenizer(text):\n",
        "    \"\"\"Custom tokenizer using our processed_text function\"\"\"\n",
        "    # Apply our custom text processing\n",
        "    processed = processed_text(text)\n",
        "    # Tokenize the processed text\n",
        "    tokens = processor.tokenizer.word_tokenize(processed) if processed else []\n",
        "    return tokens\n",
        "\n",
        "# Create TF-IDF vectorizer with custom tokenizer\n",
        "print('Creating TF-IDF vectorizer with custom tokenizer...')\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    preprocessor=None,  # Disable built-in preprocessing\n",
        "    tokenizer=custom_tokenizer,  # Use our custom tokenizer\n",
        "    token_pattern=None,  # Disable default token pattern\n",
        "    lowercase=False,     # We already handle lowercasing in custom processing\n",
        "    stop_words=None,     # We already handle stopwords in custom processing\n",
        "    max_features=10000,  # Larger vocabulary for Quora\n",
        "    min_df=2,           # Minimum document frequency\n",
        "    max_df=0.95,        # Maximum document frequency\n",
        "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
        "    sublinear_tf=True    # Use sublinear tf scaling\n",
        ")\n",
        "\n",
        "print('Fitting TF-IDF vectorizer with custom tokenizer...')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)  # Use original documents, not processed ones\n",
        "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
        "print(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}')\n",
        "print(f'Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp2oGllL8vQF",
        "outputId": "16f99b3c-c6a3-4af7-f5a9-699cc9c236ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building inverted index...\n",
            "Processing 3292278 non-zero entries...\n",
            "Processed 100000/3292278 entries\n",
            "Processed 200000/3292278 entries\n",
            "Processed 300000/3292278 entries\n",
            "Processed 400000/3292278 entries\n",
            "Processed 500000/3292278 entries\n",
            "Processed 600000/3292278 entries\n",
            "Processed 700000/3292278 entries\n",
            "Processed 800000/3292278 entries\n",
            "Processed 900000/3292278 entries\n",
            "Processed 1000000/3292278 entries\n",
            "Processed 1100000/3292278 entries\n",
            "Processed 1200000/3292278 entries\n",
            "Processed 1300000/3292278 entries\n",
            "Processed 1400000/3292278 entries\n",
            "Processed 1500000/3292278 entries\n",
            "Processed 1600000/3292278 entries\n",
            "Processed 1700000/3292278 entries\n",
            "Processed 1800000/3292278 entries\n",
            "Processed 1900000/3292278 entries\n",
            "Processed 2000000/3292278 entries\n",
            "Processed 2100000/3292278 entries\n",
            "Processed 2200000/3292278 entries\n",
            "Processed 2300000/3292278 entries\n",
            "Processed 2400000/3292278 entries\n",
            "Processed 2500000/3292278 entries\n",
            "Processed 2600000/3292278 entries\n",
            "Processed 2700000/3292278 entries\n",
            "Processed 2800000/3292278 entries\n",
            "Processed 2900000/3292278 entries\n",
            "Processed 3000000/3292278 entries\n",
            "Processed 3100000/3292278 entries\n",
            "Processed 3200000/3292278 entries\n",
            "Sorting inverted index entries...\n",
            "Inverted index created with 10000 terms\n",
            "Sample term: text -> [('doc_id', np.float64(1.0)), (444647, np.float64(0.9076237384911702)), (444648, np.float64(0.9076237384911702))]\n"
          ]
        }
      ],
      "source": [
        "# Build inverted index from TF-IDF matrix\n",
        "print('Building inverted index...')\n",
        "\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create inverted index with document IDs\n",
        "inverted_index = defaultdict(list)\n",
        "\n",
        "# Convert sparse matrix to coordinate format for efficient iteration\n",
        "coo_matrix = tfidf_matrix.tocoo()\n",
        "\n",
        "print(f'Processing {len(coo_matrix.data)} non-zero entries...')\n",
        "for i, (doc_idx, term_idx, tfidf_score) in enumerate(zip(coo_matrix.row, coo_matrix.col, coo_matrix.data)):\n",
        "    if i % 100000 == 0 and i > 0:\n",
        "        print(f'Processed {i}/{len(coo_matrix.data)} entries')\n",
        "\n",
        "    term = feature_names[term_idx]\n",
        "    doc_id = index_to_doc_id[doc_idx]\n",
        "    inverted_index[term].append((doc_id, tfidf_score))\n",
        "\n",
        "# Sort document lists by TF-IDF score (descending)\n",
        "print('Sorting inverted index entries...')\n",
        "for term in inverted_index:\n",
        "    inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f'Inverted index created with {len(inverted_index)} terms')\n",
        "print(f'Sample term: {list(inverted_index.keys())[0]} -> {inverted_index[list(inverted_index.keys())[0]][:3]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxk0O0178vQG",
        "outputId": "18cac4fa-e572-43b3-e0a1-f90db508a2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating document similarity matrix...\n",
            "Dataset too large (522769 docs). Sampling 5000 documents for similarity calculation.\n",
            "Similarity matrix shape: (5000, 5000)\n",
            "Mean similarity: 0.0042\n",
            "Max similarity: 1.0000\n",
            "Min similarity: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Calculate document similarity matrix (sample for large datasets)\n",
        "print('Calculating document similarity matrix...')\n",
        "\n",
        "# For large datasets, sample a subset for similarity calculation\n",
        "max_docs_for_similarity = 5000\n",
        "if len(documents) > max_docs_for_similarity:\n",
        "    print(f'Dataset too large ({len(documents)} docs). Sampling {max_docs_for_similarity} documents for similarity calculation.')\n",
        "    sample_indices = np.random.choice(len(documents), max_docs_for_similarity, replace=False)\n",
        "    sample_matrix = tfidf_matrix[sample_indices]\n",
        "else:\n",
        "    sample_matrix = tfidf_matrix\n",
        "    sample_indices = np.arange(len(documents))\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_matrix = cosine_similarity(sample_matrix)\n",
        "print(f'Similarity matrix shape: {similarity_matrix.shape}')\n",
        "\n",
        "# Calculate statistics\n",
        "mean_similarity = np.mean(similarity_matrix)\n",
        "max_similarity = np.max(similarity_matrix)\n",
        "min_similarity = np.min(similarity_matrix)\n",
        "\n",
        "print(f'Mean similarity: {mean_similarity:.4f}')\n",
        "print(f'Max similarity: {max_similarity:.4f}')\n",
        "print(f'Min similarity: {min_similarity:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaUVgCbc8vQG",
        "outputId": "c671abae-0aed-47c9-fb24-4f08156a7fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating MAP score using relevance judgments...\n",
            "Fitting TF-IDF vectorizer with custom tokenizer...\n",
            "TF-IDF matrix shape: (522769, 10000)\n"
          ]
        }
      ],
      "source": [
        "# Calculate MAP using relevance judgments (proper IR evaluation)\n",
        "print('Calculating MAP score using relevance judgments...')\n",
        "\n",
        "# Recreate the TF-IDF matrix (run this first)\n",
        "print('Fitting TF-IDF vectorizer with custom tokenizer...')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
        "\n",
        "\n",
        "def calculate_map_with_qrels(tfidf_matrix, queries_df, qrels_df, doc_id_to_index, k=1000):\n",
        "    \"\"\"Calculate MAP score using actual relevance judgments\"\"\"\n",
        "    # Process queries with the same text processing\n",
        "    processed_queries = []\n",
        "    for query_text in queries_df['text']:\n",
        "        processed_query = processed_text(str(query_text))\n",
        "        processed_queries.append(processed_query)\n",
        "\n",
        "    # Transform queries using the fitted vectorizer\n",
        "    query_vectors = tfidf_vectorizer.transform(processed_queries)\n",
        "\n",
        "    # Calculate similarities between queries and documents\n",
        "    query_doc_similarities = cosine_similarity(query_vectors, tfidf_matrix)\n",
        "\n",
        "    average_precisions = []\n",
        "\n",
        "    for i, query_id in enumerate(queries_df['query_id']):\n",
        "        # Get relevance judgments for this query\n",
        "        query_qrels = qrels_df[qrels_df['query_id'] == query_id]\n",
        "\n",
        "        if len(query_qrels) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get similarity scores for this query\n",
        "        similarities = query_doc_similarities[i]\n",
        "\n",
        "        # Sort documents by similarity (descending)\n",
        "        sorted_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        # Calculate precision at each relevant document\n",
        "        relevant_docs = set(query_qrels[query_qrels['relevance'] > 0]['doc_id'].values)\n",
        "\n",
        "        if len(relevant_docs) == 0:\n",
        "            continue\n",
        "\n",
        "        precisions = []\n",
        "        num_relevant_found = 0\n",
        "\n",
        "        for rank, doc_index in enumerate(sorted_indices, 1):\n",
        "            doc_id = index_to_doc_id[doc_index]\n",
        "\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant_found += 1\n",
        "                precision = num_relevant_found / rank\n",
        "                precisions.append(precision)\n",
        "\n",
        "        if precisions:\n",
        "            average_precision = np.mean(precisions)\n",
        "            average_precisions.append(average_precision)\n",
        "\n",
        "    return np.mean(average_precisions) if average_precisions else 0.0\n",
        "\n",
        "# Calculate MAP score\n",
        "map_score = calculate_map_with_qrels(tfidf_matrix, queries_df, qrels_df, doc_id_to_index)\n",
        "print(f'MAP score: {map_score:.4f}')\n",
        "\n",
        "if map_score >= 0.4:\n",
        "    print('✅ MAP score requirement met (>= 0.4)')\n",
        "else:\n",
        "    print('❌ MAP score below 0.4. This is normal for challenging datasets like Quora.')\n",
        "    print('   Consider adjusting TF-IDF parameters or text processing steps.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll-qn5iu8vQH"
      },
      "outputs": [],
      "source": [
        "# Calculate additional IR metrics\n",
        "print('Calculating additional IR metrics...')\n",
        "\n",
        "def calculate_precision_at_k(tfidf_matrix, queries_df, qrels_df, doc_id_to_index, k_values=[1, 5, 10, 20]):\n",
        "    \"\"\"Calculate Precision@K for different K values\"\"\"\n",
        "    processed_queries = []\n",
        "    for query_text in queries_df['text']:\n",
        "        processed_query = processed_text(str(query_text))\n",
        "        processed_queries.append(processed_query)\n",
        "\n",
        "    query_vectors = tfidf_vectorizer.transform(processed_queries)\n",
        "    query_doc_similarities = cosine_similarity(query_vectors, tfidf_matrix)\n",
        "\n",
        "    precision_at_k = {k: [] for k in k_values}\n",
        "\n",
        "    for i, query_id in enumerate(queries_df['query_id']):\n",
        "        query_qrels = qrels_df[qrels_df['query_id'] == query_id]\n",
        "\n",
        "        if len(query_qrels) == 0:\n",
        "            continue\n",
        "\n",
        "        similarities = query_doc_similarities[i]\n",
        "        sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "        relevant_docs = set(query_qrels[query_qrels['relevance'] > 0]['doc_id'].values)\n",
        "\n",
        "        for k in k_values:\n",
        "            top_k_indices = sorted_indices[:k]\n",
        "            top_k_doc_ids = [index_to_doc_id[idx] for idx in top_k_indices]\n",
        "            relevant_in_top_k = len([doc_id for doc_id in top_k_doc_ids if doc_id in relevant_docs])\n",
        "            precision_k = relevant_in_top_k / k\n",
        "            precision_at_k[k].append(precision_k)\n",
        "\n",
        "    return {k: np.mean(precisions) for k, precisions in precision_at_k.items()}\n",
        "\n",
        "# Calculate Precision@K\n",
        "precision_metrics = calculate_precision_at_k(tfidf_matrix, queries_df, qrels_df, doc_id_to_index)\n",
        "\n",
        "print('Precision@K results:')\n",
        "for k, precision in precision_metrics.items():\n",
        "    print(f'P@{k}: {precision:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1yuLXiTS8vQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0228efe-5a13-4151-d103-1b440d1577da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating document-term matrix and analyzing terms...\n",
            "Top 30 terms by TF-IDF frequency:\n",
            " 1. whi: 10248.8112\n",
            " 2. best: 8543.6953\n",
            " 3. doe: 7893.5112\n",
            " 4. get: 5973.3430\n",
            " 5. differ: 5066.2137\n",
            " 6. use: 4856.1986\n",
            " 7. like: 4814.8779\n",
            " 8. india: 4292.4377\n",
            " 9. good: 4273.7330\n",
            "10. peopl: 3846.5899\n",
            "11. make: 3789.0381\n",
            "12. would: 3375.9457\n",
            "13. way: 3364.1534\n",
            "14. work: 3205.7624\n",
            "15. one: 3187.3033\n",
            "16. mean: 3031.6566\n",
            "17. wa: 2941.1856\n",
            "18. learn: 2859.5873\n",
            "19. ani: 2785.0647\n",
            "20. life: 2690.1202\n",
            "21. time: 2559.8999\n",
            "22. thi: 2476.1039\n",
            "23. think: 2402.3537\n",
            "24. much: 2388.5032\n",
            "25. know: 2382.0080\n",
            "26. engin: 2348.4792\n",
            "27. quora: 2337.1986\n",
            "28. thing: 2254.3858\n",
            "29. ha: 2220.7763\n",
            "30. becom: 2216.8523\n",
            "\n",
            "Document-term matrix shape: (522769, 10000)\n",
            "Matrix density: 0.000630\n"
          ]
        }
      ],
      "source": [
        "# Create document-term matrix and term analysis\n",
        "print('Creating document-term matrix and analyzing terms...')\n",
        "\n",
        "# Calculate term frequencies across the corpus\n",
        "term_frequencies = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
        "term_freq_dict = dict(zip(feature_names, term_frequencies))\n",
        "\n",
        "# Get top terms\n",
        "top_terms = sorted(term_freq_dict.items(), key=lambda x: x[1], reverse=True)[:30]\n",
        "print('Top 30 terms by TF-IDF frequency:')\n",
        "for i, (term, freq) in enumerate(top_terms, 1):\n",
        "    print(f'{i:2d}. {term}: {freq:.4f}')\n",
        "\n",
        "# Document-term matrix (keep as sparse for memory efficiency)\n",
        "doc_term_matrix = tfidf_matrix\n",
        "print(f'\\nDocument-term matrix shape: {doc_term_matrix.shape}')\n",
        "print(f'Matrix density: {doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1]):.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WDjwjnBK8vQI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "3c7b334a-1573-455b-9522-9472b905e562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all components...\n",
            "TF-IDF vectorizer saved\n",
            "TF-IDF matrix saved\n",
            "Similarity matrix saved\n",
            "Inverted index saved\n",
            "Document mappings saved\n",
            "Feature names saved\n",
            "Processed documents saved\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'map_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-890246181.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Save evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m metrics = {\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;34m'map_score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;34m'precision_at_k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprecision_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m'mean_similarity'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_similarity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'map_score' is not defined"
          ]
        }
      ],
      "source": [
        "# Save all components using joblib\n",
        "print('Saving all components...')\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "joblib.dump(tfidf_vectorizer, 'quora_tfidf_vectorizer.joblib')\n",
        "print('TF-IDF vectorizer saved')\n",
        "\n",
        "# Save TF-IDF matrix\n",
        "joblib.dump(tfidf_matrix, 'quora_tfidf_matrix.joblib')\n",
        "print('TF-IDF matrix saved')\n",
        "\n",
        "# Save similarity matrix\n",
        "joblib.dump(similarity_matrix, 'quora_similarity_matrix.joblib')\n",
        "print('Similarity matrix saved')\n",
        "\n",
        "# Save inverted index\n",
        "joblib.dump(dict(inverted_index), 'quora_inverted_index.joblib')\n",
        "print('Inverted index saved')\n",
        "\n",
        "# Save document mappings\n",
        "joblib.dump(doc_id_to_index, 'quora_doc_id_to_index.joblib')\n",
        "joblib.dump(index_to_doc_id, 'quora_index_to_doc_id.joblib')\n",
        "print('Document mappings saved')\n",
        "\n",
        "# Save feature names\n",
        "joblib.dump(feature_names, 'quora_feature_names.joblib')\n",
        "print('Feature names saved')\n",
        "\n",
        "# Save processed documents\n",
        "joblib.dump(processed_documents, 'quora_processed_documents.joblib')\n",
        "print('Processed documents saved')\n",
        "\n",
        "# Save evaluation metrics\n",
        "metrics = {\n",
        "    'map_score': map_score,\n",
        "    'precision_at_k': precision_metrics,\n",
        "    'mean_similarity': mean_similarity,\n",
        "    'max_similarity': max_similarity,\n",
        "    'min_similarity': min_similarity,\n",
        "    'vocab_size': len(tfidf_vectorizer.vocabulary_),\n",
        "    'num_documents': len(documents),\n",
        "    'num_queries': len(queries_df),\n",
        "    'num_qrels': len(qrels_df),\n",
        "    'matrix_shape': tfidf_matrix.shape,\n",
        "    'matrix_sparsity': 1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])\n",
        "}\n",
        "joblib.dump(metrics, 'quora_ir_metrics.joblib')\n",
        "print('IR metrics saved')\n",
        "\n",
        "print('All components saved successfully!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2IROJaKH8vQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74dfa86c-14f9-4cd4-85a0-8dc09a9fc5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saving files to Google Drive...\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_tfidf_vectorizer.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_tfidf_matrix.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_similarity_matrix.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_inverted_index.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_doc_id_to_index.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_index_to_doc_id.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_feature_names.joblib\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_processed_documents.joblib\n",
            "cp: cannot stat '/content/quora_ir_metrics.joblib': No such file or directory\n",
            "Saved: /content/drive/MyDrive/quora_tfidf/quora_ir_metrics.joblib\n",
            "All files saved to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "folder_path = '/content/drive/MyDrive/quora_tfidf'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Files to save\n",
        "files_to_save = [\n",
        "    'quora_tfidf_vectorizer.joblib',\n",
        "    'quora_tfidf_matrix.joblib',\n",
        "    'quora_similarity_matrix.joblib',\n",
        "    'quora_inverted_index.joblib',\n",
        "    'quora_doc_id_to_index.joblib',\n",
        "    'quora_index_to_doc_id.joblib',\n",
        "    'quora_feature_names.joblib',\n",
        "    'quora_processed_documents.joblib',\n",
        "    'quora_ir_metrics.joblib'\n",
        "]\n",
        "\n",
        "print('Saving files to Google Drive...')\n",
        "for file_name in files_to_save:\n",
        "    try:\n",
        "        # Source path (in Colab's temporary storage)\n",
        "        src_path = f'/content/{file_name}'\n",
        "\n",
        "        # Destination path in Google Drive\n",
        "        dest_path = f'{folder_path}/{file_name}'\n",
        "\n",
        "        # Copy file\n",
        "        !cp \"{src_path}\" \"{dest_path}\"\n",
        "        print(f'Saved: {dest_path}')\n",
        "    except Exception as e:\n",
        "        print(f'Error saving {file_name}: {e}')\n",
        "\n",
        "print('All files saved to Google Drive!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-okNhEll8vQK"
      },
      "outputs": [],
      "source": [
        "# Summary and validation\n",
        "print('=== QUORA TF-IDF SYSTEM SUMMARY ===')\n",
        "print(f'Documents processed: {len(documents):,}')\n",
        "print(f'Queries processed: {len(queries_df):,}')\n",
        "print(f'Relevance judgments: {len(qrels_df):,}')\n",
        "print(f'Vocabulary size: {len(tfidf_vectorizer.vocabulary_):,}')\n",
        "print(f'TF-IDF matrix shape: {tfidf_matrix.shape}')\n",
        "print(f'Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.4f}')\n",
        "print(f'Inverted index terms: {len(inverted_index):,}')\n",
        "print(f'MAP score: {map_score:.4f}')\n",
        "\n",
        "print('Precision@K scores:')\n",
        "for k, precision in precision_metrics.items():\n",
        "    print(f'  P@{k}: {precision:.4f}')\n",
        "\n",
        "if map_score >= 0.4:\n",
        "    print('✅ MAP score requirement met (>= 0.4)')\n",
        "else:\n",
        "    print('ℹ️  MAP score below 0.4 - this is common for challenging datasets like Quora')\n",
        "\n",
        "print('\\n=== USAGE EXAMPLE ===')\n",
        "print('To load the saved components:')\n",
        "print('import joblib')\n",
        "print('vectorizer = joblib.load(\"quora_tfidf_vectorizer.joblib\")')\n",
        "print('matrix = joblib.load(\"quora_tfidf_matrix.joblib\")')\n",
        "print('inverted_index = joblib.load(\"quora_inverted_index.joblib\")')\n",
        "print('doc_mappings = joblib.load(\"quora_doc_id_to_index.joblib\")')\n",
        "print('metrics = joblib.load(\"quora_ir_metrics.joblib\")')\n",
        "\n",
        "print('\\n=== SEARCH EXAMPLE ===')\n",
        "print('# To search for similar documents:')\n",
        "print('query = \"your search query\"')\n",
        "print('processed_query = processed_text(query)')\n",
        "print('query_vector = vectorizer.transform([processed_query])')\n",
        "print('similarities = cosine_similarity(query_vector, matrix)')\n",
        "print('top_docs = similarities.argsort()[0][::-1][:10]')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFyzYKOqrl8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}