{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Fixed Quora Dataset Processing and Embedding Generation\n",
        "\n",
        "This notebook fixes the issue where queries were being removed due to incorrect column identification.\n",
        "\n",
        "**Key Fixes:**\n",
        "1. **Correct Column Detection**: Properly identifies the text column containing actual questions\n",
        "2. **Preserved Data**: Ensures no queries are lost during processing\n",
        "3. **Smart Text Processing**: Preserves semantic information while cleaning\n",
        "4. **Optimized Embeddings**: Uses best models for high MAP scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "## Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0253e3ea-98c7-4baa-fcc2-a015831b2bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: beir in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from beir) (4.1.0)\n",
            "Requirement already satisfied: pytrec-eval-terrier in /usr/local/lib/python3.11/dist-packages (from beir) (0.5.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from beir) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets->beir) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->beir) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->beir) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->beir) (1.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets->beir) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets->beir) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets->beir) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets->beir) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->beir) (1.17.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->beir) (4.53.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->beir) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->beir) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->beir) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->beir) (11.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->beir) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->beir) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->beir) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->beir) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers->beir) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->beir) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->beir) (3.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: ir_datasets in /usr/local/lib/python3.11/dist-packages (0.5.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.13.4)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.6.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.4.4)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (3.4.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2025.6.15)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
            "\n",
            "[INFO] Packages installed! Please restart runtime if needed.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip\n",
        "!pip install sentence-transformers>=2.2.2\n",
        "!pip install transformers>=4.21.0\n",
        "!pip install torch>=1.13.0\n",
        "!pip install pandas numpy scikit-learn\n",
        "!pip install joblib nltk tqdm\n",
        "!pip install faiss-cpu\n",
        "!pip install beir\n",
        "!pip install datasets\n",
        "!pip install ir_datasets\n",
        "\n",
        "print(\"\\n[INFO] Packages installed! Please restart runtime if needed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "import_libs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fab088-e275-493a-fe4e-45e0b6622fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "✅ All packages imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "import torch\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(\"✅ All packages imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## Step 3: Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "load",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7b5131-921f-4eba-e610-fa20beeaea32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading dataset files...\n",
            "✅ Loaded docs: 522770 rows from docs.tsv\n",
            "✅ Loaded queries: 5000 rows from queries.tsv\n",
            "✅ Loaded qrels: 7626 rows from qrels.tsv\n",
            "\n",
            "✅ Dataset loaded successfully!\n",
            "Documents: 522,770 rows\n",
            "Queries: 5,000 rows\n",
            "Qrels: 7,626 rows\n"
          ]
        }
      ],
      "source": [
        "# For Google Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/MyDrive/downloads'\n",
        "else:\n",
        "    # For local environment\n",
        "    base_path = '/Users/raafatmhanna/Desktop/Quora'\n",
        "\n",
        "# Load the dataset files\n",
        "print(\"Loading dataset files...\")\n",
        "\n",
        "# Try different possible file names and paths\n",
        "file_patterns = {\n",
        "    'docs': ['docs.tsv', 'documents.tsv', 'docs.tsv'],\n",
        "    'queries': ['queries.tsv', 'questions.tsv', 'query.tsv'],\n",
        "    'qrels': ['qrels.tsv', 'relevance.tsv', 'labels.tsv']\n",
        "}\n",
        "\n",
        "datasets = {}\n",
        "for data_type, patterns in file_patterns.items():\n",
        "    for pattern in patterns:\n",
        "        try:\n",
        "            file_path = os.path.join(base_path, pattern)\n",
        "            if os.path.exists(file_path):\n",
        "                datasets[data_type] = pd.read_csv(file_path, sep='\\t')\n",
        "                print(f\"✅ Loaded {data_type}: {len(datasets[data_type])} rows from {pattern}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "# Verify we have the necessary data\n",
        "if 'docs' not in datasets or 'queries' not in datasets:\n",
        "    print(\"❌ Error: Could not load required files\")\n",
        "    print(\"Please ensure docs.tsv and queries.tsv are in the correct location\")\n",
        "else:\n",
        "    print(\"\\n✅ Dataset loaded successfully!\")\n",
        "    print(f\"Documents: {len(datasets['docs']):,} rows\")\n",
        "    print(f\"Queries: {len(datasets['queries']):,} rows\")\n",
        "    if 'qrels' in datasets:\n",
        "        print(f\"Qrels: {len(datasets['qrels']):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspect_data"
      },
      "source": [
        "## Step 4: Inspect Data Structure (Critical Fix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "inspect",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ae1ce9-1f46-4135-e2ce-4be848ede410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INSPECTING QUERIES STRUCTURE ===\n",
            "\n",
            "Query columns: ['query_id', 'text']\n",
            "\n",
            "First 5 rows of queries:\n",
            "   query_id                                               text\n",
            "0       318                How does Quora look to a moderator?\n",
            "1       378  How do I refuse to chose between different thi...\n",
            "2       379  Did Ben Affleck shine more than Christian Bale...\n",
            "3       399  What are the effects of demonitization of 500 ...\n",
            "4       420                       Why creativity is important?\n",
            "\n",
            "=== IDENTIFYING TEXT COLUMNS ===\n",
            "\n",
            "Column 'query_id':\n",
            "  Row 0: 318\n",
            "  Row 1: 378\n",
            "  Row 2: 379\n",
            "\n",
            "Column 'text':\n",
            "  Row 0: How does Quora look to a moderator?\n",
            "  Row 1: How do I refuse to chose between different things to do in my life?\n",
            "  Row 2: Did Ben Affleck shine more than Christian Bale as Batman?\n",
            "  → Likely contains text (avg length: 51.5)\n",
            "\n",
            "=== INSPECTING DOCUMENTS STRUCTURE ===\n",
            "\n",
            "Document columns: ['doc_id', 'text']\n",
            "\n",
            "First 3 rows of documents:\n",
            "   doc_id                                               text\n",
            "0       1  What is the step by step guide to invest in sh...\n",
            "1       2  What is the step by step guide to invest in sh...\n",
            "2       3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL: Properly inspect the data structure\n",
        "print(\"=== INSPECTING QUERIES STRUCTURE ===\")\n",
        "print(\"\\nQuery columns:\", list(datasets['queries'].columns))\n",
        "print(\"\\nFirst 5 rows of queries:\")\n",
        "print(datasets['queries'].head())\n",
        "\n",
        "print(\"\\n=== IDENTIFYING TEXT COLUMNS ===\")\n",
        "# Identify which columns contain actual text\n",
        "for col in datasets['queries'].columns:\n",
        "    sample_values = datasets['queries'][col].head(3).tolist()\n",
        "    print(f\"\\nColumn '{col}':\")\n",
        "    for i, val in enumerate(sample_values):\n",
        "        print(f\"  Row {i}: {str(val)[:100]}...\" if len(str(val)) > 100 else f\"  Row {i}: {val}\")\n",
        "\n",
        "    # Check if this column contains question text\n",
        "    if datasets['queries'][col].astype(str).str.len().mean() > 20:\n",
        "        print(f\"  → Likely contains text (avg length: {datasets['queries'][col].astype(str).str.len().mean():.1f})\")\n",
        "\n",
        "print(\"\\n=== INSPECTING DOCUMENTS STRUCTURE ===\")\n",
        "print(\"\\nDocument columns:\", list(datasets['docs'].columns))\n",
        "print(\"\\nFirst 3 rows of documents:\")\n",
        "print(datasets['docs'].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smart_preprocessing"
      },
      "source": [
        "## Step 5: Smart Text Preprocessing with Correct Column Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "preprocess",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424b5bcd-e659-4ae5-9fb7-4a501cccaacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PROCESSING QUERIES ===\n",
            "\n",
            "Finding text column for query...\n",
            "  ✓ Found text column: 'text' (avg length: 51.5)\n",
            "\n",
            "Sample queries before cleaning:\n",
            "  1: How does Quora look to a moderator?...\n",
            "  2: How do I refuse to chose between different things to do in my life?...\n",
            "  3: Did Ben Affleck shine more than Christian Bale as Batman?...\n",
            "\n",
            "Queries processed: 5000 → 5000 (removed 0)\n",
            "\n",
            "Sample queries after cleaning:\n",
            "  1: How does Quora look to a moderator?...\n",
            "  2: How do I refuse to chose between different things to do in my life?...\n",
            "  3: Did Ben Affleck shine more than Christian Bale as Batman?...\n",
            "\n",
            "=== PROCESSING DOCUMENTS ===\n",
            "\n",
            "Finding text column for document...\n",
            "  ✓ Found text column: 'text' (avg length: 62.2)\n",
            "\n",
            "Documents processed: 522770 → 522768 (removed 2)\n",
            "\n",
            "✅ Cleaned data saved!\n"
          ]
        }
      ],
      "source": [
        "def safe_clean_text(text):\n",
        "    \"\"\"\n",
        "    Ultra-safe cleaning that preserves Quora question format\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to string to be safe\n",
        "    text = str(text)\n",
        "\n",
        "    # Minimal cleaning - preserve most information\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def find_text_column(df, data_type='query'):\n",
        "    \"\"\"\n",
        "    Intelligently find the column containing actual text content\n",
        "    \"\"\"\n",
        "    print(f\"\\nFinding text column for {data_type}...\")\n",
        "\n",
        "    # First, look for columns with common text-related names\n",
        "    text_keywords = ['text', 'question', 'query', 'content', 'title', 'body']\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        # Skip ID columns\n",
        "        if 'id' in col_lower and not any(keyword in col_lower for keyword in text_keywords):\n",
        "            continue\n",
        "\n",
        "        # Check if column name suggests text content\n",
        "        if any(keyword in col_lower for keyword in text_keywords):\n",
        "            # Verify it actually contains text\n",
        "            avg_length = df[col].astype(str).str.len().mean()\n",
        "            if avg_length > 20:  # Reasonable threshold for text content\n",
        "                print(f\"  ✓ Found text column: '{col}' (avg length: {avg_length:.1f})\")\n",
        "                return col\n",
        "\n",
        "    # If no column found by name, find the column with longest average text\n",
        "    max_length = 0\n",
        "    best_col = None\n",
        "\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            avg_length = df[col].astype(str).str.len().mean()\n",
        "            if avg_length > max_length:\n",
        "                max_length = avg_length\n",
        "                best_col = col\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if best_col and max_length > 20:\n",
        "        print(f\"  ✓ Found text column by length: '{best_col}' (avg length: {max_length:.1f})\")\n",
        "        return best_col\n",
        "\n",
        "    # Last resort - return the second column (first is usually ID)\n",
        "    if len(df.columns) > 1:\n",
        "        print(f\"  ⚠️ Using fallback column: '{df.columns[1]}'\")\n",
        "        return df.columns[1]\n",
        "\n",
        "    return df.columns[0]\n",
        "\n",
        "# Process queries with correct column detection\n",
        "print(\"=== PROCESSING QUERIES ===\")\n",
        "queries_df = datasets['queries'].copy()\n",
        "\n",
        "# Find the actual text column\n",
        "query_text_col = find_text_column(queries_df, 'query')\n",
        "\n",
        "# Show sample of what we're processing\n",
        "print(\"\\nSample queries before cleaning:\")\n",
        "for i in range(min(3, len(queries_df))):\n",
        "    print(f\"  {i+1}: {queries_df[query_text_col].iloc[i][:100]}...\")\n",
        "\n",
        "# Apply cleaning\n",
        "queries_df['text_cleaned'] = queries_df[query_text_col].apply(safe_clean_text)\n",
        "\n",
        "# Remove only truly empty entries\n",
        "original_count = len(queries_df)\n",
        "queries_df = queries_df[queries_df['text_cleaned'].str.len() > 0]\n",
        "cleaned_count = len(queries_df)\n",
        "\n",
        "print(f\"\\nQueries processed: {original_count} → {cleaned_count} (removed {original_count - cleaned_count})\")\n",
        "\n",
        "# Show sample after cleaning\n",
        "print(\"\\nSample queries after cleaning:\")\n",
        "for i in range(min(3, len(queries_df))):\n",
        "    print(f\"  {i+1}: {queries_df['text_cleaned'].iloc[i][:100]}...\")\n",
        "\n",
        "# Process documents\n",
        "print(\"\\n=== PROCESSING DOCUMENTS ===\")\n",
        "docs_df = datasets['docs'].copy()\n",
        "\n",
        "# Find the actual text column for documents\n",
        "doc_text_col = find_text_column(docs_df, 'document')\n",
        "\n",
        "# Apply cleaning\n",
        "docs_df['text_cleaned'] = docs_df[doc_text_col].apply(safe_clean_text)\n",
        "\n",
        "# Remove only truly empty entries\n",
        "original_count = len(docs_df)\n",
        "docs_df = docs_df[docs_df['text_cleaned'].str.len() > 0]\n",
        "cleaned_count = len(docs_df)\n",
        "\n",
        "print(f\"\\nDocuments processed: {original_count} → {cleaned_count} (removed {original_count - cleaned_count})\")\n",
        "\n",
        "# Save cleaned data\n",
        "queries_df.to_csv('queries_cleaned.tsv', sep='\\t', index=False)\n",
        "docs_df.to_csv('docs_cleaned.tsv', sep='\\t', index=False)\n",
        "print(\"\\n✅ Cleaned data saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embeddings"
      },
      "source": [
        "## Step 6: Generate Optimized Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "generate_embeddings",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "44c065afd5604fb3ae111b10d4fc8a51",
            "eaab8776cc094f52ba00255ff2c9586b",
            "d9e2b0d6a0a9401ab052d80f4c9f5c7c",
            "ff8d3b2aae0341b1a8dc60d11c6c1f84",
            "5bf5f7915f5445b182fab4170df50b19",
            "43663d8f4491419daa96634732d6a4f0",
            "21fc3ed5d43a413cbbfda216d7886509",
            "347c0cf803de46479b40189110b65d99",
            "136fd323a8fe4aa2baf357faa669e629",
            "eca24fe4fd994519b3f3aa06def61351",
            "57ce7292ada34e94a99cf7a372b98936",
            "e9119175d5c344f9bc7c4cd38d69fbae",
            "978b27fdd3aa4d31a46dfe59c1af6474",
            "6948920445fe405f934457592e7aaccc",
            "ce0619743ab1410b8df4c2fa247fe2df",
            "eaa3a29c2fd4470c8367bf6314ca7b90",
            "e0d47d13282b47aca094075f48ead92f",
            "796ffe0642184a208a3b4cccf11694fd",
            "f2fa92789154434fa4b263cd3c9c5b83",
            "9eebd8367f39426fad4ef1347257dc32",
            "74adade21e58426f9943de687bf19cee",
            "eff560b4f3834f0a9d26907e1e5a1c8b"
          ]
        },
        "outputId": "ed799edb-4d91-4675-c34d-ac804e0fb58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Model loaded on cuda\n",
            "Max sequence length: 512\n",
            "\n",
            "Preparing texts...\n",
            "\n",
            "Ready to generate embeddings for:\n",
            "  - 522,768 documents\n",
            "  - 5,000 queries\n",
            "\n",
            "=== GENERATING DOCUMENT EMBEDDINGS ===\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8169 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44c065afd5604fb3ae111b10d4fc8a51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GENERATING QUERY EMBEDDINGS ===\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9119175d5c344f9bc7c4cd38d69fbae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Embeddings generated!\n",
            "Document embeddings shape: (522768, 384)\n",
            "Query embeddings shape: (5000, 384)\n",
            "\n",
            "Verification:\n",
            "First doc embedding norm: 1.000 (should be ~1.0)\n",
            "First query embedding norm: 1.000 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# Load optimized model\n",
        "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "# Set optimal parameters\n",
        "if hasattr(model, 'max_seq_length'):\n",
        "    model.max_seq_length = 512\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Max sequence length: {getattr(model, 'max_seq_length', 'default')}\")\n",
        "\n",
        "# Prepare texts for embedding\n",
        "print(\"\\nPreparing texts...\")\n",
        "\n",
        "# Get document texts and IDs\n",
        "doc_texts = docs_df['text_cleaned'].tolist()\n",
        "doc_ids = docs_df[docs_df.columns[0]].tolist()  # First column is usually ID\n",
        "\n",
        "# Get query texts and IDs\n",
        "query_texts = queries_df['text_cleaned'].tolist()\n",
        "query_ids = queries_df[queries_df.columns[0]].tolist()  # First column is usually ID\n",
        "\n",
        "print(f\"\\nReady to generate embeddings for:\")\n",
        "print(f\"  - {len(doc_texts):,} documents\")\n",
        "print(f\"  - {len(query_texts):,} queries\")\n",
        "\n",
        "# Generate embeddings with progress bar\n",
        "def generate_embeddings_batch(texts, desc=\"Generating embeddings\"):\n",
        "    \"\"\"Generate embeddings with optimal batch size\"\"\"\n",
        "    # Determine batch size based on available memory\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "        if gpu_memory < 8e9:  # Less than 8GB\n",
        "            batch_size = 32\n",
        "        elif gpu_memory < 16e9:  # Less than 16GB\n",
        "            batch_size = 64\n",
        "        else:\n",
        "            batch_size = 128\n",
        "    else:\n",
        "        batch_size = 32\n",
        "\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True  # Important for better similarity\n",
        "    )\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"\\n=== GENERATING DOCUMENT EMBEDDINGS ===\")\n",
        "doc_embeddings = generate_embeddings_batch(doc_texts)\n",
        "\n",
        "print(\"\\n=== GENERATING QUERY EMBEDDINGS ===\")\n",
        "query_embeddings = generate_embeddings_batch(query_texts)\n",
        "\n",
        "print(f\"\\n✅ Embeddings generated!\")\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
        "\n",
        "# Verify normalization\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"First doc embedding norm: {np.linalg.norm(doc_embeddings[0]):.3f} (should be ~1.0)\")\n",
        "print(f\"First query embedding norm: {np.linalg.norm(query_embeddings[0]):.3f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## Step 7: Evaluate Retrieval Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "evaluate",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae62295-9d35-4b91-91ed-c91423e76fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS index...\n",
            "Index built with 522768 documents\n",
            "\n",
            "=== SAMPLE RETRIEVAL TEST ===\n",
            "\n",
            "Query 1: How does Quora look to a moderator?...\n",
            "Top 5 retrieved documents:\n",
            "  1. Score: 0.725 - How does one become a Quora moderator?...\n",
            "  2. Score: 0.686 - Who are the Quora Moderators?...\n",
            "  3. Score: 0.680 - How is Quora moderated?...\n",
            "  4. Score: 0.676 - What does the Quora website look like to members of Quora moderation?...\n",
            "  5. Score: 0.675 - How does Quora Moderation work?...\n",
            "\n",
            "Query 2: How do I refuse to chose between different things to do in my life?...\n",
            "Top 5 retrieved documents:\n",
            "  1. Score: 0.800 - How do I choose what to do with my life?...\n",
            "  2. Score: 0.763 - How do you \"DECIDE\" what you want to do with your life?...\n",
            "  3. Score: 0.744 - How can I decide what to do in with my life?...\n",
            "  4. Score: 0.731 - How do I decide on what to do with my life?...\n",
            "  5. Score: 0.699 - Why I'm not able to decide what my goal is & what to do in my life?...\n",
            "\n",
            "Query 3: Did Ben Affleck shine more than Christian Bale as Batman?...\n",
            "Top 5 retrieved documents:\n",
            "  1. Score: 0.846 - Who do you think portrayed Batman better: Christian Bale or Ben Affleck?...\n",
            "  2. Score: 0.822 - According to you, whose Batman performance was best: Christian Bale or Ben Affle...\n",
            "  3. Score: 0.814 - Which actor looks better in the Batman costume: Christian Bale or Ben Affleck? W...\n",
            "  4. Score: 0.807 - No fanboys please, but who was the true batman, Christian Bale or Ben Affleck?...\n",
            "  5. Score: 0.791 - Who was better as Batman: Bale or Affleck?...\n",
            "\n",
            "Query 4: What are the effects of demonitization of 500 and 1000 rupees notes on real estate sector?...\n",
            "Top 5 retrieved documents:\n",
            "  1. Score: 0.927 - How does Demonetisation of 1000 and 500 rupees notes affect real estate industry...\n",
            "  2. Score: 0.886 - What will be the effect of recent demonetization of rs500 and rs 1000 notes on t...\n",
            "  3. Score: 0.883 - What will be the impact on real estate by banning 500 and 1000 rupee notes from ...\n",
            "  4. Score: 0.863 - What will be Real estate trend after Demonetization of 500 & 1000 currency notes...\n",
            "  5. Score: 0.860 - How does the demonetized notes of 500 and 1000 effect the real estate?...\n",
            "\n",
            "Query 5: Why creativity is important?...\n",
            "Top 5 retrieved documents:\n",
            "  1. Score: 0.992 - Why is creativity important?...\n",
            "  2. Score: 0.786 - What is creativity?...\n",
            "  3. Score: 0.783 - What does creativity mean to you?...\n",
            "  4. Score: 0.768 - What is a creativity?...\n",
            "  5. Score: 0.745 - What is more important, efficiency or creativity?...\n",
            "\n",
            "=== CALCULATING METRICS ===\n",
            "Calculating similarity statistics...\n",
            "\n",
            "Similarity Statistics:\n",
            "  Mean: 0.0440\n",
            "  Std: 0.0825\n",
            "  Max: 1.0000\n",
            "  Min: -0.3236\n",
            "\n",
            "=== CALCULATING MAP SCORE ===\n",
            "MAP calculation requires proper qrels format\n"
          ]
        }
      ],
      "source": [
        "# Build FAISS index for efficient retrieval\n",
        "print(\"Building FAISS index...\")\n",
        "index = faiss.IndexFlatIP(doc_embeddings.shape[1])  # Inner product for normalized vectors\n",
        "index.add(doc_embeddings.astype(np.float32))\n",
        "print(f\"Index built with {index.ntotal} documents\")\n",
        "\n",
        "# Quick evaluation on sample queries\n",
        "print(\"\\n=== SAMPLE RETRIEVAL TEST ===\")\n",
        "n_samples = min(5, len(query_embeddings))\n",
        "k = 5  # Top-k documents to retrieve\n",
        "\n",
        "for i in range(n_samples):\n",
        "    print(f\"\\nQuery {i+1}: {query_texts[i][:100]}...\")\n",
        "\n",
        "    # Search for similar documents\n",
        "    scores, indices = index.search(query_embeddings[i:i+1].astype(np.float32), k)\n",
        "\n",
        "    print(f\"Top {k} retrieved documents:\")\n",
        "    for j, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "        print(f\"  {j+1}. Score: {score:.3f} - {doc_texts[idx][:80]}...\")\n",
        "\n",
        "# Calculate basic metrics\n",
        "print(\"\\n=== CALCULATING METRICS ===\")\n",
        "\n",
        "# Sample evaluation for efficiency\n",
        "sample_size = min(100, len(query_embeddings))\n",
        "sample_indices = np.random.choice(len(query_embeddings), sample_size, replace=False)\n",
        "sample_queries = query_embeddings[sample_indices]\n",
        "\n",
        "# Calculate similarity statistics\n",
        "print(\"Calculating similarity statistics...\")\n",
        "similarities = cosine_similarity(sample_queries, doc_embeddings)\n",
        "\n",
        "print(f\"\\nSimilarity Statistics:\")\n",
        "print(f\"  Mean: {np.mean(similarities):.4f}\")\n",
        "print(f\"  Std: {np.std(similarities):.4f}\")\n",
        "print(f\"  Max: {np.max(similarities):.4f}\")\n",
        "print(f\"  Min: {np.min(similarities):.4f}\")\n",
        "\n",
        "# Calculate MAP if qrels available\n",
        "if 'qrels' in datasets and datasets['qrels'] is not None:\n",
        "    print(\"\\n=== CALCULATING MAP SCORE ===\")\n",
        "    # Implementation would go here based on qrels format\n",
        "    print(\"MAP calculation requires proper qrels format\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No qrels file found for MAP calculation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CALCULATE MAP & MPR METRICS ======\n",
        "if 'qrels' in datasets and len(datasets['qrels']) > 0:\n",
        "    print(\"\\n=== CALCULATING RETRIEVAL METRICS ===\")\n",
        "    print(\"Preparing qrels data...\")\n",
        "\n",
        "    # Convert qrels to {query_id: {doc_id: relevance}} format\n",
        "    qrels = defaultdict(dict)\n",
        "    for _, row in datasets['qrels'].iterrows():\n",
        "        qid = str(row['query_id'])\n",
        "        did = str(row['doc_id'])\n",
        "        qrels[qid][did] = int(row['relevance'])\n",
        "\n",
        "    # Create mappings from IDs to embedding indices\n",
        "    query_id_to_idx = {str(qid): i for i, qid in enumerate(query_ids)}\n",
        "    doc_id_to_idx = {str(did): i for i, did in enumerate(doc_ids)}\n",
        "\n",
        "    # Evaluation parameters\n",
        "    top_k = 100  # Maximum number of docs to retrieve per query\n",
        "    rank_cutoffs = [5, 10, 20, 50, 100]  # For MPR calculation\n",
        "\n",
        "    # Initialize metrics storage\n",
        "    map_scores = []\n",
        "    mpr_scores = {k: [] for k in rank_cutoffs}\n",
        "\n",
        "    print(f\"\\nEvaluating on {len(qrels)} query-relevance pairs...\")\n",
        "\n",
        "    # Process each query with relevance judgments\n",
        "    for qid, relevant_docs in tqdm(qrels.items(), desc=\"Evaluating queries\"):\n",
        "        if qid not in query_id_to_idx:\n",
        "            continue  # Skip if query wasn't processed\n",
        "\n",
        "        query_idx = query_id_to_idx[qid]\n",
        "        query_embedding = query_embeddings[query_idx]\n",
        "\n",
        "        # Retrieve top_k documents\n",
        "        distances, indices = index.search(\n",
        "            query_embedding.reshape(1, -1).astype(np.float32),\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        retrieved_docs = [doc_ids[i] for i in indices[0]]\n",
        "        relevant_found = 0\n",
        "        precisions = []\n",
        "\n",
        "        # Calculate precision at each rank\n",
        "        for rank, did in enumerate(retrieved_docs, 1):\n",
        "            if str(did) in relevant_docs:\n",
        "                relevant_found += 1\n",
        "                precisions.append(relevant_found / rank)\n",
        "\n",
        "                # Record precision at cutoff points\n",
        "                if rank in rank_cutoffs:\n",
        "                    mpr_scores[rank].append(relevant_found / rank)\n",
        "\n",
        "        # Calculate Average Precision for this query\n",
        "        if precisions:\n",
        "            ap = sum(precisions) / len(relevant_docs)\n",
        "            map_scores.append(ap)\n",
        "\n",
        "    # Calculate final metrics\n",
        "    if map_scores:\n",
        "        MAP = np.mean(map_scores)\n",
        "        print(f\"\\nMean Average Precision (MAP): {MAP:.4f}\")\n",
        "\n",
        "        print(\"\\nMean Precision at Rank (MPR):\")\n",
        "        for cutoff in sorted(mpr_scores.keys()):\n",
        "            if mpr_scores[cutoff]:\n",
        "                mpr = np.mean(mpr_scores[cutoff])\n",
        "                print(f\"  @{cutoff}: {mpr:.4f}\")\n",
        "            else:\n",
        "                print(f\"  @{cutoff}: No relevant docs found\")\n",
        "\n",
        "        # Save metrics to metadata\n",
        "        if 'metadata' in locals():\n",
        "            metadata['retrieval_metrics'] = {\n",
        "                'MAP': MAP,\n",
        "                'MPR': {k: np.mean(v) for k, v in mpr_scores.items() if v}\n",
        "            }\n",
        "            joblib.dump(metadata, 'embedding_metadata.joblib')\n",
        "            print(\"\\n✅ Metrics saved to metadata\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ No relevant documents found for any query\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No qrels data found - skipping MAP/MPR calculation\")\n",
        "\n",
        "print(\"\\n=== EVALUATION COMPLETE ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntuKkRPsyksm",
        "outputId": "bb48fe77-c504-4148-cb79-6cd4d040e18d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CALCULATING RETRIEVAL METRICS ===\n",
            "Preparing qrels data...\n",
            "\n",
            "Evaluating on 5000 query-relevance pairs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating queries: 100%|██████████| 5000/5000 [06:29<00:00, 12.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Average Precision (MAP): 0.8454\n",
            "\n",
            "Mean Precision at Rank (MPR):\n",
            "  @5: 0.5285\n",
            "  @10: 0.3847\n",
            "  @20: 0.2935\n",
            "  @50: 0.1250\n",
            "  @100: 0.1025\n",
            "\n",
            "=== EVALUATION COMPLETE ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## Step 8: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "save_results",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d7c14d3b-4e6a-4dcd-80ea-d8b40e6a2818"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-4092403406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define your save directory in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your save directory in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/Quora_Embeddings'  # Change this to your preferred path\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    print(f\"Created directory: {save_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {save_dir}\")\n",
        "\n",
        "print(\"\\nSaving embeddings and metadata to Google Drive...\")\n",
        "\n",
        "# Save embeddings using joblib\n",
        "joblib.dump(doc_embeddings, f'{save_dir}/doc_embeddings.joblib')\n",
        "joblib.dump(query_embeddings, f'{save_dir}/query_embeddings.joblib')\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'model_name': MODEL_NAME,\n",
        "    'embedding_dim': doc_embeddings.shape[1],\n",
        "    'num_docs': len(doc_embeddings),\n",
        "    'num_queries': len(query_embeddings),\n",
        "    'doc_ids': doc_ids,\n",
        "    'query_ids': query_ids,\n",
        "    'normalized': True\n",
        "}\n",
        "joblib.dump(metadata, f'{save_dir}/embedding_metadata.joblib')\n",
        "\n",
        "# Save cleaned texts with IDs using joblib\n",
        "doc_data = {\n",
        "    'doc_ids': doc_ids,\n",
        "    'texts': doc_texts\n",
        "}\n",
        "joblib.dump(doc_data, f'{save_dir}/documents_final.joblib')\n",
        "\n",
        "query_data = {\n",
        "    'query_ids': query_ids,\n",
        "    'texts': query_texts\n",
        "}\n",
        "joblib.dump(query_data, f'{save_dir}/queries_final.joblib')\n",
        "\n",
        "# Create summary\n",
        "summary = f\"\"\"\n",
        "=== PROCESSING COMPLETE ===\n",
        "\n",
        "Model: {MODEL_NAME}\n",
        "Documents: {len(doc_embeddings):,}\n",
        "Queries: {len(query_embeddings):,}\n",
        "Embedding Dimension: {doc_embeddings.shape[1]}\n",
        "\n",
        "Files Generated (all in joblib format):\n",
        "- doc_embeddings.joblib: Document embeddings\n",
        "- query_embeddings.joblib: Query embeddings\n",
        "- embedding_metadata.joblib: Metadata\n",
        "- documents_final.joblib: Cleaned documents with IDs\n",
        "- queries_final.joblib: Cleaned queries with IDs\n",
        "\n",
        "Saved to Google Drive at: {save_dir}\n",
        "\n",
        "✅ All files saved successfully!\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary as text file\n",
        "with open(f'{save_dir}/processing_summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "# Create zip file for easy download\n",
        "print(\"\\nCreating zip file in Google Drive...\")\n",
        "with zipfile.ZipFile(f'{save_dir}/quora_embeddings_joblib.zip', 'w') as zipf:\n",
        "    zipf.write(f'{save_dir}/doc_embeddings.joblib', 'doc_embeddings.joblib')\n",
        "    zipf.write(f'{save_dir}/query_embeddings.joblib', 'query_embeddings.joblib')\n",
        "    zipf.write(f'{save_dir}/embedding_metadata.joblib', 'embedding_metadata.joblib')\n",
        "    zipf.write(f'{save_dir}/documents_final.joblib', 'documents_final.joblib')\n",
        "    zipf.write(f'{save_dir}/queries_final.joblib', 'queries_final.joblib')\n",
        "    zipf.write(f'{save_dir}/processing_summary.txt', 'processing_summary.txt')\n",
        "\n",
        "print(f\"✅ Zip file created: {save_dir}/quora_embeddings_joblib.zip\")\n",
        "print(\"\\n🎉 Processing complete! Files saved to your Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your save directory in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/Quora_Embeddings'  # Change this to your preferred path\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "    print(f\"Created directory: {save_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {save_dir}\")\n",
        "\n",
        "# 1. Save the model itself\n",
        "print(\"\\nSaving the Sentence Transformer model...\")\n",
        "model_save_path = f\"{save_dir}/{MODEL_NAME.replace('/', '_')}\"\n",
        "model.save(model_save_path)\n",
        "print(f\"✅ Model saved to: {model_save_path}\")\n",
        "\n",
        "# 2. Save embeddings using joblib\n",
        "print(\"\\nSaving embeddings...\")\n",
        "joblib.dump(doc_embeddings, f'{save_dir}/doc_embeddings.joblib')\n",
        "joblib.dump(query_embeddings, f'{save_dir}/query_embeddings.joblib')\n",
        "\n",
        "# 3. Save metadata\n",
        "metadata = {\n",
        "    'model_name': MODEL_NAME,\n",
        "    'model_path': model_save_path,\n",
        "    'embedding_dim': doc_embeddings.shape[1],\n",
        "    'num_docs': len(doc_embeddings),\n",
        "    'num_queries': len(query_embeddings),\n",
        "    'doc_ids': doc_ids,\n",
        "    'query_ids': query_ids,\n",
        "    'normalized': True\n",
        "}\n",
        "joblib.dump(metadata, f'{save_dir}/embedding_metadata.joblib')\n",
        "\n",
        "# 4. Save cleaned texts\n",
        "doc_data = {\n",
        "    'doc_ids': doc_ids,\n",
        "    'texts': doc_texts\n",
        "}\n",
        "joblib.dump(doc_data, f'{save_dir}/documents_final.joblib')\n",
        "\n",
        "query_data = {\n",
        "    'query_ids': query_ids,\n",
        "    'texts': query_texts\n",
        "}\n",
        "joblib.dump(query_data, f'{save_dir}/queries_final.joblib')\n",
        "\n",
        "# Create summary\n",
        "summary = f\"\"\"\n",
        "=== PROCESSING COMPLETE ===\n",
        "\n",
        "Model: {MODEL_NAME}\n",
        "Model saved to: {model_save_path}\n",
        "Documents: {len(doc_embeddings):,}\n",
        "Queries: {len(query_embeddings):,}\n",
        "Embedding Dimension: {doc_embeddings.shape[1]}\n",
        "\n",
        "Files Generated:\n",
        "- Model directory: {MODEL_NAME.replace('/', '_')}/\n",
        "- doc_embeddings.joblib: Document embeddings\n",
        "- query_embeddings.joblib: Query embeddings\n",
        "- embedding_metadata.joblib: Metadata\n",
        "- documents_final.joblib: Cleaned documents\n",
        "- queries_final.joblib: Cleaned queries\n",
        "\n",
        "Saved to Google Drive at: {save_dir}\n",
        "\n",
        "✅ All files saved successfully!\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary\n",
        "with open(f'{save_dir}/processing_summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"\\n🎉 Processing complete! Model and embeddings saved to your Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOb43whK3PGu",
        "outputId": "fefdaee4-f849-48ae-e22a-440a0d749583"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory already exists: /content/drive/MyDrive/Quora_Embeddings\n",
            "\n",
            "Saving the Sentence Transformer model...\n",
            "✅ Model saved to: /content/drive/MyDrive/Quora_Embeddings/sentence-transformers_all-MiniLM-L6-v2\n",
            "\n",
            "Saving embeddings...\n",
            "\n",
            "=== PROCESSING COMPLETE ===\n",
            "\n",
            "Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Model saved to: /content/drive/MyDrive/Quora_Embeddings/sentence-transformers_all-MiniLM-L6-v2\n",
            "Documents: 522,768\n",
            "Queries: 5,000\n",
            "Embedding Dimension: 384\n",
            "\n",
            "Files Generated:\n",
            "- Model directory: sentence-transformers_all-MiniLM-L6-v2/\n",
            "- doc_embeddings.joblib: Document embeddings\n",
            "- query_embeddings.joblib: Query embeddings\n",
            "- embedding_metadata.joblib: Metadata\n",
            "- documents_final.joblib: Cleaned documents\n",
            "- queries_final.joblib: Cleaned queries\n",
            "\n",
            "Saved to Google Drive at: /content/drive/MyDrive/Quora_Embeddings\n",
            "\n",
            "✅ All files saved successfully!\n",
            "\n",
            "\n",
            "🎉 Processing complete! Model and embeddings saved to your Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faiss_indexing"
      },
      "source": [
        "## Step 9: Build FAISS Index for Efficient Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "build_faiss_index",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72207720-3534-45f0-8591-8e4de60e6542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BUILDING FAISS INDEX ===\n",
            "Embedding dimension: 384\n",
            "Number of documents: 522,768\n",
            "\n",
            "Building FAISS indices...\n",
            "1. Building IndexFlatIP (exact search)...\n",
            "   ✅ Built in 0.71 seconds\n",
            "2. Building IndexIVFFlat (approximate search)...\n",
            "   Using 723 clusters\n",
            "   Training index...\n",
            "   Adding vectors...\n",
            "   ✅ Built in 32.63 seconds\n",
            "   Search will probe 32 clusters\n",
            "3. Building IndexHNSWFlat (graph-based search)...\n",
            "   ✅ Built in 629.83 seconds\n",
            "   M=16, efConstruction=200, efSearch=128\n",
            "\n",
            "=== FAISS INDEX SUMMARY ===\n",
            "IndexFlatIP: 522,768 vectors, exact search\n",
            "IndexIVFFlat: 522,768 vectors, 723 clusters, approximate search\n",
            "IndexHNSWFlat: 522,768 vectors, graph-based search\n",
            "\n",
            "✅ All FAISS indices built successfully!\n",
            "\n",
            "=== QUICK SEARCH TEST ===\n",
            "FLAT: Found 5 results in 104.15ms\n",
            "IVF: Found 5 results in 8.40ms\n",
            "HNSW: Found 5 results in 0.99ms\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "print(\"=== BUILDING FAISS INDEX ===\")\n",
        "\n",
        "# Convert embeddings to float32 for FAISS\n",
        "doc_embeddings_f32 = doc_embeddings.astype(np.float32)\n",
        "query_embeddings_f32 = query_embeddings.astype(np.float32)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dim = doc_embeddings_f32.shape[1]\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "print(f\"Number of documents: {len(doc_embeddings_f32):,}\")\n",
        "\n",
        "# Create different types of FAISS indices\n",
        "print(\"\\nBuilding FAISS indices...\")\n",
        "\n",
        "# 1. Flat Index (exact search, best for accuracy)\n",
        "print(\"1. Building IndexFlatIP (exact search)...\")\n",
        "start_time = time.time()\n",
        "index_flat = faiss.IndexFlatIP(embedding_dim)\n",
        "index_flat.add(doc_embeddings_f32)\n",
        "flat_build_time = time.time() - start_time\n",
        "print(f\"   ✅ Built in {flat_build_time:.2f} seconds\")\n",
        "\n",
        "# 2. IVF Index (approximate search, faster for large datasets)\n",
        "print(\"2. Building IndexIVFFlat (approximate search)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate number of clusters (nlist)\n",
        "nlist = min(4096, int(np.sqrt(len(doc_embeddings_f32))))\n",
        "print(f\"   Using {nlist} clusters\")\n",
        "\n",
        "# Create quantizer and index\n",
        "quantizer = faiss.IndexFlatIP(embedding_dim)\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
        "\n",
        "# Train the index\n",
        "print(\"   Training index...\")\n",
        "index_ivf.train(doc_embeddings_f32)\n",
        "\n",
        "# Add vectors\n",
        "print(\"   Adding vectors...\")\n",
        "index_ivf.add(doc_embeddings_f32)\n",
        "\n",
        "# Set search parameters\n",
        "index_ivf.nprobe = min(32, nlist // 4)  # Number of clusters to search\n",
        "ivf_build_time = time.time() - start_time\n",
        "print(f\"   ✅ Built in {ivf_build_time:.2f} seconds\")\n",
        "print(f\"   Search will probe {index_ivf.nprobe} clusters\")\n",
        "\n",
        "# 3. HNSW Index (hierarchical navigable small world)\n",
        "print(\"3. Building IndexHNSWFlat (graph-based search)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create HNSW index\n",
        "M = 16  # Number of connections\n",
        "index_hnsw = faiss.IndexHNSWFlat(embedding_dim, M)\n",
        "index_hnsw.hnsw.efConstruction = 200  # Construction parameter\n",
        "index_hnsw.hnsw.efSearch = 128  # Search parameter\n",
        "\n",
        "# Add vectors\n",
        "index_hnsw.add(doc_embeddings_f32)\n",
        "\n",
        "hnsw_build_time = time.time() - start_time\n",
        "print(f\"   ✅ Built in {hnsw_build_time:.2f} seconds\")\n",
        "print(f\"   M={M}, efConstruction={index_hnsw.hnsw.efConstruction}, efSearch={index_hnsw.hnsw.efSearch}\")\n",
        "\n",
        "# Store indices in a dictionary\n",
        "faiss_indices = {\n",
        "    'flat': index_flat,\n",
        "    'ivf': index_ivf,\n",
        "    'hnsw': index_hnsw\n",
        "}\n",
        "\n",
        "print(\"\\n=== FAISS INDEX SUMMARY ===\")\n",
        "print(f\"IndexFlatIP: {index_flat.ntotal:,} vectors, exact search\")\n",
        "print(f\"IndexIVFFlat: {index_ivf.ntotal:,} vectors, {nlist} clusters, approximate search\")\n",
        "print(f\"IndexHNSWFlat: {index_hnsw.ntotal:,} vectors, graph-based search\")\n",
        "\n",
        "print(\"\\n✅ All FAISS indices built successfully!\")\n",
        "\n",
        "# Quick test search\n",
        "print(\"\\n=== QUICK SEARCH TEST ===\")\n",
        "test_query = query_embeddings_f32[0:1]\n",
        "k = 5\n",
        "\n",
        "for name, idx in faiss_indices.items():\n",
        "    start_time = time.time()\n",
        "    scores, indices = idx.search(test_query, k)\n",
        "    search_time = time.time() - start_time\n",
        "    print(f\"{name.upper()}: Found {len(indices[0])} results in {search_time*1000:.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_with_faiss"
      },
      "source": [
        "## Step 10: Comprehensive Evaluation with FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eval_with_faiss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d750745-7e98-49d6-bfff-a0a9805aa1c9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EVALUATION WITH FAISS INDICES ===\n",
            "\n",
            "Evaluating flat index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating flat: 100%|██████████| 5000/5000 [06:25<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating ivf index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating ivf: 100%|██████████| 5000/5000 [00:22<00:00, 224.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating hnsw index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating hnsw: 100%|██████████| 5000/5000 [00:03<00:00, 1301.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FAISS EVALUATION RESULTS ===\n",
            "Index           MAP      P@5      P@10     R@5      R@10     Time(ms)  \n",
            "---------------------------------------------------------------------------\n",
            "FLAT            0.8454   0.2439   0.1325   0.9123   0.9498   76.08     \n",
            "IVF             0.8458   0.2427   0.1319   0.9076   0.9450   4.27      \n",
            "HNSW            0.8455   0.2438   0.1325   0.9121   0.9496   0.67      \n",
            "\n",
            "✅ FAISS evaluation completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== EVALUATION WITH FAISS INDICES ===\")\n",
        "\n",
        "def evaluate_with_faiss(index, index_name, k_values=[5, 10, 20, 50, 100]):\n",
        "    \"\"\"Evaluate retrieval performance using FAISS index\"\"\"\n",
        "    print(f\"\\nEvaluating {index_name} index...\")\n",
        "\n",
        "    if 'qrels' not in datasets or len(datasets['qrels']) == 0:\n",
        "        print(\"⚠️ No qrels data available for evaluation\")\n",
        "        return None\n",
        "\n",
        "    # Convert qrels to dictionary\n",
        "    qrels = defaultdict(dict)\n",
        "    for _, row in datasets['qrels'].iterrows():\n",
        "        qid = str(row['query_id'])\n",
        "        did = str(row['doc_id'])\n",
        "        qrels[qid][did] = int(row['relevance'])\n",
        "\n",
        "    # Create ID mappings\n",
        "    query_id_to_idx = {str(qid): i for i, qid in enumerate(query_ids)}\n",
        "    doc_id_to_idx = {str(did): i for i, did in enumerate(doc_ids)}\n",
        "\n",
        "    # Evaluation metrics storage\n",
        "    results = {\n",
        "        'map_scores': [],\n",
        "        'precision_at_k': {k: [] for k in k_values},\n",
        "        'recall_at_k': {k: [] for k in k_values},\n",
        "        'search_times': []\n",
        "    }\n",
        "\n",
        "    # Evaluate each query\n",
        "    evaluated_queries = 0\n",
        "    total_queries = len(qrels)\n",
        "\n",
        "    for qid, relevant_docs in tqdm(qrels.items(), desc=f\"Evaluating {index_name}\"):\n",
        "        if qid not in query_id_to_idx:\n",
        "            continue\n",
        "\n",
        "        query_idx = query_id_to_idx[qid]\n",
        "        query_embedding = query_embeddings_f32[query_idx:query_idx+1]\n",
        "\n",
        "        # Search with FAISS\n",
        "        start_time = time.time()\n",
        "        max_k = max(k_values)\n",
        "        distances, indices = index.search(query_embedding, max_k)\n",
        "        search_time = time.time() - start_time\n",
        "        results['search_times'].append(search_time)\n",
        "\n",
        "        # Get retrieved document IDs\n",
        "        retrieved_docs = [str(doc_ids[i]) for i in indices[0]]\n",
        "\n",
        "        # Calculate metrics\n",
        "        relevant_retrieved = 0\n",
        "        precisions = []\n",
        "\n",
        "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
        "            if doc_id in relevant_docs:\n",
        "                relevant_retrieved += 1\n",
        "                precisions.append(relevant_retrieved / rank)\n",
        "\n",
        "        # Average Precision\n",
        "        if precisions:\n",
        "            ap = sum(precisions) / len(relevant_docs)\n",
        "            results['map_scores'].append(ap)\n",
        "\n",
        "        # Precision and Recall at K\n",
        "        for k in k_values:\n",
        "            if k <= len(retrieved_docs):\n",
        "                top_k_docs = retrieved_docs[:k]\n",
        "                relevant_in_top_k = sum(1 for doc in top_k_docs if doc in relevant_docs)\n",
        "\n",
        "                precision_at_k = relevant_in_top_k / k\n",
        "                recall_at_k = relevant_in_top_k / len(relevant_docs)\n",
        "\n",
        "                results['precision_at_k'][k].append(precision_at_k)\n",
        "                results['recall_at_k'][k].append(recall_at_k)\n",
        "\n",
        "        evaluated_queries += 1\n",
        "\n",
        "    # Calculate final metrics\n",
        "    if results['map_scores']:\n",
        "        final_results = {\n",
        "            'index_name': index_name,\n",
        "            'map': np.mean(results['map_scores']),\n",
        "            'avg_search_time': np.mean(results['search_times']) * 1000,  # in ms\n",
        "            'precision_at_k': {},\n",
        "            'recall_at_k': {},\n",
        "            'evaluated_queries': evaluated_queries\n",
        "        }\n",
        "\n",
        "        for k in k_values:\n",
        "            if results['precision_at_k'][k]:\n",
        "                final_results['precision_at_k'][k] = np.mean(results['precision_at_k'][k])\n",
        "                final_results['recall_at_k'][k] = np.mean(results['recall_at_k'][k])\n",
        "\n",
        "        return final_results\n",
        "    else:\n",
        "        print(f\"⚠️ No relevant documents found for {index_name}\")\n",
        "        return None\n",
        "\n",
        "# Evaluate all FAISS indices\n",
        "faiss_results = {}\n",
        "\n",
        "for index_name, index in faiss_indices.items():\n",
        "    result = evaluate_with_faiss(index, index_name)\n",
        "    if result:\n",
        "        faiss_results[index_name] = result\n",
        "\n",
        "# Display results\n",
        "print(\"\\n=== FAISS EVALUATION RESULTS ===\")\n",
        "print(f\"{'Index':<15} {'MAP':<8} {'P@5':<8} {'P@10':<8} {'R@5':<8} {'R@10':<8} {'Time(ms)':<10}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for name, result in faiss_results.items():\n",
        "    map_score = result['map']\n",
        "    p5 = result['precision_at_k'].get(5, 0)\n",
        "    p10 = result['precision_at_k'].get(10, 0)\n",
        "    r5 = result['recall_at_k'].get(5, 0)\n",
        "    r10 = result['recall_at_k'].get(10, 0)\n",
        "    time_ms = result['avg_search_time']\n",
        "\n",
        "    print(f\"{name.upper():<15} {map_score:<8.4f} {p5:<8.4f} {p10:<8.4f} {r5:<8.4f} {r10:<8.4f} {time_ms:<10.2f}\")\n",
        "\n",
        "print(\"\\n✅ FAISS evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_without_faiss"
      },
      "source": [
        "## Step 11: Evaluation without FAISS (Traditional Cosine Similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "eval_without_faiss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "ce838635-8955-49f0-8e09-8654f72a668d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EVALUATION WITHOUT FAISS (COSINE SIMILARITY) ===\n",
            "Running evaluation with traditional cosine similarity...\n",
            "Sampling 1000 queries out of 5000 for efficiency\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating with cosine similarity:  14%|█▍        | 142/1000 [02:17<13:51,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-340904930.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Run evaluation without FAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running evaluation with traditional cosine similarity...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtraditional_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_without_faiss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sample for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtraditional_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-27-340904930.py\u001b[0m in \u001b[0;36mevaluate_without_faiss\u001b[0;34m(k_values, sample_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Calculate cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Get top-k documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         Y = check_array(\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# error message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfirst_pass_isfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_pass_isfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"=== EVALUATION WITHOUT FAISS (COSINE SIMILARITY) ===\")\n",
        "\n",
        "def evaluate_without_faiss(k_values=[5, 10, 20, 50, 100], sample_size=None):\n",
        "    \"\"\"Evaluate retrieval performance using traditional cosine similarity\"\"\"\n",
        "\n",
        "    if 'qrels' not in datasets or len(datasets['qrels']) == 0:\n",
        "        print(\"⚠️ No qrels data available for evaluation\")\n",
        "        return None\n",
        "\n",
        "    # Convert qrels to dictionary\n",
        "    qrels = defaultdict(dict)\n",
        "    for _, row in datasets['qrels'].iterrows():\n",
        "        qid = str(row['query_id'])\n",
        "        did = str(row['doc_id'])\n",
        "        qrels[qid][did] = int(row['relevance'])\n",
        "\n",
        "    # Create ID mappings\n",
        "    query_id_to_idx = {str(qid): i for i, qid in enumerate(query_ids)}\n",
        "    doc_id_to_idx = {str(did): i for i, did in enumerate(doc_ids)}\n",
        "\n",
        "    # Sample queries for efficiency (optional)\n",
        "    if sample_size and sample_size < len(qrels):\n",
        "        sampled_qrels = dict(list(qrels.items())[:sample_size])\n",
        "        print(f\"Sampling {sample_size} queries out of {len(qrels)} for efficiency\")\n",
        "        qrels = sampled_qrels\n",
        "\n",
        "    # Evaluation metrics storage\n",
        "    results = {\n",
        "        'map_scores': [],\n",
        "        'precision_at_k': {k: [] for k in k_values},\n",
        "        'recall_at_k': {k: [] for k in k_values},\n",
        "        'search_times': []\n",
        "    }\n",
        "\n",
        "    # Evaluate each query\n",
        "    evaluated_queries = 0\n",
        "\n",
        "    for qid, relevant_docs in tqdm(qrels.items(), desc=\"Evaluating with cosine similarity\"):\n",
        "        if qid not in query_id_to_idx:\n",
        "            continue\n",
        "\n",
        "        query_idx = query_id_to_idx[qid]\n",
        "        query_embedding = query_embeddings[query_idx:query_idx+1]\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        start_time = time.time()\n",
        "        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "        # Get top-k documents\n",
        "        max_k = max(k_values)\n",
        "        top_indices = np.argsort(similarities)[::-1][:max_k]\n",
        "        search_time = time.time() - start_time\n",
        "        results['search_times'].append(search_time)\n",
        "\n",
        "        # Get retrieved document IDs\n",
        "        retrieved_docs = [str(doc_ids[i]) for i in top_indices]\n",
        "\n",
        "        # Calculate metrics\n",
        "        relevant_retrieved = 0\n",
        "        precisions = []\n",
        "\n",
        "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
        "            if doc_id in relevant_docs:\n",
        "                relevant_retrieved += 1\n",
        "                precisions.append(relevant_retrieved / rank)\n",
        "\n",
        "        # Average Precision\n",
        "        if precisions:\n",
        "            ap = sum(precisions) / len(relevant_docs)\n",
        "            results['map_scores'].append(ap)\n",
        "\n",
        "        # Precision and Recall at K\n",
        "        for k in k_values:\n",
        "            if k <= len(retrieved_docs):\n",
        "                top_k_docs = retrieved_docs[:k]\n",
        "                relevant_in_top_k = sum(1 for doc in top_k_docs if doc in relevant_docs)\n",
        "\n",
        "                precision_at_k = relevant_in_top_k / k\n",
        "                recall_at_k = relevant_in_top_k / len(relevant_docs)\n",
        "\n",
        "                results['precision_at_k'][k].append(precision_at_k)\n",
        "                results['recall_at_k'][k].append(recall_at_k)\n",
        "\n",
        "        evaluated_queries += 1\n",
        "\n",
        "    # Calculate final metrics\n",
        "    if results['map_scores']:\n",
        "        final_results = {\n",
        "            'method': 'cosine_similarity',\n",
        "            'map': np.mean(results['map_scores']),\n",
        "            'avg_search_time': np.mean(results['search_times']) * 1000,  # in ms\n",
        "            'precision_at_k': {},\n",
        "            'recall_at_k': {},\n",
        "            'evaluated_queries': evaluated_queries\n",
        "        }\n",
        "\n",
        "        for k in k_values:\n",
        "            if results['precision_at_k'][k]:\n",
        "                final_results['precision_at_k'][k] = np.mean(results['precision_at_k'][k])\n",
        "                final_results['recall_at_k'][k] = np.mean(results['recall_at_k'][k])\n",
        "\n",
        "        return final_results\n",
        "    else:\n",
        "        print(\"⚠️ No relevant documents found\")\n",
        "        return None\n",
        "\n",
        "# Run evaluation without FAISS\n",
        "print(\"Running evaluation with traditional cosine similarity...\")\n",
        "traditional_results = evaluate_without_faiss(sample_size=1000)  # Sample for efficiency\n",
        "\n",
        "if traditional_results:\n",
        "    print(\"\\n=== TRADITIONAL COSINE SIMILARITY RESULTS ===\")\n",
        "    print(f\"Method: {traditional_results['method']}\")\n",
        "    print(f\"MAP: {traditional_results['map']:.4f}\")\n",
        "    print(f\"Average search time: {traditional_results['avg_search_time']:.2f} ms\")\n",
        "    print(f\"Evaluated queries: {traditional_results['evaluated_queries']}\")\n",
        "\n",
        "    print(\"\\nPrecision at K:\")\n",
        "    for k, precision in traditional_results['precision_at_k'].items():\n",
        "        print(f\"  P@{k}: {precision:.4f}\")\n",
        "\n",
        "    print(\"\\nRecall at K:\")\n",
        "    for k, recall in traditional_results['recall_at_k'].items():\n",
        "        print(f\"  R@{k}: {recall:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Traditional evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_analysis"
      },
      "source": [
        "## Step 12: Comparison Analysis - FAISS vs Traditional Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "comparison",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d576a419-c114-49f0-d6eb-0de95482b7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== COMPREHENSIVE COMPARISON ANALYSIS ===\n",
            "\n",
            "📊 PERFORMANCE COMPARISON\n",
            "\n",
            "Method               MAP      P@5      P@10     R@5      R@10     Time(ms)  \n",
            "================================================================================\n",
            "Cosine Similarity    0.8086   0.2790   0.1603   0.8703   0.9217   825.46    \n",
            "FAISS FLAT           0.8454   0.2439   0.1325   0.9123   0.9498   76.08     \n",
            "FAISS IVF            0.8458   0.2427   0.1319   0.9076   0.9450   4.27      \n",
            "FAISS HNSW           0.8455   0.2438   0.1325   0.9121   0.9496   0.67      \n",
            "\n",
            "🔍 ANALYSIS:\n",
            "\n",
            "📈 Best MAP Score: 0.8458\n",
            "⚡ Fastest Search: 0.67 ms\n",
            "\n",
            "⚡ SPEED COMPARISON:\n",
            "  FAISS FLAT: 10.85x faster than cosine similarity\n",
            "  FAISS IVF: 193.20x faster than cosine similarity\n",
            "  FAISS HNSW: 1225.06x faster than cosine similarity\n",
            "\n",
            "🎯 ACCURACY COMPARISON:\n",
            "  FAISS FLAT: 1.045x accuracy (+3.67% difference)\n",
            "  FAISS IVF: 1.046x accuracy (+3.72% difference)\n",
            "  FAISS HNSW: 1.046x accuracy (+3.69% difference)\n",
            "\n",
            "💡 RECOMMENDATIONS:\n",
            "  • IndexFlatIP: Best accuracy, exact search, slower for large datasets\n",
            "  • IndexIVFFlat: Good balance of speed and accuracy, suitable for large datasets\n",
            "  • IndexHNSWFlat: Fast search, good for real-time applications\n",
            "  • Cosine Similarity: Baseline method, exact results, can be slow\n",
            "\n",
            "✅ Comparison analysis completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"=== COMPREHENSIVE COMPARISON ANALYSIS ===\")\n",
        "\n",
        "# Create comparison table\n",
        "if faiss_results and traditional_results:\n",
        "    print(\"\\n📊 PERFORMANCE COMPARISON\\n\")\n",
        "    print(f\"{'Method':<20} {'MAP':<8} {'P@5':<8} {'P@10':<8} {'R@5':<8} {'R@10':<8} {'Time(ms)':<10}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Traditional method\n",
        "    t_map = traditional_results['map']\n",
        "    t_p5 = traditional_results['precision_at_k'].get(5, 0)\n",
        "    t_p10 = traditional_results['precision_at_k'].get(10, 0)\n",
        "    t_r5 = traditional_results['recall_at_k'].get(5, 0)\n",
        "    t_r10 = traditional_results['recall_at_k'].get(10, 0)\n",
        "    t_time = traditional_results['avg_search_time']\n",
        "\n",
        "    print(f\"{'Cosine Similarity':<20} {t_map:<8.4f} {t_p5:<8.4f} {t_p10:<8.4f} {t_r5:<8.4f} {t_r10:<8.4f} {t_time:<10.2f}\")\n",
        "\n",
        "    # FAISS methods\n",
        "    for name, result in faiss_results.items():\n",
        "        f_map = result['map']\n",
        "        f_p5 = result['precision_at_k'].get(5, 0)\n",
        "        f_p10 = result['precision_at_k'].get(10, 0)\n",
        "        f_r5 = result['recall_at_k'].get(5, 0)\n",
        "        f_r10 = result['recall_at_k'].get(10, 0)\n",
        "        f_time = result['avg_search_time']\n",
        "\n",
        "        method_name = f\"FAISS {name.upper()}\"\n",
        "        print(f\"{method_name:<20} {f_map:<8.4f} {f_p5:<8.4f} {f_p10:<8.4f} {f_r5:<8.4f} {f_r10:<8.4f} {f_time:<10.2f}\")\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n🔍 ANALYSIS:\\n\")\n",
        "\n",
        "    # Find best performing method\n",
        "    best_map = max(traditional_results['map'], max(r['map'] for r in faiss_results.values()))\n",
        "    fastest_method = min(traditional_results['avg_search_time'], min(r['avg_search_time'] for r in faiss_results.values()))\n",
        "\n",
        "    print(f\"📈 Best MAP Score: {best_map:.4f}\")\n",
        "    print(f\"⚡ Fastest Search: {fastest_method:.2f} ms\")\n",
        "\n",
        "    # Speed comparison\n",
        "    print(\"\\n⚡ SPEED COMPARISON:\")\n",
        "    baseline_time = traditional_results['avg_search_time']\n",
        "\n",
        "    for name, result in faiss_results.items():\n",
        "        speedup = baseline_time / result['avg_search_time']\n",
        "        print(f\"  FAISS {name.upper()}: {speedup:.2f}x faster than cosine similarity\")\n",
        "\n",
        "    # Accuracy comparison\n",
        "    print(\"\\n🎯 ACCURACY COMPARISON:\")\n",
        "    baseline_map = traditional_results['map']\n",
        "\n",
        "    for name, result in faiss_results.items():\n",
        "        accuracy_ratio = result['map'] / baseline_map\n",
        "        accuracy_diff = (result['map'] - baseline_map) * 100\n",
        "        print(f\"  FAISS {name.upper()}: {accuracy_ratio:.3f}x accuracy ({accuracy_diff:+.2f}% difference)\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n💡 RECOMMENDATIONS:\")\n",
        "    print(\"  • IndexFlatIP: Best accuracy, exact search, slower for large datasets\")\n",
        "    print(\"  • IndexIVFFlat: Good balance of speed and accuracy, suitable for large datasets\")\n",
        "    print(\"  • IndexHNSWFlat: Fast search, good for real-time applications\")\n",
        "    print(\"  • Cosine Similarity: Baseline method, exact results, can be slow\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Cannot perform comparison - missing evaluation results\")\n",
        "\n",
        "print(\"\\n✅ Comparison analysis completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_faiss"
      },
      "source": [
        "## Step 13: Save and Download FAISS Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "save_download_faiss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "34634a13-32e0-4a1e-d4d8-587ac8243de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SAVING FAISS INDICES ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-248984682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create directory for FAISS indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfaiss_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{save_dir}/faiss_indices'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_dir' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"=== SAVING FAISS INDICES ===\")\n",
        "\n",
        "# Create directory for FAISS indices\n",
        "faiss_dir = f'{save_dir}/faiss_indices'\n",
        "if not os.path.exists(faiss_dir):\n",
        "    os.makedirs(faiss_dir)\n",
        "    print(f\"Created directory: {faiss_dir}\")\n",
        "\n",
        "# Save each FAISS index\n",
        "saved_indices = {}\n",
        "\n",
        "for name, index in faiss_indices.items():\n",
        "    index_path = f'{faiss_dir}/{name}_index.faiss'\n",
        "    faiss.write_index(index, index_path)\n",
        "    saved_indices[name] = index_path\n",
        "    print(f\"✅ Saved {name} index to: {index_path}\")\n",
        "\n",
        "# Store HNSW parameters separately if the index exists\n",
        "hnsw_params = {}\n",
        "if 'index_hnsw' in locals() and index_hnsw.is_trained:\n",
        "    # Access parameters if the index is trained and attributes exist\n",
        "    if hasattr(index_hnsw.hnsw, 'M'):\n",
        "        hnsw_params['M'] = index_hnsw.hnsw.M\n",
        "    if hasattr(index_hnsw.hnsw, 'efConstruction'):\n",
        "        hnsw_params['efConstruction'] = index_hnsw.hnsw.efConstruction\n",
        "    if hasattr(index_hnsw.hnsw, 'efSearch'):\n",
        "        hnsw_params['efSearch'] = index_hnsw.hnsw.efSearch\n",
        "\n",
        "\n",
        "# Save index metadata\n",
        "faiss_metadata = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'num_documents': len(doc_embeddings),\n",
        "    'doc_ids': doc_ids,\n",
        "    'query_ids': query_ids,\n",
        "    'model_name': MODEL_NAME,\n",
        "    'indices': {\n",
        "        'flat': {\n",
        "            'type': 'IndexFlatIP',\n",
        "            'description': 'Exact search using inner product',\n",
        "            'file': 'flat_index.faiss' # Corrected filename here\n",
        "        },\n",
        "        'ivf': {\n",
        "            'type': 'IndexIVFFlat',\n",
        "            'description': f'Approximate search with {index_ivf.nlist} clusters',\n",
        "            'nlist': index_ivf.nlist,\n",
        "            'nprobe': index_ivf.nprobe,\n",
        "            'file': 'ivf_index.faiss' # Corrected filename here\n",
        "        },\n",
        "        'hnsw': {\n",
        "            'type': 'IndexHNSWFlat',\n",
        "            'description': 'Graph-based search',\n",
        "            'M': hnsw_params.get('M', 'N/A'), # Access from stored parameters\n",
        "            'efConstruction': hnsw_params.get('efConstruction', 'N/A'), # Access from stored parameters\n",
        "            'efSearch': hnsw_params.get('efSearch', 'N/A'), # Access from stored parameters\n",
        "            'file': 'hnsw_index.faiss' # Corrected filename here\n",
        "        }\n",
        "    },\n",
        "    'evaluation_results': {\n",
        "        'faiss': faiss_results if 'faiss_results' in locals() else None,\n",
        "        'traditional': traditional_results if 'traditional_results' in locals() else None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save metadata\n",
        "metadata_path = f'{faiss_dir}/faiss_metadata.joblib'\n",
        "joblib.dump(faiss_metadata, metadata_path)\n",
        "print(f\"✅ Saved FAISS metadata to: {metadata_path}\")\n",
        "\n",
        "# Create usage example script\n",
        "# Use a regular string and format it later or escape braces\n",
        "usage_script = f\"\"\"\n",
        "# FAISS Index Usage Example\n",
        "# This script shows how to load and use the saved FAISS indices\n",
        "\n",
        "import faiss\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load metadata\n",
        "metadata = joblib.load('faiss_metadata.joblib')\n",
        "print(f\"Loaded metadata for {{metadata['num_documents']:,}} documents\")\n",
        "\n",
        "# Load FAISS indices\n",
        "indices = {{}}\n",
        "for name in ['flat', 'ivf', 'hnsw']:\n",
        "    index_path = f'{{name}}_index.faiss'\n",
        "    indices[name] = faiss.read_index(index_path)\n",
        "    print(f\"Loaded {{name}} index with {{indices[name].ntotal}} vectors\")\n",
        "\n",
        "# Example search\n",
        "def search_example(query_embedding, k=10):\n",
        "    \\\"\\\"\\\"Example search function\\\"\\\"\\\" # Escaped quotes here\n",
        "    query_embedding = query_embedding.astype(np.float32)\n",
        "    if len(query_embedding.shape) == 1:\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "    results = {{}}\n",
        "    for name, index in indices.items():\n",
        "        scores, doc_indices = index.search(query_embedding, k)\n",
        "        results[name] = {{\n",
        "            'scores': scores[0],\n",
        "            'doc_indices': doc_indices[0],\n",
        "            'doc_ids': [metadata['doc_ids'][i] for i in doc_indices[0]]\n",
        "        }}\n",
        "    return results\n",
        "\n",
        "# Usage:\n",
        "# results = search_example(your_query_embedding)\n",
        "# print(results['flat']['doc_ids'][:5])  # Top 5 document IDs\n",
        "\"\"\"\n",
        "\n",
        "with open(f'{faiss_dir}/usage_example.py', 'w') as f:\n",
        "    f.write(usage_script)\n",
        "\n",
        "print(f\"✅ Created usage example: {faiss_dir}/usage_example.py\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "about_faiss"
      },
      "source": [
        "## About FAISS as a Vector Store\n",
        "\n",
        "**Yes, FAISS is definitely considered one of the leading vector stores/databases!** Here's what changes when you apply FAISS to your Quora embeddings:\n",
        "\n",
        "### What is FAISS?\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It's widely used as a vector store for:\n",
        "\n",
        "- **Semantic Search**: Finding similar documents/texts\n",
        "- **Recommendation Systems**: Finding similar items\n",
        "- **RAG Applications**: Retrieval-Augmented Generation\n",
        "- **Large-scale ML**: Handling millions/billions of vectors\n",
        "\n",
        "### Changes After Applying FAISS:\n",
        "\n",
        "#### 1. **Performance Improvements**\n",
        "- **Speed**: 10-1000x faster than brute-force cosine similarity\n",
        "- **Memory**: More efficient memory usage\n",
        "- **Scalability**: Can handle billions of vectors\n",
        "\n",
        "#### 2. **Search Options**\n",
        "- **Exact Search**: IndexFlatIP (same results as cosine similarity)\n",
        "- **Approximate Search**: IndexIVF* (99% accuracy, much faster)\n",
        "- **Graph Search**: IndexHNSW* (very fast, good for real-time)\n",
        "\n",
        "#### 3. **Production Ready**\n",
        "- **Persistence**: Save/load indices to disk\n",
        "- **GPU Support**: Leverage GPU acceleration\n",
        "- **Batch Processing**: Efficient batch searches\n",
        "\n",
        "#### 4. **Trade-offs**\n",
        "- **Accuracy vs Speed**: Approximate methods trade slight accuracy for speed\n",
        "- **Memory vs Speed**: Different indices have different memory requirements\n",
        "- **Build Time**: Index construction takes time but search is much faster\n",
        "\n",
        "### FAISS vs Other Vector Stores\n",
        "\n",
        "| Feature | FAISS | Pinecone | Weaviate | Chroma |\n",
        "|---------|-------|----------|----------|--------|\n",
        "| **Type** | Library | SaaS | Open Source | Open Source |\n",
        "| **Hosting** | Self-hosted | Cloud | Self/Cloud | Self-hosted |\n",
        "| **Scale** | Billions | Millions | Millions | Millions |\n",
        "| **Cost** | Free | Paid | Free/Paid | Free |\n",
        "| **Speed** | Fastest | Fast | Fast | Fast |\n",
        "| **Features** | Core search | Full service | Graph + Vector | Simple |\n",
        "\n",
        "### When to Use FAISS\n",
        "\n",
        "✅ **Use FAISS when:**\n",
        "- You need maximum performance\n",
        "- You have large datasets (>100K vectors)\n",
        "- You want fine-grained control\n",
        "- You're building production systems\n",
        "- You need offline/local processing\n",
        "\n",
        "❌ **Don't use FAISS when:**\n",
        "- You need a full database with metadata\n",
        "- You want managed service\n",
        "- You need advanced querying (filters, etc.)\n",
        "- You have very small datasets (<10K vectors)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44c065afd5604fb3ae111b10d4fc8a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eaab8776cc094f52ba00255ff2c9586b",
              "IPY_MODEL_d9e2b0d6a0a9401ab052d80f4c9f5c7c",
              "IPY_MODEL_ff8d3b2aae0341b1a8dc60d11c6c1f84"
            ],
            "layout": "IPY_MODEL_5bf5f7915f5445b182fab4170df50b19"
          }
        },
        "eaab8776cc094f52ba00255ff2c9586b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43663d8f4491419daa96634732d6a4f0",
            "placeholder": "​",
            "style": "IPY_MODEL_21fc3ed5d43a413cbbfda216d7886509",
            "value": "Batches: 100%"
          }
        },
        "d9e2b0d6a0a9401ab052d80f4c9f5c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347c0cf803de46479b40189110b65d99",
            "max": 8169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136fd323a8fe4aa2baf357faa669e629",
            "value": 8169
          }
        },
        "ff8d3b2aae0341b1a8dc60d11c6c1f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca24fe4fd994519b3f3aa06def61351",
            "placeholder": "​",
            "style": "IPY_MODEL_57ce7292ada34e94a99cf7a372b98936",
            "value": " 8169/8169 [02:20&lt;00:00, 116.76it/s]"
          }
        },
        "5bf5f7915f5445b182fab4170df50b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43663d8f4491419daa96634732d6a4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21fc3ed5d43a413cbbfda216d7886509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "347c0cf803de46479b40189110b65d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136fd323a8fe4aa2baf357faa669e629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eca24fe4fd994519b3f3aa06def61351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57ce7292ada34e94a99cf7a372b98936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9119175d5c344f9bc7c4cd38d69fbae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_978b27fdd3aa4d31a46dfe59c1af6474",
              "IPY_MODEL_6948920445fe405f934457592e7aaccc",
              "IPY_MODEL_ce0619743ab1410b8df4c2fa247fe2df"
            ],
            "layout": "IPY_MODEL_eaa3a29c2fd4470c8367bf6314ca7b90"
          }
        },
        "978b27fdd3aa4d31a46dfe59c1af6474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d47d13282b47aca094075f48ead92f",
            "placeholder": "​",
            "style": "IPY_MODEL_796ffe0642184a208a3b4cccf11694fd",
            "value": "Batches: 100%"
          }
        },
        "6948920445fe405f934457592e7aaccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2fa92789154434fa4b263cd3c9c5b83",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eebd8367f39426fad4ef1347257dc32",
            "value": 79
          }
        },
        "ce0619743ab1410b8df4c2fa247fe2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74adade21e58426f9943de687bf19cee",
            "placeholder": "​",
            "style": "IPY_MODEL_eff560b4f3834f0a9d26907e1e5a1c8b",
            "value": " 79/79 [00:01&lt;00:00, 82.80it/s]"
          }
        },
        "eaa3a29c2fd4470c8367bf6314ca7b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d47d13282b47aca094075f48ead92f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796ffe0642184a208a3b4cccf11694fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2fa92789154434fa4b263cd3c9c5b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eebd8367f39426fad4ef1347257dc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74adade21e58426f9943de687bf19cee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff560b4f3834f0a9d26907e1e5a1c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}