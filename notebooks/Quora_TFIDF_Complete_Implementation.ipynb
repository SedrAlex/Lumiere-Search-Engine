{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Quora Dataset TF-IDF Implementation with Advanced Text Cleaning\n",
        "\n",
        "This notebook implements TF-IDF vectorization on the Quora dataset with:\n",
        "- **Advanced custom text cleaning optimized for Quora question pairs**\n",
        "- **Custom tokenization with semantic preservation**\n",
        "- **Inverted index construction**\n",
        "- **Model persistence using joblib**\n",
        "- **Evaluation using MAP metric (target: ≥ 0.3)**\n",
        "\n",
        "## Dataset Structure\n",
        "- Documents: `/content/drive/MyDrive/downloads/docs.csv`\n",
        "- Queries: `/content/drive/MyDrive/downloads/queries.csv`\n",
        "- Relevance judgments: `/content/drive/MyDrive/downloads/qrels.csv`\n",
        "\n",
        "## Key Optimizations for Quora\n",
        "- Question-specific text preprocessing\n",
        "- Preservation of question markers (what, how, why, etc.)\n",
        "- Handling of duplicate question patterns\n",
        "- Optimized n-gram features for question matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_packages",
        "outputId": "a272731c-a672-4e42-ce39-806abf3ce285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install nltk scikit-learn pandas numpy joblib tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "ce9cf621-f2b4-40e7-bd18-c4b8c9534a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import joblib\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab') # Added this line\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## 2. Data Loading and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data",
        "outputId": "e09792db-6028-4fe2-c42f-295b8f310ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found: /content/drive/MyDrive/downloads/docs.tsv\n",
            "✓ Found: /content/drive/MyDrive/downloads/queries.tsv\n",
            "✓ Found: /content/drive/MyDrive/downloads/qrels.tsv\n",
            "\n",
            "Loading datasets...\n",
            "Documents: 522770 rows\n",
            "Queries: 5000 rows\n",
            "Qrels: 7626 rows\n",
            "\n",
            "Document columns: ['doc_id', 'text']\n",
            "Query columns: ['query_id', 'text']\n",
            "Qrels columns: ['query_id', 'doc_id', 'relevance']\n",
            "\n",
            "Sample document:\n",
            "   doc_id                                               text\n",
            "0       1  What is the step by step guide to invest in sh...\n",
            "\n",
            "Sample query:\n",
            "   query_id                                 text\n",
            "0       318  How does Quora look to a moderator?\n",
            "\n",
            "Sample qrel:\n",
            "   query_id  doc_id  relevance\n",
            "0       318     317          1\n"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "DATA_PATH = '/content/drive/MyDrive/downloads/'\n",
        "DOCS_FILE = os.path.join(DATA_PATH, 'docs.tsv')\n",
        "QUERIES_FILE = os.path.join(DATA_PATH, 'queries.tsv')\n",
        "QRELS_FILE = os.path.join(DATA_PATH, 'qrels.tsv')\n",
        "\n",
        "# Verify files exist\n",
        "files_to_check = [DOCS_FILE, QUERIES_FILE, QRELS_FILE]\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"✓ Found: {file_path}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file_path}\")\n",
        "\n",
        "# Load datasets\n",
        "print(\"\\nLoading datasets...\")\n",
        "docs_df = pd.read_csv(DOCS_FILE, sep='\\t')\n",
        "queries_df = pd.read_csv(QUERIES_FILE, sep='\\t')\n",
        "qrels_df = pd.read_csv(QRELS_FILE, sep='\\t')\n",
        "\n",
        "print(f\"Documents: {len(docs_df)} rows\")\n",
        "print(f\"Queries: {len(queries_df)} rows\")\n",
        "print(f\"Qrels: {len(qrels_df)} rows\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nDocument columns:\", docs_df.columns.tolist())\n",
        "print(\"Query columns:\", queries_df.columns.tolist())\n",
        "print(\"Qrels columns:\", qrels_df.columns.tolist())\n",
        "\n",
        "print(\"\\nSample document:\")\n",
        "print(docs_df.head(1))\n",
        "\n",
        "print(\"\\nSample query:\")\n",
        "print(queries_df.head(1))\n",
        "\n",
        "print(\"\\nSample qrel:\")\n",
        "print(qrels_df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "## 3. Advanced Text Cleaning for Quora Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "text_cleaning_functions",
        "outputId": "d6661867-1f7d-4582-e004-357e93fcfa3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: What's the best way to learn machine learning? How can I improve my programming skills?\n",
            "Cleaned text: how to learn machine learning how to improve my programming skills\n",
            "Tokens: ['how', 'learn', 'machine', 'learning', 'how', 'improve', 'programming', 'skill']\n",
            "\n",
            "Quora-optimized text cleaning functions ready!\n"
          ]
        }
      ],
      "source": [
        "class QuoraTextCleaner:\n",
        "    \"\"\"\n",
        "    Advanced text cleaning class optimized for Quora question pairs\n",
        "    with semantic preservation and question-specific optimizations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Setup stopwords with exceptions for important question words\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Remove question words and semantic indicators that are crucial for Quora\n",
        "        question_words = {\n",
        "            'what', 'when', 'where', 'why', 'who', 'which', 'how',\n",
        "            'can', 'could', 'would', 'should', 'will', 'shall',\n",
        "            'do', 'does', 'did', 'is', 'are', 'was', 'were',\n",
        "            'not', 'no', 'never', 'none', 'nothing', 'neither',\n",
        "            'more', 'most', 'less', 'least', 'very', 'quite',\n",
        "            'much', 'many', 'few', 'some', 'any', 'all',\n",
        "            'best', 'better', 'good', 'bad', 'right', 'wrong'\n",
        "        }\n",
        "        self.stop_words = self.stop_words - question_words\n",
        "\n",
        "        # Initialize lemmatizer\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # Common contractions for question text\n",
        "        self.contractions = {\n",
        "            \"don't\": \"do not\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"can't\": \"cannot\",\n",
        "            \"n't\": \" not\",\n",
        "            \"'re\": \" are\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'ll\": \" will\",\n",
        "            \"'d\": \" would\",\n",
        "            \"'m\": \" am\",\n",
        "            \"what's\": \"what is\",\n",
        "            \"that's\": \"that is\",\n",
        "            \"there's\": \"there is\",\n",
        "            \"here's\": \"here is\",\n",
        "            \"where's\": \"where is\",\n",
        "            \"how's\": \"how is\"\n",
        "        }\n",
        "\n",
        "        # Question patterns that should be normalized\n",
        "        self.question_patterns = {\n",
        "            r'\\bhow do i\\b': 'how to',\n",
        "            r'\\bhow can i\\b': 'how to',\n",
        "            r'\\bhow should i\\b': 'how to',\n",
        "            r'\\bwhat is the best way to\\b': 'how to',\n",
        "            r'\\bwhat are the ways to\\b': 'how to',\n",
        "            r'\\bwhat are some\\b': 'what are',\n",
        "            r'\\bwhat are the\\b': 'what are'\n",
        "        }\n",
        "\n",
        "    def smart_clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Enhanced text cleaning optimized for Quora questions.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to clean\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned text\n",
        "        \"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Expand contractions\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "\n",
        "        # Normalize question patterns\n",
        "        for pattern, replacement in self.question_patterns.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "\n",
        "        # Remove or normalize specific patterns\n",
        "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL ', text)\n",
        "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' EMAIL ', text)\n",
        "        text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "        # Handle numbers more intelligently for questions\n",
        "        text = re.sub(r'\\b(19|20)\\d{2}\\b', ' YEAR ', text)  # Years\n",
        "        text = re.sub(r'\\b\\d+\\.\\d+\\b', ' DECIMAL ', text)  # Decimals\n",
        "        text = re.sub(r'\\b\\d+(?:st|nd|rd|th)\\b', ' ORDINAL ', text)  # Ordinals\n",
        "        text = re.sub(r'\\b\\d+\\b', ' NUMBER ', text)  # Other numbers\n",
        "\n",
        "        # Handle emphasis and punctuation\n",
        "        text = re.sub(r'[!]{2,}', ' EMPHASIS ', text)\n",
        "        text = re.sub(r'[?]{2,}', ' MULTIQUEST ', text)\n",
        "        text = re.sub(r'[.]{3,}', ' ELLIPSIS ', text)\n",
        "\n",
        "        # Remove special characters but preserve some important ones\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\-\\'_]', ' ', text)\n",
        "\n",
        "        # Handle hyphenated words carefully (important for compound terms)\n",
        "        text = re.sub(r'\\b(\\w+)-(\\w+)\\b', r'\\1 \\2 \\1\\2', text)  # Keep both forms\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def custom_tokenizer(self, text):\n",
        "        \"\"\"\n",
        "        Custom tokenizer optimized for Quora questions.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            list: List of processed tokens\n",
        "        \"\"\"\n",
        "        # Clean the text first\n",
        "        cleaned_text = self.smart_clean_text(text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "        # Filter and lemmatize\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            # Skip very short tokens or stopwords\n",
        "            if len(token) < 2 or token in self.stop_words:\n",
        "                continue\n",
        "\n",
        "            # Skip tokens that are just underscores or dashes\n",
        "            if re.match(r'^[_\\-]+$', token):\n",
        "                continue\n",
        "\n",
        "            # Lemmatize\n",
        "            lemmatized = self.lemmatizer.lemmatize(token)\n",
        "            processed_tokens.append(lemmatized)\n",
        "\n",
        "        return processed_tokens\n",
        "\n",
        "# Initialize the text cleaner\n",
        "text_cleaner = QuoraTextCleaner()\n",
        "\n",
        "# Test the cleaning function\n",
        "sample_text = \"What's the best way to learn machine learning? How can I improve my programming skills?\"\n",
        "cleaned_sample = text_cleaner.smart_clean_text(sample_text)\n",
        "tokens_sample = text_cleaner.custom_tokenizer(sample_text)\n",
        "\n",
        "print(\"Original text:\", sample_text)\n",
        "print(\"Cleaned text:\", cleaned_sample)\n",
        "print(\"Tokens:\", tokens_sample)\n",
        "print(\"\\nQuora-optimized text cleaning functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preprocessing"
      },
      "source": [
        "## 4. Data Preprocessing and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocess_data",
        "outputId": "a2492eed-49a1-4a4e-c09c-4afb4312b82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing documents...\n",
            "Documents after cleaning: 522751\n",
            "Preprocessing queries...\n",
            "Queries after cleaning: 5000\n",
            "\n",
            "Data preprocessing complete!\n",
            "Final dataset sizes:\n",
            "- Documents: 522751\n",
            "- Queries: 5000\n",
            "- Qrels: 7626\n",
            "\n",
            "Sample cleaned document:\n",
            "Original: What is the step by step guide to invest in share market in india?...\n",
            "Cleaned: what is the step by step guide to invest in share market in india...\n",
            "\n",
            "Sample cleaned query:\n",
            "Original: How does Quora look to a moderator?\n",
            "Cleaned: how does quora look to a moderator\n"
          ]
        }
      ],
      "source": [
        "# Preprocess documents\n",
        "print(\"Preprocessing documents...\")\n",
        "\n",
        "# Handle different possible column names for documents\n",
        "doc_text_col = 'text' if 'text' in docs_df.columns else 'question' if 'question' in docs_df.columns else docs_df.columns[1]\n",
        "doc_id_col = 'doc_id' if 'doc_id' in docs_df.columns else 'id' if 'id' in docs_df.columns else docs_df.columns[0]\n",
        "\n",
        "docs_df['cleaned_text'] = docs_df[doc_text_col].apply(text_cleaner.smart_clean_text)\n",
        "docs_df['doc_id'] = docs_df[doc_id_col].astype(str)\n",
        "\n",
        "# Remove empty documents\n",
        "docs_df = docs_df[docs_df['cleaned_text'].str.len() > 0]\n",
        "print(f\"Documents after cleaning: {len(docs_df)}\")\n",
        "\n",
        "# Preprocess queries\n",
        "print(\"Preprocessing queries...\")\n",
        "\n",
        "# Handle different possible column names for queries\n",
        "query_text_col = 'query' if 'query' in queries_df.columns else 'text' if 'text' in queries_df.columns else 'question' if 'question' in queries_df.columns else queries_df.columns[1]\n",
        "query_id_col = 'query_id' if 'query_id' in queries_df.columns else 'id' if 'id' in queries_df.columns else queries_df.columns[0]\n",
        "\n",
        "queries_df['cleaned_query'] = queries_df[query_text_col].apply(text_cleaner.smart_clean_text)\n",
        "queries_df['query_id'] = queries_df[query_id_col].astype(str)\n",
        "\n",
        "# Remove empty queries\n",
        "queries_df = queries_df[queries_df['cleaned_query'].str.len() > 0]\n",
        "print(f\"Queries after cleaning: {len(queries_df)}\")\n",
        "\n",
        "# Prepare qrels\n",
        "qrels_columns = qrels_df.columns.tolist()\n",
        "if 'query_id' not in qrels_columns:\n",
        "    qrels_df['query_id'] = qrels_df[qrels_columns[0]].astype(str)\n",
        "if 'doc_id' not in qrels_columns:\n",
        "    qrels_df['doc_id'] = qrels_df[qrels_columns[1]].astype(str)\n",
        "\n",
        "qrels_df['query_id'] = qrels_df['query_id'].astype(str)\n",
        "qrels_df['doc_id'] = qrels_df['doc_id'].astype(str)\n",
        "\n",
        "print(\"\\nData preprocessing complete!\")\n",
        "print(f\"Final dataset sizes:\")\n",
        "print(f\"- Documents: {len(docs_df)}\")\n",
        "print(f\"- Queries: {len(queries_df)}\")\n",
        "print(f\"- Qrels: {len(qrels_df)}\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"\\nSample cleaned document:\")\n",
        "print(f\"Original: {docs_df.iloc[0][doc_text_col][:200]}...\")\n",
        "print(f\"Cleaned: {docs_df.iloc[0]['cleaned_text'][:200]}...\")\n",
        "\n",
        "print(\"\\nSample cleaned query:\")\n",
        "print(f\"Original: {queries_df.iloc[0][query_text_col]}\")\n",
        "print(f\"Cleaned: {queries_df.iloc[0]['cleaned_query']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_vectorization"
      },
      "source": [
        "## 5. TF-IDF Vectorization with Custom Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create_tfidf",
        "outputId": "c2ee92f7-f627-4f94-9709-345720f2034a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TF-IDF vectorizer optimized for Quora questions...\n",
            "Fitting TF-IDF vectorizer on documents...\n",
            "TF-IDF matrix shape: (522751, 12000)\n",
            "Number of features: 12000\n",
            "Matrix sparsity: 99.92%\n",
            "\n",
            "Sample features: [\"''\" \"'s\" \"'s best\" \"'s biggest\" \"'s birthday\" \"'s body\" \"'s book\"\n",
            " \"'s business\" \"'s cube\" \"'s day\" \"'s death\" \"'s decision\" \"'s degree\"\n",
            " \"'s economy\" \"'s first\" \"'s greatest\" \"'s home\" \"'s last\" \"'s law\"\n",
            " \"'s license\"]\n",
            "Sample bigrams: [\"'s best\", \"'s biggest\", \"'s birthday\", \"'s body\", \"'s book\", \"'s business\", \"'s cube\", \"'s day\", \"'s death\", \"'s decision\"]\n",
            "Question-related terms: [\"'s whatsapp\", 'account how', 'also what', 'answer why', 'anywhere', 'avoid when', 'backstory how', 'better why', 'book what', 'bring when', 'business what', 'card what', 'care what', 'child when', 'china what']\n"
          ]
        }
      ],
      "source": [
        "# Create TF-IDF vectorizer with custom preprocessing\n",
        "print(\"Creating TF-IDF vectorizer optimized for Quora questions...\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    preprocessor=None,  # We handle preprocessing ourselves\n",
        "    tokenizer=text_cleaner.custom_tokenizer,  # Use our custom tokenizer\n",
        "    token_pattern=None,  # Disable default tokenization\n",
        "    lowercase=False,  # Already handled in custom tokenizer\n",
        "    stop_words=None,  # Already handled in custom tokenizer\n",
        "    max_features=12000,  # Optimized vocabulary size for questions\n",
        "    min_df=1,  # Keep rare terms (important for specific questions)\n",
        "    max_df=0.85,  # Remove very common terms\n",
        "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    sublinear_tf=True,  # Apply sublinear TF scaling\n",
        "    norm='l2'  # L2 normalization\n",
        ")\n",
        "\n",
        "# Fit and transform documents\n",
        "print(\"Fitting TF-IDF vectorizer on documents...\")\n",
        "document_texts = docs_df['cleaned_text'].tolist()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(document_texts)\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
        "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "# Display sample features\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample features: {feature_names[:20]}\")\n",
        "bigrams = [f for f in feature_names if ' ' in f]\n",
        "print(f\"Sample bigrams: {bigrams[:10]}\")\n",
        "\n",
        "# Show some question-specific terms\n",
        "question_terms = [f for f in feature_names if any(q in f for q in ['what', 'how', 'why', 'where', 'when'])]\n",
        "print(f\"Question-related terms: {question_terms[:15]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inverted_index"
      },
      "source": [
        "## 6. Inverted Index Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "build_inverted_index",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc95f28-c4d7-4087-ce9b-4226c03b39e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building inverted index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building index: 100%|██████████| 4800066/4800066 [00:08<00:00, 564523.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inverted index statistics:\n",
            "Number of terms: 12000\n",
            "Average documents per term: 400.01\n",
            "Most frequent terms: ['what', 'is', 'how', 'are', 'what is', 'do', 'what are', 'why', 'can', 'number']\n",
            "\n",
            "Sample inverted index entry for 'what':\n",
            "{'33681': 1.0, '47035': 1.0, '83329': 1.0, '92847': 1.0, '141933': 1.0}\n"
          ]
        }
      ],
      "source": [
        "def build_inverted_index(tfidf_matrix, feature_names, doc_ids):\n",
        "    \"\"\"\n",
        "    Build inverted index from TF-IDF matrix.\n",
        "\n",
        "    Args:\n",
        "        tfidf_matrix: Sparse TF-IDF matrix\n",
        "        feature_names: List of feature names\n",
        "        doc_ids: List of document IDs\n",
        "\n",
        "    Returns:\n",
        "        dict: Inverted index mapping terms to documents and scores\n",
        "    \"\"\"\n",
        "    print(\"Building inverted index...\")\n",
        "\n",
        "    inverted_index = defaultdict(dict)\n",
        "\n",
        "    # Convert to COO format for efficient iteration\n",
        "    coo_matrix = tfidf_matrix.tocoo()\n",
        "\n",
        "    # Build inverted index\n",
        "    for doc_idx, term_idx, score in tqdm(zip(coo_matrix.row, coo_matrix.col, coo_matrix.data),\n",
        "                                          total=coo_matrix.nnz, desc=\"Building index\"):\n",
        "        if score > 0:  # Only include non-zero scores\n",
        "            term = feature_names[term_idx]\n",
        "            doc_id = doc_ids[doc_idx]\n",
        "            inverted_index[term][doc_id] = float(score)\n",
        "\n",
        "    # Sort documents by score for each term\n",
        "    for term in inverted_index:\n",
        "        inverted_index[term] = dict(sorted(inverted_index[term].items(),\n",
        "                                          key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    return dict(inverted_index)\n",
        "\n",
        "# Build inverted index\n",
        "doc_ids = docs_df['doc_id'].tolist()\n",
        "inverted_index = build_inverted_index(tfidf_matrix, feature_names, doc_ids)\n",
        "\n",
        "print(f\"\\nInverted index statistics:\")\n",
        "print(f\"Number of terms: {len(inverted_index)}\")\n",
        "print(f\"Average documents per term: {np.mean([len(docs) for docs in inverted_index.values()]):.2f}\")\n",
        "\n",
        "# Show most frequent terms\n",
        "most_frequent_terms = sorted(inverted_index.keys(), key=lambda x: len(inverted_index[x]), reverse=True)[:10]\n",
        "print(f\"Most frequent terms: {most_frequent_terms}\")\n",
        "\n",
        "# Display sample inverted index entries\n",
        "sample_term = list(inverted_index.keys())[0]\n",
        "print(f\"\\nSample inverted index entry for '{sample_term}':\")\n",
        "sample_docs = dict(list(inverted_index[sample_term].items())[:5])\n",
        "print(sample_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_models"
      },
      "source": [
        "## 7. Save Models and Data using Joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "save_models_joblib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a93612c-73fe-4335-8c68-9d53602f5738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models and data...\n",
            "✓ Saved TF-IDF vectorizer to /content/drive/MyDrive/quora_tfidf_models/tfidf_vectorizer.joblib\n",
            "✓ Saved TF-IDF matrix to /content/drive/MyDrive/quora_tfidf_models/tfidf_matrix.joblib\n",
            "✓ Saved inverted index to /content/drive/MyDrive/quora_tfidf_models/inverted_index.joblib\n",
            "✓ Saved document mappings to /content/drive/MyDrive/quora_tfidf_models/document_mappings.joblib\n",
            "✓ Saved text cleaner to /content/drive/MyDrive/quora_tfidf_models/text_cleaner.joblib\n",
            "\n",
            "All models saved successfully to /content/drive/MyDrive/quora_tfidf_models/\n",
            "\n",
            "Saved files:\n",
            "- tfidf_vectorizer.joblib: 0.51 MB\n",
            "- tfidf_matrix.joblib: 56.93 MB\n",
            "- inverted_index.joblib: 66.17 MB\n",
            "- document_mappings.joblib: 77.85 MB\n",
            "- text_cleaner.joblib: 0.00 MB\n",
            "- evaluation_results.joblib: 0.09 MB\n"
          ]
        }
      ],
      "source": [
        "# Create output directory\n",
        "output_dir = '/content/drive/MyDrive/quora_tfidf_models/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Saving models and data...\")\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "vectorizer_path = os.path.join(output_dir, 'tfidf_vectorizer.joblib')\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
        "print(f\"✓ Saved TF-IDF vectorizer to {vectorizer_path}\")\n",
        "\n",
        "# Save TF-IDF matrix\n",
        "matrix_path = os.path.join(output_dir, 'tfidf_matrix.joblib')\n",
        "joblib.dump(tfidf_matrix, matrix_path)\n",
        "print(f\"✓ Saved TF-IDF matrix to {matrix_path}\")\n",
        "\n",
        "# Save inverted index\n",
        "index_path = os.path.join(output_dir, 'inverted_index.joblib')\n",
        "joblib.dump(inverted_index, index_path)\n",
        "print(f\"✓ Saved inverted index to {index_path}\")\n",
        "\n",
        "# Save document mappings\n",
        "doc_mapping = {\n",
        "    'doc_ids': doc_ids,\n",
        "    'docs_df': docs_df,\n",
        "    'queries_df': queries_df,\n",
        "    'qrels_df': qrels_df\n",
        "}\n",
        "mapping_path = os.path.join(output_dir, 'document_mappings.joblib')\n",
        "joblib.dump(doc_mapping, mapping_path)\n",
        "print(f\"✓ Saved document mappings to {mapping_path}\")\n",
        "\n",
        "# Save text cleaner\n",
        "cleaner_path = os.path.join(output_dir, 'text_cleaner.joblib')\n",
        "joblib.dump(text_cleaner, cleaner_path)\n",
        "print(f\"✓ Saved text cleaner to {cleaner_path}\")\n",
        "\n",
        "print(f\"\\nAll models saved successfully to {output_dir}\")\n",
        "\n",
        "# Display saved files\n",
        "saved_files = os.listdir(output_dir)\n",
        "print(f\"\\nSaved files:\")\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
        "    print(f\"- {file}: {file_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "search_function"
      },
      "source": [
        "## 8. Search Function Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "implement_search",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c8307b-0d34-4559-9b4a-34e50a1dec6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing search with query: 'How to learn programming efficiently?'\n",
            "\n",
            "Top 5 results (TF-IDF):\n",
            "1. Doc 501586: 0.9100\n",
            "2. Doc 527808: 0.8332\n",
            "3. Doc 133783: 0.8332\n",
            "4. Doc 37522: 0.8087\n",
            "5. Doc 22651: 0.7739\n",
            "\n",
            "Top 5 results (Inverted Index):\n",
            "1. Doc 133783: 2.1032\n",
            "2. Doc 527808: 2.1032\n",
            "3. Doc 501586: 2.0978\n",
            "4. Doc 37522: 2.0414\n",
            "5. Doc 22651: 1.9537\n",
            "\n",
            "Search functions implemented successfully!\n"
          ]
        }
      ],
      "source": [
        "def search_documents(query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=1000):\n",
        "    \"\"\"\n",
        "    Search documents using TF-IDF cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        tfidf_matrix: Document TF-IDF matrix\n",
        "        doc_ids (list): List of document IDs\n",
        "        top_k (int): Number of top results to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of (doc_id, score) tuples ranked by relevance\n",
        "    \"\"\"\n",
        "    # Transform query using the fitted vectorizer\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Get top-k results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    # Create results list\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0:  # Only include documents with non-zero similarity\n",
        "            results.append((doc_ids[idx], similarities[idx]))\n",
        "\n",
        "    return results\n",
        "\n",
        "def search_with_inverted_index(query, inverted_index, tfidf_vectorizer, doc_ids, top_k=1000):\n",
        "    \"\"\"\n",
        "    Search documents using inverted index for faster retrieval.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        inverted_index (dict): Inverted index\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        doc_ids (list): List of document IDs\n",
        "        top_k (int): Number of top results to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of (doc_id, score) tuples ranked by relevance\n",
        "    \"\"\"\n",
        "    # Get query terms using the same tokenizer\n",
        "    query_terms = tfidf_vectorizer.build_analyzer()(query)\n",
        "\n",
        "    # Collect candidate documents\n",
        "    candidate_docs = defaultdict(float)\n",
        "\n",
        "    for term in query_terms:\n",
        "        if term in inverted_index:\n",
        "            for doc_id, score in inverted_index[term].items():\n",
        "                candidate_docs[doc_id] += score\n",
        "\n",
        "    # Sort by score and return top-k\n",
        "    sorted_docs = sorted(candidate_docs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_docs[:top_k]\n",
        "\n",
        "# Test search function\n",
        "test_query = \"How to learn programming efficiently?\"\n",
        "print(f\"Testing search with query: '{test_query}'\")\n",
        "\n",
        "# Search using TF-IDF matrix\n",
        "results_tfidf = search_documents(test_query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=5)\n",
        "print(f\"\\nTop 5 results (TF-IDF):\")\n",
        "for i, (doc_id, score) in enumerate(results_tfidf, 1):\n",
        "    print(f\"{i}. Doc {doc_id}: {score:.4f}\")\n",
        "\n",
        "# Search using inverted index\n",
        "results_index = search_with_inverted_index(test_query, inverted_index, tfidf_vectorizer, doc_ids, top_k=5)\n",
        "print(f\"\\nTop 5 results (Inverted Index):\")\n",
        "for i, (doc_id, score) in enumerate(results_index, 1):\n",
        "    print(f\"{i}. Doc {doc_id}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nSearch functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 9. Evaluation Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "evaluation_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc453db-980a-4847-d308-c8f915ab70ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation functions implemented successfully!\n"
          ]
        }
      ],
      "source": [
        "def calculate_average_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision for a single query.\n",
        "\n",
        "    Args:\n",
        "        retrieved_docs (list): List of retrieved document IDs in rank order\n",
        "        relevant_docs (set): Set of relevant document IDs\n",
        "\n",
        "    Returns:\n",
        "        float: Average Precision score\n",
        "    \"\"\"\n",
        "    if not relevant_docs:\n",
        "        return 0.0\n",
        "\n",
        "    precision_at_k = []\n",
        "    relevant_retrieved = 0\n",
        "\n",
        "    for k, doc_id in enumerate(retrieved_docs, 1):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "            precision_at_k.append(relevant_retrieved / k)\n",
        "\n",
        "    if not precision_at_k:\n",
        "        return 0.0\n",
        "\n",
        "    return sum(precision_at_k) / len(relevant_docs)\n",
        "\n",
        "def calculate_map(queries_df, qrels_df, search_function, **search_kwargs):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision (MAP) for all queries.\n",
        "\n",
        "    Args:\n",
        "        queries_df (pd.DataFrame): DataFrame with queries\n",
        "        qrels_df (pd.DataFrame): DataFrame with relevance judgments\n",
        "        search_function (callable): Search function to use\n",
        "        **search_kwargs: Additional arguments for search function\n",
        "\n",
        "    Returns:\n",
        "        tuple: (MAP score, list of individual AP scores)\n",
        "    \"\"\"\n",
        "    # Group relevance judgments by query\n",
        "    qrels_grouped = qrels_df.groupby('query_id')['doc_id'].apply(set).to_dict()\n",
        "\n",
        "    ap_scores = []\n",
        "\n",
        "    print(\"Calculating MAP...\")\n",
        "\n",
        "    for _, query_row in tqdm(queries_df.iterrows(), total=len(queries_df), desc=\"Evaluating queries\"):\n",
        "        query_id = query_row['query_id']\n",
        "        query_text = query_row['cleaned_query']\n",
        "\n",
        "        # Get relevant documents for this query\n",
        "        relevant_docs = qrels_grouped.get(query_id, set())\n",
        "\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "\n",
        "        # Search for documents\n",
        "        results = search_function(query_text, **search_kwargs)\n",
        "\n",
        "        # Extract document IDs from results\n",
        "        retrieved_docs = [doc_id for doc_id, _ in results]\n",
        "\n",
        "        # Calculate Average Precision\n",
        "        ap = calculate_average_precision(retrieved_docs, relevant_docs)\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "    # Calculate MAP\n",
        "    map_score = np.mean(ap_scores) if ap_scores else 0.0\n",
        "\n",
        "    return map_score, ap_scores\n",
        "\n",
        "def evaluate_system(queries_df, qrels_df, tfidf_vectorizer, tfidf_matrix, inverted_index, doc_ids):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the TF-IDF system.\n",
        "\n",
        "    Args:\n",
        "        queries_df (pd.DataFrame): DataFrame with queries\n",
        "        qrels_df (pd.DataFrame): DataFrame with relevance judgments\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        tfidf_matrix: Document TF-IDF matrix\n",
        "        inverted_index (dict): Inverted index\n",
        "        doc_ids (list): List of document IDs\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Evaluate using TF-IDF matrix search\n",
        "    print(\"Evaluating TF-IDF matrix search...\")\n",
        "    map_tfidf, ap_scores_tfidf = calculate_map(\n",
        "        queries_df, qrels_df, search_documents,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "\n",
        "    results['tfidf_matrix'] = {\n",
        "        'MAP': map_tfidf,\n",
        "        'AP_scores': ap_scores_tfidf,\n",
        "        'num_queries': len(ap_scores_tfidf)\n",
        "    }\n",
        "\n",
        "    # Evaluate using inverted index search\n",
        "    print(\"Evaluating inverted index search...\")\n",
        "    map_index, ap_scores_index = calculate_map(\n",
        "        queries_df, qrels_df, search_with_inverted_index,\n",
        "        inverted_index=inverted_index,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "\n",
        "    results['inverted_index'] = {\n",
        "        'MAP': map_index,\n",
        "        'AP_scores': ap_scores_index,\n",
        "        'num_queries': len(ap_scores_index)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Evaluation functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_evaluation"
      },
      "source": [
        "## 10. Run Comprehensive Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run_evaluation_code",
        "outputId": "b66104c5-8bd5-4a72-c5bd-065a13b3b7b7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting comprehensive evaluation...\n",
            "==================================================\n",
            "Evaluating TF-IDF matrix search...\n",
            "Calculating MAP...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating queries: 100%|██████████| 5000/5000 [15:08<00:00,  5.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating inverted index search...\n",
            "Calculating MAP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating queries: 100%|██████████| 5000/5000 [29:10<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS\n",
            "==================================================\n",
            "\n",
            "TFIDF_MATRIX SEARCH:\n",
            "MAP Score: 0.4849\n",
            "Number of queries evaluated: 5000\n",
            "Average Precision scores - Min: 0.0000, Max: 1.0000\n",
            "Standard deviation: 0.4293\n",
            "✅ MAP > 0.3 TARGET ACHIEVED! (0.4849)\n",
            "\n",
            "INVERTED_INDEX SEARCH:\n",
            "MAP Score: 0.3946\n",
            "Number of queries evaluated: 5000\n",
            "Average Precision scores - Min: 0.0000, Max: 1.0000\n",
            "Standard deviation: 0.4282\n",
            "✅ MAP > 0.3 TARGET ACHIEVED! (0.3946)\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Detailed Performance Analysis:\n",
            "TF-IDF Matrix Search:\n",
            "  - Queries with AP > 0.5: 2118\n",
            "  - Queries with AP > 0.3: 2689\n",
            "  - Queries with AP > 0.1: 3289\n",
            "  - Queries with AP = 0: 177\n",
            "\n",
            "Inverted Index Search:\n",
            "  - Queries with AP > 0.5: 1707\n",
            "  - Queries with AP > 0.3: 2194\n",
            "  - Queries with AP > 0.1: 2697\n",
            "  - Queries with AP = 0: 547\n",
            "\n",
            "✓ Evaluation results saved to /content/drive/MyDrive/quora_tfidf_models/evaluation_results.joblib\n",
            "\n",
            "==================================================\n",
            "EVALUATION COMPLETE!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive evaluation\n",
        "print(\"Starting comprehensive evaluation...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "evaluation_results = evaluate_system(\n",
        "    queries_df, qrels_df, tfidf_vectorizer, tfidf_matrix, inverted_index, doc_ids\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for method, results in evaluation_results.items():\n",
        "    print(f\"\\n{method.upper()} SEARCH:\")\n",
        "    print(f\"MAP Score: {results['MAP']:.4f}\")\n",
        "    print(f\"Number of queries evaluated: {results['num_queries']}\")\n",
        "    print(f\"Average Precision scores - Min: {min(results['AP_scores']):.4f}, Max: {max(results['AP_scores']):.4f}\")\n",
        "    print(f\"Standard deviation: {np.std(results['AP_scores']):.4f}\")\n",
        "\n",
        "    # Check if MAP is above 0.3\n",
        "    if results['MAP'] > 0.3:\n",
        "        print(f\"✅ MAP > 0.3 TARGET ACHIEVED! ({results['MAP']:.4f})\")\n",
        "    else:\n",
        "        print(f\"❌ MAP < 0.3 target not met ({results['MAP']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Performance breakdown\n",
        "tfidf_ap_scores = evaluation_results['tfidf_matrix']['AP_scores']\n",
        "index_ap_scores = evaluation_results['inverted_index']['AP_scores']\n",
        "\n",
        "print(f\"\\nDetailed Performance Analysis:\")\n",
        "print(f\"TF-IDF Matrix Search:\")\n",
        "print(f\"  - Queries with AP > 0.5: {sum(1 for ap in tfidf_ap_scores if ap > 0.5)}\")\n",
        "print(f\"  - Queries with AP > 0.3: {sum(1 for ap in tfidf_ap_scores if ap > 0.3)}\")\n",
        "print(f\"  - Queries with AP > 0.1: {sum(1 for ap in tfidf_ap_scores if ap > 0.1)}\")\n",
        "print(f\"  - Queries with AP = 0: {sum(1 for ap in tfidf_ap_scores if ap == 0)}\")\n",
        "\n",
        "print(f\"\\nInverted Index Search:\")\n",
        "print(f\"  - Queries with AP > 0.5: {sum(1 for ap in index_ap_scores if ap > 0.5)}\")\n",
        "print(f\"  - Queries with AP > 0.3: {sum(1 for ap in index_ap_scores if ap > 0.3)}\")\n",
        "print(f\"  - Queries with AP > 0.1: {sum(1 for ap in index_ap_scores if ap > 0.1)}\")\n",
        "print(f\"  - Queries with AP = 0: {sum(1 for ap in index_ap_scores if ap == 0)}\")\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results_path = os.path.join(output_dir, 'evaluation_results.joblib')\n",
        "joblib.dump(evaluation_results, eval_results_path)\n",
        "print(f\"\\n✓ Evaluation results saved to {eval_results_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optimization"
      },
      "source": [
        "## 11. Optimization for Better MAP Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "optimization_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4da36d-ca07-4bcc-a2b8-345c6650e0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERFORMANCE OPTIMIZATION\n",
            "========================================\n",
            "\n",
            "Current MAP: 0.4849\n",
            "Target MAP: 0.3000\n",
            "\n",
            "🎉 EXCELLENT! MAP > 0.3 TARGET ACHIEVED!\n",
            "The system is performing well with current configuration.\n",
            "\n",
            "Final MAP Score: 0.4849\n",
            "\n",
            "========================================\n",
            "OPTIMIZATION COMPLETE!\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# If MAP is below 0.3, try optimization strategies\n",
        "current_map = evaluation_results['tfidf_matrix']['MAP']\n",
        "\n",
        "print(\"PERFORMANCE OPTIMIZATION\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"\\nCurrent MAP: {current_map:.4f}\")\n",
        "print(f\"Target MAP: 0.3000\")\n",
        "\n",
        "if current_map < 0.3:\n",
        "    print(\"\\n🔧 IMPLEMENTING OPTIMIZATIONS...\")\n",
        "\n",
        "    # Strategy 1: Adjusted TF-IDF parameters\n",
        "    print(\"\\n1. Testing optimized TF-IDF parameters...\")\n",
        "\n",
        "    optimized_vectorizer = TfidfVectorizer(\n",
        "        preprocessor=None,\n",
        "        tokenizer=text_cleaner.custom_tokenizer,\n",
        "        token_pattern=None,\n",
        "        lowercase=False,\n",
        "        stop_words=None,\n",
        "        max_features=15000,  # Increased vocabulary\n",
        "        min_df=1,  # Keep all terms\n",
        "        max_df=0.8,  # More restrictive on common terms\n",
        "        ngram_range=(1, 3),  # Include trigrams\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        sublinear_tf=True,\n",
        "        norm='l2'\n",
        "    )\n",
        "\n",
        "    # Fit optimized vectorizer\n",
        "    optimized_tfidf_matrix = optimized_vectorizer.fit_transform(document_texts)\n",
        "    print(f\"Optimized TF-IDF matrix shape: {optimized_tfidf_matrix.shape}\")\n",
        "\n",
        "    # Evaluate optimized system\n",
        "    print(\"Evaluating optimized system...\")\n",
        "    optimized_map, optimized_ap_scores = calculate_map(\n",
        "        queries_df, qrels_df, search_documents,\n",
        "        tfidf_vectorizer=optimized_vectorizer,\n",
        "        tfidf_matrix=optimized_tfidf_matrix,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "\n",
        "    print(f\"\\nOptimization Results:\")\n",
        "    print(f\"Original MAP: {current_map:.4f}\")\n",
        "    print(f\"Optimized MAP: {optimized_map:.4f}\")\n",
        "    print(f\"Improvement: {optimized_map - current_map:.4f}\")\n",
        "\n",
        "    if optimized_map > 0.3:\n",
        "        print(f\"\\n🎉 SUCCESS! MAP > 0.3 TARGET ACHIEVED!\")\n",
        "\n",
        "        # Save optimized models\n",
        "        optimized_vectorizer_path = os.path.join(output_dir, 'optimized_tfidf_vectorizer.joblib')\n",
        "        optimized_matrix_path = os.path.join(output_dir, 'optimized_tfidf_matrix.joblib')\n",
        "\n",
        "        joblib.dump(optimized_vectorizer, optimized_vectorizer_path)\n",
        "        joblib.dump(optimized_tfidf_matrix, optimized_matrix_path)\n",
        "\n",
        "        print(f\"✓ Saved optimized models to {output_dir}\")\n",
        "\n",
        "        # Update the main models\n",
        "        tfidf_vectorizer = optimized_vectorizer\n",
        "        tfidf_matrix = optimized_tfidf_matrix\n",
        "        current_map = optimized_map\n",
        "\n",
        "    elif optimized_map > current_map:\n",
        "        print(f\"\\n⚡ Improvement achieved but still below target.\")\n",
        "        print(f\"\\n📝 Additional strategies to try:\")\n",
        "        print(f\"   - Query expansion using word similarity\")\n",
        "        print(f\"   - Different text preprocessing approaches\")\n",
        "        print(f\"   - BM25 scoring instead of TF-IDF\")\n",
        "        print(f\"   - Learning-to-rank methods\")\n",
        "\n",
        "        # Update with improved model\n",
        "        tfidf_vectorizer = optimized_vectorizer\n",
        "        tfidf_matrix = optimized_tfidf_matrix\n",
        "        current_map = optimized_map\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n⚠️ No improvement with parameter optimization.\")\n",
        "        print(f\"Consider more advanced techniques.\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n🎉 EXCELLENT! MAP > 0.3 TARGET ACHIEVED!\")\n",
        "    print(f\"The system is performing well with current configuration.\")\n",
        "\n",
        "print(f\"\\nFinal MAP Score: {current_map:.4f}\")\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(\"OPTIMIZATION COMPLETE!\")\n",
        "print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_queries"
      },
      "source": [
        "## 12. Sample Query Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "test_sample_queries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef2a90d-7db3-40f6-c20b-df7e2908ec6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLE QUERY TESTING\n",
            "========================================\n",
            "\n",
            "Query ID: 318\n",
            "Original: How does Quora look to a moderator?\n",
            "Cleaned: how does quora look to a moderator\n",
            "Relevant documents: 1\n",
            "Top 5 search results:\n",
            "  1. Doc 246749 (✗): 0.8078\n",
            "     \"How does Quora Moderation work?\"\n",
            "  2. Doc 256802 (✗): 0.8078\n",
            "     \"How does Quora work?\"\n",
            "  3. Doc 194711 (✗): 0.8078\n",
            "     \"How does Quora work?\"\n",
            "  4. Doc 277591 (✗): 0.7849\n",
            "     \"How does Quora change you?\"\n",
            "  5. Doc 474641 (✗): 0.7375\n",
            "     \"How does Quora recruit?\"\n",
            "----------------------------------------\n",
            "\n",
            "Query ID: 378\n",
            "Original: How do I refuse to chose between different things to do in my life?\n",
            "Cleaned: how to refuse to chose between different things to do in my life\n",
            "Relevant documents: 1\n",
            "Top 5 search results:\n",
            "  1. Doc 334413 (✗): 0.6058\n",
            "     \"What are some things you have to do in life?\"\n",
            "  2. Doc 122773 (✗): 0.5620\n",
            "     \"What's the best thing to do in life?\"\n",
            "  3. Doc 28050 (✗): 0.5620\n",
            "     \"What is the best thing to do with your life?\"\n",
            "  4. Doc 254814 (✗): 0.5546\n",
            "     \"What are the best things to do in life?\"\n",
            "  5. Doc 307956 (✗): 0.5274\n",
            "     \"What are the hardest and the easiest things to do in life?\"\n",
            "----------------------------------------\n",
            "\n",
            "Query ID: 379\n",
            "Original: Did Ben Affleck shine more than Christian Bale as Batman?\n",
            "Cleaned: did ben affleck shine more than christian bale as batman\n",
            "Relevant documents: 5\n",
            "Top 5 search results:\n",
            "  1. Doc 143064 (✗): 0.7879\n",
            "     \"Did you like Ben Affleck as Batman?\"\n",
            "  2. Doc 380 (✓): 0.6377\n",
            "     \"No fanboys please, but who was the true batman, Christian Bale or Ben Affleck?\"\n",
            "  3. Doc 525443 (✗): 0.6286\n",
            "     \"Which actor looks better in the Batman costume: Christian Bale or Ben Affleck? Why?\"\n",
            "  4. Doc 143063 (✗): 0.6238\n",
            "     \"Do you think Ben Affleck should be Batman?\"\n",
            "  5. Doc 45646 (✓): 0.6002\n",
            "     \"Who do you think portrayed Batman better: Christian Bale or Ben Affleck?\"\n",
            "----------------------------------------\n",
            "\n",
            "Query ID: 399\n",
            "Original: What are the effects of demonitization of 500 and 1000 rupees notes on real estate sector?\n",
            "Cleaned: what are effects of demonitization of NUMBER and NUMBER rupees notes on real estate sector\n",
            "Relevant documents: 28\n",
            "Top 5 search results:\n",
            "  1. Doc 50270 (✓): 0.6850\n",
            "     \"How does Demonetisation of 1000 and 500 rupees notes affect real estate industry?\"\n",
            "  2. Doc 364917 (✓): 0.6633\n",
            "     \"How does this \"demonetisation of 500 & 1000 rupee notes\" help to reduce the price of real estate?\"\n",
            "  3. Doc 400 (✓): 0.6295\n",
            "     \"What will be the impact of scrapping of ₹500 and ₹1000 rupee notes on the real estate market?\"\n",
            "  4. Doc 2606 (✓): 0.6156\n",
            "     \"How will real estate prices be affected in India after banning of 500 and 1000 rupees notes?\"\n",
            "  5. Doc 141304 (✗): 0.6045\n",
            "     \"Why did GOI demobilise 500 and 1000 rupee notes?\"\n",
            "----------------------------------------\n",
            "\n",
            "Query ID: 420\n",
            "Original: Why creativity is important?\n",
            "Cleaned: why creativity is important\n",
            "Relevant documents: 1\n",
            "Top 5 search results:\n",
            "  1. Doc 419 (✓): 0.7686\n",
            "     \"Why is creativity important?\"\n",
            "  2. Doc 180893 (✗): 0.6854\n",
            "     \"Why it is important to normalize a wavefunction?\"\n",
            "  3. Doc 74401 (✗): 0.6854\n",
            "     \"Why is it important to be unmateralistic?\"\n",
            "  4. Doc 375278 (✗): 0.6801\n",
            "     \"What is important offence of IPC?\"\n",
            "  5. Doc 177068 (✗): 0.6801\n",
            "     \"What is important for grils?\"\n",
            "----------------------------------------\n",
            "\n",
            "Sample query testing complete!\n"
          ]
        }
      ],
      "source": [
        "# Test with sample queries to demonstrate the system\n",
        "print(\"SAMPLE QUERY TESTING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Select some sample queries\n",
        "sample_queries = queries_df.head(5)\n",
        "\n",
        "for _, query_row in sample_queries.iterrows():\n",
        "    query_id = query_row['query_id']\n",
        "    original_query = query_row[query_text_col]\n",
        "    cleaned_query = query_row['cleaned_query']\n",
        "\n",
        "    print(f\"\\nQuery ID: {query_id}\")\n",
        "    print(f\"Original: {original_query}\")\n",
        "    print(f\"Cleaned: {cleaned_query}\")\n",
        "\n",
        "    # Get search results\n",
        "    results = search_documents(cleaned_query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=5)\n",
        "\n",
        "    # Get relevant documents from qrels\n",
        "    relevant_docs = set(qrels_df[qrels_df['query_id'] == query_id]['doc_id'].astype(str))\n",
        "\n",
        "    print(f\"Relevant documents: {len(relevant_docs)}\")\n",
        "    print(f\"Top 5 search results:\")\n",
        "\n",
        "    for i, (doc_id, score) in enumerate(results[:5], 1):\n",
        "        relevance = \"✓\" if doc_id in relevant_docs else \"✗\"\n",
        "        print(f\"  {i}. Doc {doc_id} ({relevance}): {score:.4f}\")\n",
        "\n",
        "        # Show snippet of the document\n",
        "        if doc_id in docs_df['doc_id'].values:\n",
        "            doc_text = docs_df[docs_df['doc_id'] == doc_id][doc_text_col].iloc[0]\n",
        "            snippet = doc_text[:200] + \"...\" if len(doc_text) > 200 else doc_text\n",
        "            print(f\"     \\\"{snippet}\\\"\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nSample query testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 13. Final Summary and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "final_summary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f365cfd-bf22-44e0-9023-5123a5f278eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL SUMMARY - QUORA TF-IDF IMPLEMENTATION\n",
            "============================================================\n",
            "\n",
            "📊 DATASET STATISTICS:\n",
            "Documents processed: 522751\n",
            "Queries processed: 5000\n",
            "Relevance judgments: 7626\n",
            "\n",
            "🔧 MODEL CONFIGURATION:\n",
            "TF-IDF Features: 12000\n",
            "Matrix sparsity: 99.92%\n",
            "Inverted index terms: 12000\n",
            "N-gram range: (1, 2)\n",
            "\n",
            "📈 PERFORMANCE RESULTS:\n",
            "Final MAP Score: 0.4849\n",
            "\n",
            "💾 SAVED MODELS:\n",
            "- tfidf_vectorizer.joblib\n",
            "- tfidf_matrix.joblib\n",
            "- inverted_index.joblib\n",
            "- document_mappings.joblib\n",
            "- text_cleaner.joblib\n",
            "- evaluation_results.joblib\n",
            "\n",
            "🎯 TARGET ACHIEVEMENT:\n",
            "✅ SUCCESS! MAP score: 0.4849 ≥ 0.3\n",
            "🎉 Quora TF-IDF system meets performance requirements!\n",
            "\n",
            "🚀 SYSTEM FEATURES:\n",
            "✓ Advanced Quora-specific text cleaning\n",
            "✓ Custom tokenization with semantic preservation\n",
            "✓ Optimized TF-IDF vectorization\n",
            "✓ Efficient inverted index\n",
            "✓ Comprehensive MAP evaluation\n",
            "✓ Complete model persistence\n",
            "\n",
            "📂 SYSTEM READY FOR USE!\n",
            "All models saved to: /content/drive/MyDrive/quora_tfidf_models/\n",
            "\n",
            "To use the system:\n",
            "1. Load models using joblib.load()\n",
            "2. Use search_documents() for new queries\n",
            "3. Inverted index provides faster term-based search\n",
            "\n",
            "============================================================\n",
            "QUORA TF-IDF IMPLEMENTATION COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"FINAL SUMMARY - QUORA TF-IDF IMPLEMENTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n📊 DATASET STATISTICS:\")\n",
        "print(f\"Documents processed: {len(docs_df)}\")\n",
        "print(f\"Queries processed: {len(queries_df)}\")\n",
        "print(f\"Relevance judgments: {len(qrels_df)}\")\n",
        "\n",
        "print(f\"\\n🔧 MODEL CONFIGURATION:\")\n",
        "print(f\"TF-IDF Features: {tfidf_matrix.shape[1]}\")\n",
        "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "print(f\"Inverted index terms: {len(inverted_index)}\")\n",
        "print(f\"N-gram range: {tfidf_vectorizer.ngram_range}\")\n",
        "\n",
        "print(f\"\\n📈 PERFORMANCE RESULTS:\")\n",
        "print(f\"Final MAP Score: {current_map:.4f}\")\n",
        "\n",
        "print(f\"\\n💾 SAVED MODELS:\")\n",
        "saved_files = os.listdir(output_dir)\n",
        "for file in saved_files:\n",
        "    print(f\"- {file}\")\n",
        "\n",
        "print(f\"\\n🎯 TARGET ACHIEVEMENT:\")\n",
        "if current_map >= 0.3:\n",
        "    print(f\"✅ SUCCESS! MAP score: {current_map:.4f} ≥ 0.3\")\n",
        "    print(f\"🎉 Quora TF-IDF system meets performance requirements!\")\n",
        "else:\n",
        "    print(f\"❌ Target not fully met. MAP score: {current_map:.4f} < 0.3\")\n",
        "    print(f\"⚡ Consider implementing advanced optimization techniques.\")\n",
        "\n",
        "print(f\"\\n🚀 SYSTEM FEATURES:\")\n",
        "print(f\"✓ Advanced Quora-specific text cleaning\")\n",
        "print(f\"✓ Custom tokenization with semantic preservation\")\n",
        "print(f\"✓ Optimized TF-IDF vectorization\")\n",
        "print(f\"✓ Efficient inverted index\")\n",
        "print(f\"✓ Comprehensive MAP evaluation\")\n",
        "print(f\"✓ Complete model persistence\")\n",
        "\n",
        "print(f\"\\n📂 SYSTEM READY FOR USE!\")\n",
        "print(f\"All models saved to: {output_dir}\")\n",
        "print(f\"\\nTo use the system:\")\n",
        "print(f\"1. Load models using joblib.load()\")\n",
        "print(f\"2. Use search_documents() for new queries\")\n",
        "print(f\"3. Inverted index provides faster term-based search\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"QUORA TF-IDF IMPLEMENTATION COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_example"
      },
      "source": [
        "## 14. Usage Example for Future Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "usage_example_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab770da-c9e5-41b2-ba9b-d703fb2c2415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USAGE EXAMPLE FOR FUTURE USE\n",
            "========================================\n",
            "Copy and save this code for future use:\n",
            "\n",
            "# How to load and use the saved Quora TF-IDF models\n",
            "import joblib\n",
            "import numpy as np\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "\n",
            "# Load saved models\n",
            "output_dir = '/content/drive/MyDrive/quora_tfidf_models/'\n",
            "\n",
            "tfidf_vectorizer = joblib.load(output_dir + 'tfidf_vectorizer.joblib')\n",
            "tfidf_matrix = joblib.load(output_dir + 'tfidf_matrix.joblib')\n",
            "inverted_index = joblib.load(output_dir + 'inverted_index.joblib')\n",
            "doc_mappings = joblib.load(output_dir + 'document_mappings.joblib')\n",
            "text_cleaner = joblib.load(output_dir + 'text_cleaner.joblib')\n",
            "\n",
            "# Extract document IDs\n",
            "doc_ids = doc_mappings['doc_ids']\n",
            "\n",
            "# Search function for new queries\n",
            "def search_quora_questions(query, top_k=10):\n",
            "    \"\"\"Search for similar Quora questions\"\"\"\n",
            "    # Transform query using the fitted vectorizer\n",
            "    query_vector = tfidf_vectorizer.transform([query])\n",
            "\n",
            "    # Calculate similarities\n",
            "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
            "\n",
            "    # Get top results\n",
            "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
            "\n",
            "    results = []\n",
            "    for idx in top_indices:\n",
            "        if similarities[idx] > 0:\n",
            "            results.append((doc_ids[idx], similarities[idx]))\n",
            "\n",
            "    return results\n",
            "\n",
            "# Example usage\n",
            "query = \"How to learn machine learning effectively?\"\n",
            "results = search_quora_questions(query)\n",
            "print(f\"Top results for '{query}':\")\n",
            "for doc_id, score in results[:5]:\n",
            "    print(f\"Doc {doc_id}: {score:.4f}\")\n",
            "\n",
            "\n",
            "✓ Usage example saved to: /content/drive/MyDrive/quora_tfidf_models/quora_usage_example.py\n",
            "\n",
            "This completes the Quora TF-IDF implementation!\n"
          ]
        }
      ],
      "source": [
        "# Example code for loading and using the saved models\n",
        "print(\"USAGE EXAMPLE FOR FUTURE USE\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "example_code = '''\n",
        "# How to load and use the saved Quora TF-IDF models\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load saved models\n",
        "output_dir = '/content/drive/MyDrive/quora_tfidf_models/'\n",
        "\n",
        "tfidf_vectorizer = joblib.load(output_dir + 'tfidf_vectorizer.joblib')\n",
        "tfidf_matrix = joblib.load(output_dir + 'tfidf_matrix.joblib')\n",
        "inverted_index = joblib.load(output_dir + 'inverted_index.joblib')\n",
        "doc_mappings = joblib.load(output_dir + 'document_mappings.joblib')\n",
        "text_cleaner = joblib.load(output_dir + 'text_cleaner.joblib')\n",
        "\n",
        "# Extract document IDs\n",
        "doc_ids = doc_mappings['doc_ids']\n",
        "\n",
        "# Search function for new queries\n",
        "def search_quora_questions(query, top_k=10):\n",
        "    \"\"\"Search for similar Quora questions\"\"\"\n",
        "    # Transform query using the fitted vectorizer\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Get top results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0:\n",
        "            results.append((doc_ids[idx], similarities[idx]))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "query = \"How to learn machine learning effectively?\"\n",
        "results = search_quora_questions(query)\n",
        "print(f\"Top results for '{query}':\")\n",
        "for doc_id, score in results[:5]:\n",
        "    print(f\"Doc {doc_id}: {score:.4f}\")\n",
        "'''\n",
        "\n",
        "print(\"Copy and save this code for future use:\")\n",
        "print(example_code)\n",
        "\n",
        "# Save the example code to a file\n",
        "example_file_path = os.path.join(output_dir, 'quora_usage_example.py')\n",
        "with open(example_file_path, 'w') as f:\n",
        "    f.write(example_code)\n",
        "\n",
        "print(f\"\\n✓ Usage example saved to: {example_file_path}\")\n",
        "print(\"\\nThis completes the Quora TF-IDF implementation!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}