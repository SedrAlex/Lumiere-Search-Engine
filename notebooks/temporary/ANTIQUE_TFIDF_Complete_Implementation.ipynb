{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ANTIQUE Dataset TF-IDF Implementation with Custom Text Cleaning\n",
        "\n",
        "This notebook implements TF-IDF vectorization on the ANTIQUE dataset with:\n",
        "- Custom text cleaning and preprocessing\n",
        "- Custom tokenization\n",
        "- Inverted index construction\n",
        "- Model persistence using joblib\n",
        "- Evaluation using MAP metric (target: > 0.4)\n",
        "\n",
        "## Dataset Structure\n",
        "- Documents: `/content/drive/MyDrive/downloads/documents.tsv`\n",
        "- Queries: `/content/drive/MyDrive/downloads/queries.tsv`\n",
        "- Relevance judgments: `/content/drive/MyDrive/downloads/qrels.tsv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install nltk scikit-learn pandas numpy joblib tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import joblib\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## 2. Data Loading and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Define file paths\n",
        "DATA_PATH = '/content/drive/MyDrive/downloads/'\n",
        "DOCUMENTS_FILE = os.path.join(DATA_PATH, 'documents.tsv')\n",
        "QUERIES_FILE = os.path.join(DATA_PATH, 'queries.tsv')\n",
        "QRELS_FILE = os.path.join(DATA_PATH, 'qrels.tsv')\n",
        "\n",
        "# Verify files exist\n",
        "files_to_check = [DOCUMENTS_FILE, QUERIES_FILE, QRELS_FILE]\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"✓ Found: {file_path}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {file_path}\")\n",
        "\n",
        "# Load datasets\n",
        "print(\"\\nLoading datasets...\")\n",
        "documents_df = pd.read_csv(DOCUMENTS_FILE, sep='\\t')\n",
        "queries_df = pd.read_csv(QUERIES_FILE, sep='\\t')\n",
        "qrels_df = pd.read_csv(QRELS_FILE, sep='\\t')\n",
        "\n",
        "print(f\"Documents: {len(documents_df)} rows\")\n",
        "print(f\"Queries: {len(queries_df)} rows\")\n",
        "print(f\"Qrels: {len(qrels_df)} rows\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nDocument columns:\", documents_df.columns.tolist())\n",
        "print(\"Query columns:\", queries_df.columns.tolist())\n",
        "print(\"Qrels columns:\", qrels_df.columns.tolist())\n",
        "\n",
        "print(\"\\nSample document:\")\n",
        "print(documents_df.head(1))\n",
        "\n",
        "print(\"\\nSample query:\")\n",
        "print(queries_df.head(1))\n",
        "\n",
        "print(\"\\nSample qrel:\")\n",
        "print(qrels_df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "## 3. Custom Text Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "text_cleaning_functions"
      },
      "outputs": [],
      "source": [
        "class AntiqueTextCleaner:\n",
        "    \"\"\"\n",
        "    Advanced text cleaning class optimized for ANTIQUE dataset\n",
        "    with semantic preservation and IR-specific optimizations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Setup stopwords with exceptions for important semantic words\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        \n",
        "        # Remove words that might be important for semantic meaning\n",
        "        semantic_words = {\n",
        "            'not', 'no', 'nor', 'neither', 'never', 'none', 'nothing',\n",
        "            'against', 'up', 'down', 'over', 'under', 'above', 'below',\n",
        "            'more', 'most', 'less', 'least', 'very', 'quite', 'rather',\n",
        "            'how', 'what', 'when', 'where', 'why', 'who', 'which',\n",
        "            'before', 'after', 'during', 'between', 'among'\n",
        "        }\n",
        "        self.stop_words = self.stop_words - semantic_words\n",
        "        \n",
        "        # Initialize lemmatizer\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        \n",
        "        # Common abbreviations and their expansions\n",
        "        self.abbreviations = {\n",
        "            \"don't\": \"do not\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"can't\": \"cannot\",\n",
        "            \"n't\": \" not\",\n",
        "            \"'re\": \" are\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'ll\": \" will\",\n",
        "            \"'d\": \" would\",\n",
        "            \"'m\": \" am\"\n",
        "        }\n",
        "    \n",
        "    def smart_clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Enhanced text cleaning with semantic preservation.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text to clean\n",
        "            \n",
        "        Returns:\n",
        "            str: Cleaned text\n",
        "        \"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "            \n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Expand contractions\n",
        "        for abbrev, expansion in self.abbreviations.items():\n",
        "            text = text.replace(abbrev, expansion)\n",
        "        \n",
        "        # Replace URLs with placeholder\n",
        "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL ', text)\n",
        "        \n",
        "        # Replace email addresses\n",
        "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' EMAIL ', text)\n",
        "        \n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<.*?>', ' ', text)\n",
        "        \n",
        "        # Normalize specific patterns for better IR performance\n",
        "        text = re.sub(r'\\b(19|20)\\d{2}\\b', ' YEAR ', text)  # Years\n",
        "        text = re.sub(r'\\b\\d+\\.\\d+\\b', ' DECIMAL ', text)  # Decimals\n",
        "        text = re.sub(r'\\b\\d+\\b', ' NUMBER ', text)  # Numbers\n",
        "        \n",
        "        # Normalize emphasis patterns\n",
        "        text = re.sub(r'[!]{2,}', ' EMPHASIS ', text)\n",
        "        text = re.sub(r'[?]{2,}', ' QUESTION ', text)\n",
        "        text = re.sub(r'[.]{3,}', ' ELLIPSIS ', text)\n",
        "        \n",
        "        # Remove special characters but keep some punctuation\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\-\\']', ' ', text)\n",
        "        \n",
        "        # Handle hyphenated words\n",
        "        text = re.sub(r'\\b(\\w+)-(\\w+)\\b', r'\\1 \\2', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def custom_tokenizer(self, text):\n",
        "        \"\"\"\n",
        "        Custom tokenizer that cleans, tokenizes, removes stopwords, and lemmatizes.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            \n",
        "        Returns:\n",
        "            list: List of processed tokens\n",
        "        \"\"\"\n",
        "        # Clean the text first\n",
        "        cleaned_text = self.smart_clean_text(text)\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "        \n",
        "        # Filter and lemmatize\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            # Skip if token is too short or is a stopword\n",
        "            if len(token) < 2 or token in self.stop_words:\n",
        "                continue\n",
        "                \n",
        "            # Lemmatize\n",
        "            lemmatized = self.lemmatizer.lemmatize(token)\n",
        "            processed_tokens.append(lemmatized)\n",
        "        \n",
        "        return processed_tokens\n",
        "\n",
        "# Initialize the text cleaner\n",
        "text_cleaner = AntiqueTextCleaner()\n",
        "\n",
        "# Test the cleaning function\n",
        "sample_text = \"I can't believe this isn't working! Visit http://example.com for more info. The year 2023 was amazing.\"\n",
        "cleaned_sample = text_cleaner.smart_clean_text(sample_text)\n",
        "tokens_sample = text_cleaner.custom_tokenizer(sample_text)\n",
        "\n",
        "print(\"Original text:\", sample_text)\n",
        "print(\"Cleaned text:\", cleaned_sample)\n",
        "print(\"Tokens:\", tokens_sample)\n",
        "print(\"\\nText cleaning functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_preprocessing"
      },
      "source": [
        "## 4. Data Preprocessing and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_data"
      },
      "outputs": [],
      "source": [
        "# Preprocess documents\n",
        "print(\"Preprocessing documents...\")\n",
        "documents_df['cleaned_text'] = documents_df['text'].apply(text_cleaner.smart_clean_text)\n",
        "documents_df['doc_id'] = documents_df['doc_id'].astype(str)\n",
        "\n",
        "# Remove empty documents\n",
        "documents_df = documents_df[documents_df['cleaned_text'].str.len() > 0]\n",
        "print(f\"Documents after cleaning: {len(documents_df)}\")\n",
        "\n",
        "# Preprocess queries\n",
        "print(\"Preprocessing queries...\")\n",
        "queries_df['cleaned_query'] = queries_df['query'].apply(text_cleaner.smart_clean_text)\n",
        "queries_df['query_id'] = queries_df['query_id'].astype(str)\n",
        "\n",
        "# Remove empty queries\n",
        "queries_df = queries_df[queries_df['cleaned_query'].str.len() > 0]\n",
        "print(f\"Queries after cleaning: {len(queries_df)}\")\n",
        "\n",
        "# Prepare qrels\n",
        "qrels_df['query_id'] = qrels_df['query_id'].astype(str)\n",
        "qrels_df['doc_id'] = qrels_df['doc_id'].astype(str)\n",
        "\n",
        "print(\"\\nData preprocessing complete!\")\n",
        "print(f\"Final dataset sizes:\")\n",
        "print(f\"- Documents: {len(documents_df)}\")\n",
        "print(f\"- Queries: {len(queries_df)}\")\n",
        "print(f\"- Qrels: {len(qrels_df)}\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"\\nSample cleaned document:\")\n",
        "print(f\"Original: {documents_df.iloc[0]['text'][:200]}...\")\n",
        "print(f\"Cleaned: {documents_df.iloc[0]['cleaned_text'][:200]}...\")\n",
        "\n",
        "print(\"\\nSample cleaned query:\")\n",
        "print(f\"Original: {queries_df.iloc[0]['query']}\")\n",
        "print(f\"Cleaned: {queries_df.iloc[0]['cleaned_query']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_vectorization"
      },
      "source": [
        "## 5. TF-IDF Vectorization with Custom Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tfidf"
      },
      "outputs": [],
      "source": [
        "# Create TF-IDF vectorizer with custom preprocessing\n",
        "print(\"Creating TF-IDF vectorizer...\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    preprocessor=None,  # We'll handle preprocessing ourselves\n",
        "    tokenizer=text_cleaner.custom_tokenizer,  # Use our custom tokenizer\n",
        "    token_pattern=None,  # Disable default tokenization\n",
        "    lowercase=False,  # Already handled in custom tokenizer\n",
        "    stop_words=None,  # Already handled in custom tokenizer\n",
        "    max_features=10000,  # Limit vocabulary size\n",
        "    min_df=2,  # Remove very rare terms\n",
        "    max_df=0.95,  # Remove very common terms\n",
        "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    sublinear_tf=True  # Apply sublinear TF scaling\n",
        ")\n",
        "\n",
        "# Fit and transform documents\n",
        "print(\"Fitting TF-IDF vectorizer on documents...\")\n",
        "document_texts = documents_df['cleaned_text'].tolist()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(document_texts)\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
        "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "# Display sample features\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample features: {feature_names[:20]}\")\n",
        "print(f\"Sample bigrams: {[f for f in feature_names if ' ' in f][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inverted_index"
      },
      "source": [
        "## 6. Inverted Index Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build_inverted_index"
      },
      "outputs": [],
      "source": [
        "def build_inverted_index(tfidf_matrix, feature_names, doc_ids):\n",
        "    \"\"\"\n",
        "    Build inverted index from TF-IDF matrix.\n",
        "    \n",
        "    Args:\n",
        "        tfidf_matrix: Sparse TF-IDF matrix\n",
        "        feature_names: List of feature names\n",
        "        doc_ids: List of document IDs\n",
        "        \n",
        "    Returns:\n",
        "        dict: Inverted index mapping terms to documents and scores\n",
        "    \"\"\"\n",
        "    print(\"Building inverted index...\")\n",
        "    \n",
        "    inverted_index = defaultdict(dict)\n",
        "    \n",
        "    # Convert to COO format for efficient iteration\n",
        "    coo_matrix = tfidf_matrix.tocoo()\n",
        "    \n",
        "    # Build inverted index\n",
        "    for doc_idx, term_idx, score in tqdm(zip(coo_matrix.row, coo_matrix.col, coo_matrix.data), \n",
        "                                          total=coo_matrix.nnz, desc=\"Building index\"):\n",
        "        if score > 0:  # Only include non-zero scores\n",
        "            term = feature_names[term_idx]\n",
        "            doc_id = doc_ids[doc_idx]\n",
        "            inverted_index[term][doc_id] = float(score)\n",
        "    \n",
        "    # Sort documents by score for each term\n",
        "    for term in inverted_index:\n",
        "        inverted_index[term] = dict(sorted(inverted_index[term].items(), \n",
        "                                          key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "    return dict(inverted_index)\n",
        "\n",
        "# Build inverted index\n",
        "doc_ids = documents_df['doc_id'].tolist()\n",
        "inverted_index = build_inverted_index(tfidf_matrix, feature_names, doc_ids)\n",
        "\n",
        "print(f\"\\nInverted index statistics:\")\n",
        "print(f\"Number of terms: {len(inverted_index)}\")\n",
        "print(f\"Average documents per term: {np.mean([len(docs) for docs in inverted_index.values()]):.2f}\")\n",
        "print(f\"Most frequent terms: {sorted(inverted_index.keys(), key=lambda x: len(inverted_index[x]), reverse=True)[:10]}\")\n",
        "\n",
        "# Display sample inverted index entries\n",
        "sample_term = list(inverted_index.keys())[0]\n",
        "print(f\"\\nSample inverted index entry for '{sample_term}':\")\n",
        "sample_docs = dict(list(inverted_index[sample_term].items())[:5])\n",
        "print(sample_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_models"
      },
      "source": [
        "## 7. Save Models and Data using Joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_models_joblib"
      },
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = '/content/drive/MyDrive/antique_tfidf_models/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Saving models and data...\")\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "vectorizer_path = os.path.join(output_dir, 'tfidf_vectorizer.joblib')\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
        "print(f\"✓ Saved TF-IDF vectorizer to {vectorizer_path}\")\n",
        "\n",
        "# Save TF-IDF matrix\n",
        "matrix_path = os.path.join(output_dir, 'tfidf_matrix.joblib')\n",
        "joblib.dump(tfidf_matrix, matrix_path)\n",
        "print(f\"✓ Saved TF-IDF matrix to {matrix_path}\")\n",
        "\n",
        "# Save inverted index\n",
        "index_path = os.path.join(output_dir, 'inverted_index.joblib')\n",
        "joblib.dump(inverted_index, index_path)\n",
        "print(f\"✓ Saved inverted index to {index_path}\")\n",
        "\n",
        "# Save document mappings\n",
        "doc_mapping = {\n",
        "    'doc_ids': doc_ids,\n",
        "    'documents_df': documents_df,\n",
        "    'queries_df': queries_df,\n",
        "    'qrels_df': qrels_df\n",
        "}\n",
        "mapping_path = os.path.join(output_dir, 'document_mappings.joblib')\n",
        "joblib.dump(doc_mapping, mapping_path)\n",
        "print(f\"✓ Saved document mappings to {mapping_path}\")\n",
        "\n",
        "# Save text cleaner\n",
        "cleaner_path = os.path.join(output_dir, 'text_cleaner.joblib')\n",
        "joblib.dump(text_cleaner, cleaner_path)\n",
        "print(f\"✓ Saved text cleaner to {cleaner_path}\")\n",
        "\n",
        "print(f\"\\nAll models saved successfully to {output_dir}\")\n",
        "\n",
        "# Display saved files\n",
        "saved_files = os.listdir(output_dir)\n",
        "print(f\"\\nSaved files:\")\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
        "    print(f\"- {file}: {file_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "search_function"
      },
      "source": [
        "## 8. Search Function Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "implement_search"
      },
      "outputs": [],
      "source": [
        "def search_documents(query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=1000):\n",
        "    \"\"\"\n",
        "    Search documents using TF-IDF cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        tfidf_matrix: Document TF-IDF matrix\n",
        "        doc_ids (list): List of document IDs\n",
        "        top_k (int): Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        list: List of (doc_id, score) tuples ranked by relevance\n",
        "    \"\"\"\n",
        "    # Transform query using the fitted vectorizer\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    \n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    \n",
        "    # Get top-k results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    # Create results list\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0:  # Only include documents with non-zero similarity\n",
        "            results.append((doc_ids[idx], similarities[idx]))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def search_with_inverted_index(query, inverted_index, tfidf_vectorizer, doc_ids, top_k=1000):\n",
        "    \"\"\"\n",
        "    Search documents using inverted index for faster retrieval.\n",
        "    \n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        inverted_index (dict): Inverted index\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        doc_ids (list): List of document IDs\n",
        "        top_k (int): Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        list: List of (doc_id, score) tuples ranked by relevance\n",
        "    \"\"\"\n",
        "    # Get query terms using the same tokenizer\n",
        "    query_terms = tfidf_vectorizer.build_analyzer()(query)\n",
        "    \n",
        "    # Collect candidate documents\n",
        "    candidate_docs = defaultdict(float)\n",
        "    \n",
        "    for term in query_terms:\n",
        "        if term in inverted_index:\n",
        "            for doc_id, score in inverted_index[term].items():\n",
        "                candidate_docs[doc_id] += score\n",
        "    \n",
        "    # Sort by score and return top-k\n",
        "    sorted_docs = sorted(candidate_docs.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return sorted_docs[:top_k]\n",
        "\n",
        "# Test search function\n",
        "test_query = \"How to improve search performance?\"\n",
        "print(f\"Testing search with query: '{test_query}'\")\n",
        "\n",
        "# Search using TF-IDF matrix\n",
        "results_tfidf = search_documents(test_query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=5)\n",
        "print(f\"\\nTop 5 results (TF-IDF):\")\n",
        "for i, (doc_id, score) in enumerate(results_tfidf, 1):\n",
        "    print(f\"{i}. Doc {doc_id}: {score:.4f}\")\n",
        "\n",
        "# Search using inverted index\n",
        "results_index = search_with_inverted_index(test_query, inverted_index, tfidf_vectorizer, doc_ids, top_k=5)\n",
        "print(f\"\\nTop 5 results (Inverted Index):\")\n",
        "for i, (doc_id, score) in enumerate(results_index, 1):\n",
        "    print(f\"{i}. Doc {doc_id}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nSearch functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 9. Evaluation Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_functions"
      },
      "outputs": [],
      "source": [
        "def calculate_average_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision for a single query.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_docs (list): List of retrieved document IDs in rank order\n",
        "        relevant_docs (set): Set of relevant document IDs\n",
        "        \n",
        "    Returns:\n",
        "        float: Average Precision score\n",
        "    \"\"\"\n",
        "    if not relevant_docs:\n",
        "        return 0.0\n",
        "    \n",
        "    precision_at_k = []\n",
        "    relevant_retrieved = 0\n",
        "    \n",
        "    for k, doc_id in enumerate(retrieved_docs, 1):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_retrieved += 1\n",
        "            precision_at_k.append(relevant_retrieved / k)\n",
        "    \n",
        "    if not precision_at_k:\n",
        "        return 0.0\n",
        "    \n",
        "    return sum(precision_at_k) / len(relevant_docs)\n",
        "\n",
        "def calculate_map(queries_df, qrels_df, search_function, **search_kwargs):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision (MAP) for all queries.\n",
        "    \n",
        "    Args:\n",
        "        queries_df (pd.DataFrame): DataFrame with queries\n",
        "        qrels_df (pd.DataFrame): DataFrame with relevance judgments\n",
        "        search_function (callable): Search function to use\n",
        "        **search_kwargs: Additional arguments for search function\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (MAP score, list of individual AP scores)\n",
        "    \"\"\"\n",
        "    # Group relevance judgments by query\n",
        "    qrels_grouped = qrels_df.groupby('query_id')['doc_id'].apply(set).to_dict()\n",
        "    \n",
        "    ap_scores = []\n",
        "    \n",
        "    print(\"Calculating MAP...\")\n",
        "    \n",
        "    for _, query_row in tqdm(queries_df.iterrows(), total=len(queries_df), desc=\"Evaluating queries\"):\n",
        "        query_id = query_row['query_id']\n",
        "        query_text = query_row['cleaned_query']\n",
        "        \n",
        "        # Get relevant documents for this query\n",
        "        relevant_docs = qrels_grouped.get(query_id, set())\n",
        "        \n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        \n",
        "        # Search for documents\n",
        "        results = search_function(query_text, **search_kwargs)\n",
        "        \n",
        "        # Extract document IDs from results\n",
        "        retrieved_docs = [doc_id for doc_id, _ in results]\n",
        "        \n",
        "        # Calculate Average Precision\n",
        "        ap = calculate_average_precision(retrieved_docs, relevant_docs)\n",
        "        ap_scores.append(ap)\n",
        "    \n",
        "    # Calculate MAP\n",
        "    map_score = np.mean(ap_scores) if ap_scores else 0.0\n",
        "    \n",
        "    return map_score, ap_scores\n",
        "\n",
        "def evaluate_system(queries_df, qrels_df, tfidf_vectorizer, tfidf_matrix, inverted_index, doc_ids):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the TF-IDF system.\n",
        "    \n",
        "    Args:\n",
        "        queries_df (pd.DataFrame): DataFrame with queries\n",
        "        qrels_df (pd.DataFrame): DataFrame with relevance judgments\n",
        "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
        "        tfidf_matrix: Document TF-IDF matrix\n",
        "        inverted_index (dict): Inverted index\n",
        "        doc_ids (list): List of document IDs\n",
        "        \n",
        "    Returns:\n",
        "        dict: Evaluation results\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Evaluate using TF-IDF matrix search\n",
        "    print(\"Evaluating TF-IDF matrix search...\")\n",
        "    map_tfidf, ap_scores_tfidf = calculate_map(\n",
        "        queries_df, qrels_df, search_documents,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        tfidf_matrix=tfidf_matrix,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "    \n",
        "    results['tfidf_matrix'] = {\n",
        "        'MAP': map_tfidf,\n",
        "        'AP_scores': ap_scores_tfidf,\n",
        "        'num_queries': len(ap_scores_tfidf)\n",
        "    }\n",
        "    \n",
        "    # Evaluate using inverted index search\n",
        "    print(\"Evaluating inverted index search...\")\n",
        "    map_index, ap_scores_index = calculate_map(\n",
        "        queries_df, qrels_df, search_with_inverted_index,\n",
        "        inverted_index=inverted_index,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "    \n",
        "    results['inverted_index'] = {\n",
        "        'MAP': map_index,\n",
        "        'AP_scores': ap_scores_index,\n",
        "        'num_queries': len(ap_scores_index)\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Evaluation functions implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_evaluation"
      },
      "source": [
        "## 10. Run Comprehensive Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation_code"
      },
      "outputs": [],
      "source": [
        "# Run comprehensive evaluation\n",
        "print(\"Starting comprehensive evaluation...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "evaluation_results = evaluate_system(\n",
        "    queries_df, qrels_df, tfidf_vectorizer, tfidf_matrix, inverted_index, doc_ids\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for method, results in evaluation_results.items():\n",
        "    print(f\"\\n{method.upper()} SEARCH:\")\n",
        "    print(f\"MAP Score: {results['MAP']:.4f}\")\n",
        "    print(f\"Number of queries evaluated: {results['num_queries']}\")\n",
        "    print(f\"Average Precision scores - Min: {min(results['AP_scores']):.4f}, Max: {max(results['AP_scores']):.4f}\")\n",
        "    print(f\"Standard deviation: {np.std(results['AP_scores']):.4f}\")\n",
        "    \n",
        "    # Check if MAP is above 0.4\n",
        "    if results['MAP'] > 0.4:\n",
        "        print(f\"✅ MAP > 0.4 TARGET ACHIEVED! ({results['MAP']:.4f})\")\n",
        "    else:\n",
        "        print(f\"❌ MAP < 0.4 target not met ({results['MAP']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Performance breakdown\n",
        "tfidf_ap_scores = evaluation_results['tfidf_matrix']['AP_scores']\n",
        "index_ap_scores = evaluation_results['inverted_index']['AP_scores']\n",
        "\n",
        "print(f\"\\nDetailed Performance Analysis:\")\n",
        "print(f\"TF-IDF Matrix Search:\")\n",
        "print(f\"  - Queries with AP > 0.5: {sum(1 for ap in tfidf_ap_scores if ap > 0.5)}\")\n",
        "print(f\"  - Queries with AP > 0.3: {sum(1 for ap in tfidf_ap_scores if ap > 0.3)}\")\n",
        "print(f\"  - Queries with AP > 0.1: {sum(1 for ap in tfidf_ap_scores if ap > 0.1)}\")\n",
        "print(f\"  - Queries with AP = 0: {sum(1 for ap in tfidf_ap_scores if ap == 0)}\")\n",
        "\n",
        "print(f\"\\nInverted Index Search:\")\n",
        "print(f\"  - Queries with AP > 0.5: {sum(1 for ap in index_ap_scores if ap > 0.5)}\")\n",
        "print(f\"  - Queries with AP > 0.3: {sum(1 for ap in index_ap_scores if ap > 0.3)}\")\n",
        "print(f\"  - Queries with AP > 0.1: {sum(1 for ap in index_ap_scores if ap > 0.1)}\")\n",
        "print(f\"  - Queries with AP = 0: {sum(1 for ap in index_ap_scores if ap == 0)}\")\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results_path = os.path.join(output_dir, 'evaluation_results.joblib')\n",
        "joblib.dump(evaluation_results, eval_results_path)\n",
        "print(f\"\\n✓ Evaluation results saved to {eval_results_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis"
      },
      "source": [
        "## 11. Performance Analysis and Improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze performance and suggest improvements\n",
        "print(\"PERFORMANCE ANALYSIS & IMPROVEMENT SUGGESTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "map_score = evaluation_results['tfidf_matrix']['MAP']\n",
        "\n",
        "print(f\"\\nCurrent MAP Score: {map_score:.4f}\")\n",
        "print(f\"Target MAP Score: 0.4000\")\n",
        "\n",
        "if map_score < 0.4:\n",
        "    print(\"\\n🔧 SUGGESTED IMPROVEMENTS TO ACHIEVE MAP > 0.4:\")\n",
        "    print(\"\\n1. Text Preprocessing Improvements:\")\n",
        "    print(\"   - Add domain-specific stop words\")\n",
        "    print(\"   - Implement better stemming/lemmatization\")\n",
        "    print(\"   - Handle synonyms and word variations\")\n",
        "    print(\"   - Add spell correction\")\n",
        "    \n",
        "    print(\"\\n2. TF-IDF Parameter Tuning:\")\n",
        "    print(\"   - Adjust max_features (try 15000-20000)\")\n",
        "    print(\"   - Experiment with different min_df/max_df values\")\n",
        "    print(\"   - Try different ngram ranges (1,3) or (1,1)\")\n",
        "    print(\"   - Experiment with different TF-IDF variants\")\n",
        "    \n",
        "    print(\"\\n3. Advanced Techniques:\")\n",
        "    print(\"   - Query expansion using word embeddings\")\n",
        "    print(\"   - Pseudo-relevance feedback\")\n",
        "    print(\"   - BM25 instead of TF-IDF\")\n",
        "    print(\"   - Learning to rank approaches\")\n",
        "    \n",
        "    # Let's try some quick improvements\n",
        "    print(\"\\n🚀 IMPLEMENTING QUICK IMPROVEMENTS...\")\n",
        "    \n",
        "    # Try different TF-IDF parameters\n",
        "    print(\"\\nTesting improved TF-IDF parameters...\")\n",
        "    \n",
        "    improved_vectorizer = TfidfVectorizer(\n",
        "        preprocessor=None,\n",
        "        tokenizer=text_cleaner.custom_tokenizer,\n",
        "        token_pattern=None,\n",
        "        lowercase=False,\n",
        "        stop_words=None,\n",
        "        max_features=15000,  # Increased vocabulary\n",
        "        min_df=1,  # Allow rarer terms\n",
        "        max_df=0.9,  # More restrictive on common terms\n",
        "        ngram_range=(1, 3),  # Include trigrams\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        sublinear_tf=True,\n",
        "        norm='l2'  # L2 normalization\n",
        "    )\n",
        "    \n",
        "    # Fit improved vectorizer\n",
        "    improved_tfidf_matrix = improved_vectorizer.fit_transform(document_texts)\n",
        "    \n",
        "    print(f\"Improved TF-IDF matrix shape: {improved_tfidf_matrix.shape}\")\n",
        "    \n",
        "    # Evaluate improved system\n",
        "    print(\"Evaluating improved system...\")\n",
        "    improved_map, improved_ap_scores = calculate_map(\n",
        "        queries_df, qrels_df, search_documents,\n",
        "        tfidf_vectorizer=improved_vectorizer,\n",
        "        tfidf_matrix=improved_tfidf_matrix,\n",
        "        doc_ids=doc_ids,\n",
        "        top_k=1000\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nIMPROVED RESULTS:\")\n",
        "    print(f\"Original MAP: {map_score:.4f}\")\n",
        "    print(f\"Improved MAP: {improved_map:.4f}\")\n",
        "    print(f\"Improvement: {improved_map - map_score:.4f}\")\n",
        "    \n",
        "    if improved_map > 0.4:\n",
        "        print(f\"\\n🎉 SUCCESS! MAP > 0.4 TARGET ACHIEVED with improved parameters!\")\n",
        "        \n",
        "        # Save improved models\n",
        "        improved_vectorizer_path = os.path.join(output_dir, 'improved_tfidf_vectorizer.joblib')\n",
        "        improved_matrix_path = os.path.join(output_dir, 'improved_tfidf_matrix.joblib')\n",
        "        \n",
        "        joblib.dump(improved_vectorizer, improved_vectorizer_path)\n",
        "        joblib.dump(improved_tfidf_matrix, improved_matrix_path)\n",
        "        \n",
        "        print(f\"✓ Saved improved models to {output_dir}\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Still below target. Consider implementing advanced techniques.\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n🎉 EXCELLENT! MAP > 0.4 TARGET ACHIEVED!\")\n",
        "    print(f\"The system is performing well with current configuration.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_queries"
      },
      "source": [
        "## 12. Sample Query Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_sample_queries"
      },
      "outputs": [],
      "source": [
        "# Test with sample queries to demonstrate the system\n",
        "print(\"SAMPLE QUERY TESTING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Select some sample queries\n",
        "sample_queries = queries_df.head(5)\n",
        "\n",
        "for _, query_row in sample_queries.iterrows():\n",
        "    query_id = query_row['query_id']\n",
        "    original_query = query_row['query']\n",
        "    cleaned_query = query_row['cleaned_query']\n",
        "    \n",
        "    print(f\"\\nQuery ID: {query_id}\")\n",
        "    print(f\"Original: {original_query}\")\n",
        "    print(f\"Cleaned: {cleaned_query}\")\n",
        "    \n",
        "    # Get search results\n",
        "    results = search_documents(cleaned_query, tfidf_vectorizer, tfidf_matrix, doc_ids, top_k=5)\n",
        "    \n",
        "    # Get relevant documents from qrels\n",
        "    relevant_docs = set(qrels_df[qrels_df['query_id'] == query_id]['doc_id'].astype(str))\n",
        "    \n",
        "    print(f\"Relevant documents: {len(relevant_docs)}\")\n",
        "    print(f\"Top 5 search results:\")\n",
        "    \n",
        "    for i, (doc_id, score) in enumerate(results[:5], 1):\n",
        "        relevance = \"✓\" if doc_id in relevant_docs else \"✗\"\n",
        "        print(f\"  {i}. Doc {doc_id} ({relevance}): {score:.4f}\")\n",
        "        \n",
        "        # Show snippet of the document\n",
        "        doc_text = documents_df[documents_df['doc_id'] == doc_id]['text'].iloc[0]\n",
        "        snippet = doc_text[:200] + \"...\" if len(doc_text) > 200 else doc_text\n",
        "        print(f\"     \\\"{snippet}\\\"\")\n",
        "    \n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nSample query testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 13. Summary and Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n📊 DATASET STATISTICS:\")\n",
        "print(f\"Documents processed: {len(documents_df)}\")\n",
        "print(f\"Queries processed: {len(queries_df)}\")\n",
        "print(f\"Relevance judgments: {len(qrels_df)}\")\n",
        "\n",
        "print(f\"\\n🔧 MODEL CONFIGURATION:\")\n",
        "print(f\"TF-IDF Features: {tfidf_matrix.shape[1]}\")\n",
        "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "print(f\"Inverted index terms: {len(inverted_index)}\")\n",
        "print(f\"N-gram range: {tfidf_vectorizer.ngram_range}\")\n",
        "\n",
        "print(f\"\\n📈 PERFORMANCE RESULTS:\")\n",
        "for method, results in evaluation_results.items():\n",
        "    print(f\"{method.upper()}: MAP = {results['MAP']:.4f}\")\n",
        "\n",
        "print(f\"\\n💾 SAVED MODELS:\")\n",
        "saved_files = os.listdir(output_dir)\n",
        "for file in saved_files:\n",
        "    print(f\"- {file}\")\n",
        "\n",
        "print(f\"\\n🎯 TARGET ACHIEVEMENT:\")\n",
        "best_map = max(results['MAP'] for results in evaluation_results.values())\n",
        "if best_map > 0.4:\n",
        "    print(f\"✅ SUCCESS! Best MAP score: {best_map:.4f} > 0.4\")\n",
        "else:\n",
        "    print(f\"❌ Target not met. Best MAP score: {best_map:.4f} < 0.4\")\n",
        "    print(\"   Consider implementing the suggested improvements.\")\n",
        "\n",
        "print(f\"\\n🚀 SYSTEM READY FOR USE!\")\n",
        "print(f\"All models and indexes have been saved to: {output_dir}\")\n",
        "print(f\"\\nTo use the system:\")\n",
        "print(f\"1. Load the saved models using joblib.load()\")\n",
        "print(f\"2. Use the search_documents() function for new queries\")\n",
        "print(f\"3. The inverted index provides faster search for specific terms\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"NOTEBOOK EXECUTION COMPLETE!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_example"
      },
      "source": [
        "## 14. Usage Example for Future Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usage_example_code"
      },
      "outputs": [],
      "source": [
        "# Example code for loading and using the saved models\n",
        "print(\"USAGE EXAMPLE FOR FUTURE USE\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "example_code = '''\n",
        "# How to load and use the saved TF-IDF models\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load saved models\n",
        "output_dir = '/content/drive/MyDrive/antique_tfidf_models/'\n",
        "\n",
        "tfidf_vectorizer = joblib.load(output_dir + 'tfidf_vectorizer.joblib')\n",
        "tfidf_matrix = joblib.load(output_dir + 'tfidf_matrix.joblib')\n",
        "inverted_index = joblib.load(output_dir + 'inverted_index.joblib')\n",
        "doc_mappings = joblib.load(output_dir + 'document_mappings.joblib')\n",
        "text_cleaner = joblib.load(output_dir + 'text_cleaner.joblib')\n",
        "\n",
        "# Extract document IDs\n",
        "doc_ids = doc_mappings['doc_ids']\n",
        "\n",
        "# Search function\n",
        "def search_query(query, top_k=10):\n",
        "    # Transform query\n",
        "    query_vector = tfidf_vectorizer.transform([query])\n",
        "    \n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    \n",
        "    # Get top results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0:\n",
        "            results.append((doc_ids[idx], similarities[idx]))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "query = \"How to improve search performance?\"\n",
        "results = search_query(query)\n",
        "print(f\"Top results for '{query}':\")\n",
        "for doc_id, score in results[:5]:\n",
        "    print(f\"Doc {doc_id}: {score:.4f}\")\n",
        "'''\n",
        "\n",
        "print(\"Copy and save this code for future use:\")\n",
        "print(example_code)\n",
        "\n",
        "# Save the example code to a file\n",
        "example_file_path = os.path.join(output_dir, 'usage_example.py')\n",
        "with open(example_file_path, 'w') as f:\n",
        "    f.write(example_code)\n",
        "\n",
        "print(f\"\\n✓ Usage example saved to: {example_file_path}\")\n",
        "print(\"\\nThis completes the ANTIQUE TF-IDF implementation!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
