#!/usr/bin/env python3
"""
BERTweet Embedding Service
Service for loading and managing BERTweet embeddings generated from Google Colab.
Integrates with the search engine backend for document retrieval.
"""

import numpy as np
import json
import pickle
import logging
import asyncio
import aiosqlite
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import faiss
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BERTweetEmbeddingService:
    """
    Service for managing BERTweet embeddings and performing similarity search.
    """
    
    def __init__(self, db_path: str = "data/search_engine.db"):
        """
        Initialize the BERTweet embedding service.
        
        Args:
            db_path: Path to SQLite database
        """
        self.db_path = db_path
        self.embeddings = None
        self.doc_ids = None
        self.metadata = None
        self.faiss_index = None
        self.embedding_dimension = None
        
        # Ensure database path exists
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"BERTweet embedding service initialized with database: {self.db_path}")
    
    async def create_embedding_table(self):
        """Create table for storing embedding metadata."""
        try:
            async with aiosqlite.connect(self.db_path) as conn:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS bertweet_embeddings (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        doc_id TEXT UNIQUE NOT NULL,
                        embedding_vector BLOB,
                        norm REAL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS embedding_metadata (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        model_name TEXT NOT NULL,
                        embedding_dimension INTEGER NOT NULL,
                        total_documents INTEGER NOT NULL,
                        generation_timestamp TEXT,
                        processing_time_seconds REAL,
                        device_used TEXT,
                        file_path TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Create index for faster lookups
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_bertweet_doc_id 
                    ON bertweet_embeddings(doc_id)
                """)
                
                await conn.commit()
                logger.info("‚úÖ Created BERTweet embedding tables")
                
        except Exception as e:
            logger.error(f"‚ùå Error creating embedding tables: {e}")
            raise
    
    def load_embeddings_from_numpy(self, 
                                  embeddings_path: str, 
                                  doc_ids_path: str, 
                                  metadata_path: str) -> bool:
        """
        Load embeddings from NumPy files generated by Colab.
        
        Args:
            embeddings_path: Path to embeddings .npy file
            doc_ids_path: Path to document IDs .npy file
            metadata_path: Path to metadata .json file
            
        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info("üìñ Loading BERTweet embeddings from NumPy files...")
            
            # Load embeddings
            logger.info(f"   Loading embeddings from {embeddings_path}")
            self.embeddings = np.load(embeddings_path)
            
            # Load document IDs
            logger.info(f"   Loading document IDs from {doc_ids_path}")
            self.doc_ids = np.load(doc_ids_path)
            
            # Load metadata
            logger.info(f"   Loading metadata from {metadata_path}")
            with open(metadata_path, 'r') as f:
                self.metadata = json.load(f)
            
            self.embedding_dimension = self.embeddings.shape[1]
            
            logger.info("‚úÖ BERTweet embeddings loaded successfully!")
            logger.info(f"   Embeddings shape: {self.embeddings.shape}")
            logger.info(f"   Document count: {len(self.doc_ids):,}")
            logger.info(f"   Embedding dimension: {self.embedding_dimension}")
            logger.info(f"   Model: {self.metadata.get('model_name', 'Unknown')}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading embeddings: {e}")
            return False
    
    def load_embeddings_from_pickle(self, pickle_path: str) -> bool:
        """
        Load complete dataset from pickle file generated by Colab.
        
        Args:
            pickle_path: Path to pickle file
            
        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info(f"üìñ Loading BERTweet embeddings from pickle file: {pickle_path}")
            
            with open(pickle_path, 'rb') as f:
                data = pickle.load(f)
            
            self.embeddings = data['embeddings']
            self.doc_ids = np.array(data['doc_ids'])
            self.metadata = data['metadata']
            self.embedding_dimension = self.embeddings.shape[1]
            
            logger.info("‚úÖ BERTweet embeddings loaded successfully!")
            logger.info(f"   Embeddings shape: {self.embeddings.shape}")
            logger.info(f"   Document count: {len(self.doc_ids):,}")
            logger.info(f"   Embedding dimension: {self.embedding_dimension}")
            logger.info(f"   Model: {self.metadata.get('model_name', 'Unknown')}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading embeddings from pickle: {e}")
            return False
    
    def build_faiss_index(self, index_type: str = "IVF") -> bool:
        """
        Build FAISS index for fast similarity search.
        
        Args:
            index_type: Type of FAISS index ("Flat", "IVF", "HNSW")
            
        Returns:
            True if successful, False otherwise
        """
        try:
            if self.embeddings is None:
                logger.error("‚ùå No embeddings loaded. Load embeddings first.")
                return False
            
            logger.info(f"üîß Building FAISS index (type: {index_type})...")
            
            # Normalize embeddings for cosine similarity
            normalized_embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)
            
            if index_type == "Flat":
                # Flat index for exact search
                self.faiss_index = faiss.IndexFlatIP(self.embedding_dimension)
                self.faiss_index.add(normalized_embeddings.astype(np.float32))
                
            elif index_type == "IVF":
                # IVF index for approximate search
                nlist = min(100, int(np.sqrt(len(self.embeddings))))  # Number of clusters
                quantizer = faiss.IndexFlatIP(self.embedding_dimension)
                self.faiss_index = faiss.IndexIVFFlat(quantizer, self.embedding_dimension, nlist)
                
                # Train the index
                self.faiss_index.train(normalized_embeddings.astype(np.float32))
                self.faiss_index.add(normalized_embeddings.astype(np.float32))
                
                # Set search parameters
                self.faiss_index.nprobe = min(10, nlist)
                
            elif index_type == "HNSW":
                # HNSW index for fast approximate search
                M = 32  # Number of connections
                self.faiss_index = faiss.IndexHNSWFlat(self.embedding_dimension, M)
                self.faiss_index.hnsw.efConstruction = 200
                self.faiss_index.hnsw.efSearch = 50
                self.faiss_index.add(normalized_embeddings.astype(np.float32))
                
            else:
                logger.error(f"‚ùå Unknown index type: {index_type}")
                return False
            
            logger.info(f"‚úÖ FAISS index built successfully!")
            logger.info(f"   Index type: {index_type}")
            logger.info(f"   Total vectors: {self.faiss_index.ntotal:,}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error building FAISS index: {e}")
            return False
    
    async def save_embeddings_to_database(self) -> bool:
        """
        Save embeddings to database for persistence.
        
        Returns:
            True if successful, False otherwise
        """
        try:
            if self.embeddings is None or self.doc_ids is None:
                logger.error("‚ùå No embeddings loaded. Load embeddings first.")
                return False
            
            await self.create_embedding_table()
            
            logger.info("üíæ Saving embeddings to database...")
            
            async with aiosqlite.connect(self.db_path) as conn:
                # Save embedding metadata
                await conn.execute("""
                    INSERT OR REPLACE INTO embedding_metadata (
                        model_name, embedding_dimension, total_documents,
                        generation_timestamp, processing_time_seconds, device_used, file_path
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    self.metadata.get('model_name', 'vinai/bertweet-base'),
                    self.embedding_dimension,
                    len(self.doc_ids),
                    self.metadata.get('generation_timestamp'),
                    self.metadata.get('processing_time_seconds'),
                    self.metadata.get('device_used'),
                    'bertweet_embeddings'
                ))
                
                # Prepare embedding data for batch insert
                embedding_data = []
                for i, (doc_id, embedding) in enumerate(zip(self.doc_ids, self.embeddings)):
                    # Convert embedding to bytes
                    embedding_bytes = embedding.astype(np.float32).tobytes()
                    norm = np.linalg.norm(embedding)
                    
                    embedding_data.append((
                        str(doc_id),
                        embedding_bytes,
                        float(norm)
                    ))
                    
                    # Process in batches to avoid memory issues
                    if len(embedding_data) >= 1000:
                        await conn.executemany("""
                            INSERT OR REPLACE INTO bertweet_embeddings (doc_id, embedding_vector, norm)
                            VALUES (?, ?, ?)
                        """, embedding_data)
                        embedding_data = []
                        logger.info(f"   Saved batch ending at index {i:,}")
                
                # Save remaining data
                if embedding_data:
                    await conn.executemany("""
                        INSERT OR REPLACE INTO bertweet_embeddings (doc_id, embedding_vector, norm)
                        VALUES (?, ?, ?)
                    """, embedding_data)
                
                await conn.commit()
                
            logger.info(f"‚úÖ Saved {len(self.doc_ids):,} embeddings to database")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error saving embeddings to database: {e}")
            return False
    
    def search_similar_documents(self, 
                                query_embedding: np.ndarray, 
                                k: int = 10, 
                                threshold: float = 0.0) -> List[Tuple[str, float]]:
        """
        Search for similar documents using the query embedding.
        
        Args:
            query_embedding: Query embedding vector
            k: Number of results to return
            threshold: Minimum similarity threshold
            
        Returns:
            List of (doc_id, similarity_score) tuples
        """
        try:
            if self.faiss_index is None:
                logger.error("‚ùå No FAISS index built. Build index first.")
                return []
            
            # Normalize query embedding
            query_norm = query_embedding / np.linalg.norm(query_embedding)
            query_norm = query_norm.reshape(1, -1).astype(np.float32)
            
            # Search using FAISS
            similarities, indices = self.faiss_index.search(query_norm, k)
            
            # Filter results and return
            results = []
            for sim, idx in zip(similarities[0], indices[0]):
                if idx >= 0 and sim >= threshold:  # Valid index and above threshold
                    doc_id = str(self.doc_ids[idx])
                    results.append((doc_id, float(sim)))
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error searching similar documents: {e}")
            return []
    
    def get_document_embedding(self, doc_id: str) -> Optional[np.ndarray]:
        """
        Get embedding for a specific document.
        
        Args:
            doc_id: Document ID
            
        Returns:
            Embedding vector or None if not found
        """
        try:
            if self.doc_ids is None or self.embeddings is None:
                return None
            
            # Find document index
            doc_indices = np.where(self.doc_ids == doc_id)[0]
            
            if len(doc_indices) == 0:
                return None
            
            return self.embeddings[doc_indices[0]]
            
        except Exception as e:
            logger.error(f"‚ùå Error getting document embedding: {e}")
            return None
    
    def compute_document_similarities(self, doc_id: str, k: int = 10) -> List[Tuple[str, float]]:
        """
        Compute similarities for a specific document.
        
        Args:
            doc_id: Document ID
            k: Number of similar documents to return
            
        Returns:
            List of (doc_id, similarity_score) tuples
        """
        try:
            # Get document embedding
            doc_embedding = self.get_document_embedding(doc_id)
            
            if doc_embedding is None:
                logger.warning(f"Document {doc_id} not found")
                return []
            
            # Search for similar documents
            results = self.search_similar_documents(doc_embedding, k + 1)  # +1 to exclude self
            
            # Remove self from results
            filtered_results = [(did, sim) for did, sim in results if did != doc_id]
            
            return filtered_results[:k]
            
        except Exception as e:
            logger.error(f"‚ùå Error computing document similarities: {e}")
            return []
    
    def get_service_statistics(self) -> Dict[str, Any]:
        """
        Get service statistics and information.
        
        Returns:
            Dictionary with service statistics
        """
        try:
            stats = {
                "embeddings_loaded": self.embeddings is not None,
                "faiss_index_built": self.faiss_index is not None,
                "embedding_dimension": self.embedding_dimension,
                "total_documents": len(self.doc_ids) if self.doc_ids is not None else 0,
                "index_type": type(self.faiss_index).__name__ if self.faiss_index else None,
                "total_vectors_indexed": self.faiss_index.ntotal if self.faiss_index else 0
            }
            
            if self.metadata:
                stats.update({
                    "model_name": self.metadata.get('model_name'),
                    "generation_timestamp": self.metadata.get('generation_timestamp'),
                    "processing_time_seconds": self.metadata.get('processing_time_seconds'),
                    "device_used": self.metadata.get('device_used')
                })
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Error getting service statistics: {e}")
            return {"error": str(e)}
    
    def save_index_to_disk(self, index_path: str) -> bool:
        """
        Save FAISS index to disk.
        
        Args:
            index_path: Path to save the index
            
        Returns:
            True if successful, False otherwise
        """
        try:
            if self.faiss_index is None:
                logger.error("‚ùå No FAISS index to save")
                return False
            
            logger.info(f"üíæ Saving FAISS index to {index_path}")
            faiss.write_index(self.faiss_index, index_path)
            logger.info("‚úÖ FAISS index saved successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error saving FAISS index: {e}")
            return False
    
    def load_index_from_disk(self, index_path: str) -> bool:
        """
        Load FAISS index from disk.
        
        Args:
            index_path: Path to load the index from
            
        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info(f"üìñ Loading FAISS index from {index_path}")
            self.faiss_index = faiss.read_index(index_path)
            logger.info("‚úÖ FAISS index loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading FAISS index: {e}")
            return False

# Factory function
def create_bertweet_embedding_service(db_path: str = "data/search_engine.db") -> BERTweetEmbeddingService:
    """
    Factory function to create BERTweet embedding service.
    
    Args:
        db_path: Path to SQLite database
        
    Returns:
        BERTweetEmbeddingService instance
    """
    return BERTweetEmbeddingService(db_path=db_path)

# Example usage
if __name__ == "__main__":
    async def test_service():
        # Create service
        service = BERTweetEmbeddingService()
        
        # Load embeddings (example paths - adjust as needed)
        success = service.load_embeddings_from_numpy(
            "data/antique_bertweet_embeddings.npy",
            "data/antique_bertweet_doc_ids.npy",
            "data/antique_bertweet_metadata.json"
        )
        
        if success:
            # Build FAISS index
            service.build_faiss_index("IVF")
            
            # Save to database
            await service.save_embeddings_to_database()
            
            # Get statistics
            stats = service.get_service_statistics()
            print("Service Statistics:", json.dumps(stats, indent=2))
    
    # Run test
    asyncio.run(test_service())
