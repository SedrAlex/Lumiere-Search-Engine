{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Evaluation Notebook\n",
    "\n",
    "This notebook evaluates TF-IDF system performance with service query processing.\n",
    "\n",
    "## Metrics Calculated:\n",
    "- **Mean Average Precision (MAP)**: Average precision across all queries\n",
    "- **Mean Reciprocal Rank (MRR)**: Average reciprocal rank of first relevant document\n",
    "- **Precision@10**: Precision at rank 10\n",
    "- **Recall@10**: Recall at rank 10\n",
    "\n",
    "## Prerequisites:\n",
    "- TF-IDF query service running on port 8004\n",
    "- Enhanced text cleaning service running on port 8003\n",
    "- Pre-trained TF-IDF models in the models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import logging\n",
    "import time\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import evaluation engine\n",
    "from evaluation_engine import IRMetrics, SearchEvaluator\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TFIDF_QUERY_SERVICE_URL = \"http://localhost:8004\"\n",
    "MODEL_BASE_PATH = \"/Users/raafatmhanna/Desktop/custom-search-engine/backend/models\"\n",
    "\n",
    "print(f\"🔧 TF-IDF Service URL: {TFIDF_QUERY_SERVICE_URL}\")\n",
    "print(f\"📁 Model Base Path: {MODEL_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service availability\n",
    "async def check_service_status():\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(f\"{TFIDF_QUERY_SERVICE_URL}/health\")\n",
    "            return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Check if service is running\n",
    "service_available = await check_service_status()\n",
    "if service_available:\n",
    "    print(\"✅ TF-IDF query service is available\")\n",
    "else:\n",
    "    print(\"❌ TF-IDF query service is NOT available\")\n",
    "    print(\"   Please start the service with: python services/query_processing/tfidf_query_processor.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ANTIQUE dataset queries and relevance judgments\n",
    "def load_dataset_queries_and_qrels():\n",
    "    \"\"\"Load queries and relevance judgments from ANTIQUE dataset\"\"\"\n",
    "    logger.info(\"Loading ANTIQUE dataset queries and qrels...\")\n",
    "    \n",
    "    dataset = ir_datasets.load('antique/train')\n",
    "    \n",
    "    # Load queries\n",
    "    queries = []\n",
    "    for query in dataset.queries_iter():\n",
    "        queries.append({\n",
    "            'query_id': query.query_id,\n",
    "            'text': query.text\n",
    "        })\n",
    "    \n",
    "    # Load qrels (relevance judgments)\n",
    "    qrels = {}\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel.query_id not in qrels:\n",
    "            qrels[qrel.query_id] = {}\n",
    "        qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
    "    \n",
    "    logger.info(f\"Loaded {len(queries)} queries and qrels for {len(qrels)} queries\")\n",
    "    return queries, qrels\n",
    "\n",
    "# Load dataset\n",
    "queries, qrels = load_dataset_queries_and_qrels()\n",
    "print(f\"📚 Loaded {len(queries)} queries\")\n",
    "print(f\"📊 Loaded qrels for {len(qrels)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query TF-IDF service\n",
    "async def query_tfidf_service(query: str, top_k: int = 1000):\n",
    "    \"\"\"Query the TF-IDF service and return ranked document IDs\"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "            response = await client.post(\n",
    "                f\"{TFIDF_QUERY_SERVICE_URL}/search\",\n",
    "                json={\n",
    "                    \"query\": query,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"similarity_threshold\": 0.0,\n",
    "                    \"use_enhanced_cleaning\": True\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract document IDs in rank order\n",
    "            return [doc[\"doc_id\"] for doc in result[\"results\"]]\n",
    "            \n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"Service request error for query '{query}': {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Query function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample query\n",
    "if service_available:\n",
    "    sample_query = \"machine learning algorithms\"\n",
    "    print(f\"🧪 Testing with sample query: '{sample_query}'...\")\n",
    "    \n",
    "    sample_results = await query_tfidf_service(sample_query, top_k=5)\n",
    "    print(f\"📄 Retrieved {len(sample_results)} documents\")\n",
    "    \n",
    "    if sample_results:\n",
    "        print(\"✅ Sample query successful\")\n",
    "        print(f\"   Top 3 results: {sample_results[:3]}\")\n",
    "    else:\n",
    "        print(\"❌ Sample query failed\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping sample query test - service not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation metrics\n",
    "metrics = IRMetrics()\n",
    "evaluator = SearchEvaluator()\n",
    "\n",
    "print(\"✅ Evaluation engines initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single query\n",
    "async def evaluate_single_query(query: Dict[str, str], query_qrels: Dict[str, float]):\n",
    "    \"\"\"Evaluate a single query and return metrics\"\"\"\n",
    "    query_id = query['query_id']\n",
    "    query_text = query['text']\n",
    "    \n",
    "    # Get search results from TF-IDF service\n",
    "    retrieved_docs = await query_tfidf_service(query_text, top_k=1000)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        logger.warning(f\"No results for query {query_id}\")\n",
    "        return {\n",
    "            'precision_at_10': 0.0,\n",
    "            'recall_at_10': 0.0,\n",
    "            'average_precision': 0.0,\n",
    "            'reciprocal_rank': 0.0\n",
    "        }\n",
    "    \n",
    "    # Get relevant documents (relevance > 0)\n",
    "    relevant_docs = [doc_id for doc_id, rel in query_qrels.items() if rel > 0]\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        logger.warning(f\"No relevant documents for query {query_id}\")\n",
    "        return {\n",
    "            'precision_at_10': 0.0,\n",
    "            'recall_at_10': 0.0,\n",
    "            'average_precision': 0.0,\n",
    "            'reciprocal_rank': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_at_10 = metrics.precision_at_k(retrieved_docs, relevant_docs, 10)\n",
    "    recall_at_10 = metrics.recall_at_k(retrieved_docs, relevant_docs, 10)\n",
    "    average_precision = metrics.average_precision(retrieved_docs, relevant_docs)\n",
    "    reciprocal_rank = metrics.reciprocal_rank(retrieved_docs, relevant_docs)\n",
    "    \n",
    "    return {\n",
    "        'precision_at_10': precision_at_10,\n",
    "        'recall_at_10': recall_at_10,\n",
    "        'average_precision': average_precision,\n",
    "        'reciprocal_rank': reciprocal_rank\n",
    "    }\n",
    "\n",
    "print(\"✅ Single query evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evaluation function\n",
    "async def evaluate_tfidf_system():\n",
    "    \"\"\"Evaluate the entire TF-IDF system\"\"\"\n",
    "    if not service_available:\n",
    "        raise RuntimeError(\"TF-IDF service is not available. Please start the service first.\")\n",
    "    \n",
    "    logger.info(\"Starting TF-IDF system evaluation...\")\n",
    "    \n",
    "    query_results = []\n",
    "    evaluated_queries = 0\n",
    "    \n",
    "    # Process each query\n",
    "    for query in tqdm(queries, desc=\"Evaluating queries\"):\n",
    "        query_id = query['query_id']\n",
    "        \n",
    "        # Skip queries without relevance judgments\n",
    "        if query_id not in qrels or not qrels[query_id]:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            query_metrics = await evaluate_single_query(query, qrels[query_id])\n",
    "            query_results.append(query_metrics)\n",
    "            evaluated_queries += 1\n",
    "            \n",
    "            # Log progress every 50 queries\n",
    "            if evaluated_queries % 50 == 0:\n",
    "                logger.info(f\"Evaluated {evaluated_queries} queries...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating query {query_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not query_results:\n",
    "        logger.error(\"No queries were successfully evaluated\")\n",
    "        return {}\n",
    "    \n",
    "    # Aggregate results\n",
    "    aggregated = {}\n",
    "    for metric in query_results[0].keys():\n",
    "        values = [result[metric] for result in query_results if metric in result]\n",
    "        aggregated[metric] = sum(values) / len(values) if values else 0.0\n",
    "    \n",
    "    # Calculate MAP and MRR\n",
    "    aggregated['map'] = aggregated.get('average_precision', 0.0)  # MAP = mean AP\n",
    "    aggregated['mrr'] = aggregated.get('reciprocal_rank', 0.0)    # MRR = mean RR\n",
    "    aggregated['num_queries_evaluated'] = evaluated_queries\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "print(\"✅ Main evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "if service_available:\n",
    "    print(\"🚀 Starting evaluation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = await evaluate_tfidf_system()\n",
    "    \n",
    "    evaluation_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation completed in {evaluation_time:.2f} seconds\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot run evaluation - service not available\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "def print_detailed_results(results):\n",
    "    \"\"\"Print detailed evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TF-IDF EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ No results available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Evaluated on {results.get('num_queries_evaluated', 0)} queries\")\n",
    "    print(\"\\n📈 CORE METRICS:\")\n",
    "    print(f\"   Mean Average Precision (MAP): {results.get('map', 0.0):.4f}\")\n",
    "    print(f\"   Mean Reciprocal Rank (MRR):  {results.get('mrr', 0.0):.4f}\")\n",
    "    print(f\"   Precision@10:                {results.get('precision_at_10', 0.0):.4f}\")\n",
    "    print(f\"   Recall@10:                   {results.get('recall_at_10', 0.0):.4f}\")\n",
    "    \n",
    "    print(\"\\n📋 INTERPRETATION:\")\n",
    "    map_score = results.get('map', 0.0)\n",
    "    if map_score >= 0.3:\n",
    "        print(\"   MAP: Excellent performance ✅\")\n",
    "    elif map_score >= 0.2:\n",
    "        print(\"   MAP: Good performance 👍\")\n",
    "    elif map_score >= 0.1:\n",
    "        print(\"   MAP: Fair performance ⚠️\")\n",
    "    else:\n",
    "        print(\"   MAP: Poor performance ❌\")\n",
    "    \n",
    "    mrr_score = results.get('mrr', 0.0)\n",
    "    if mrr_score >= 0.5:\n",
    "        print(\"   MRR: Excellent first relevant result ranking ✅\")\n",
    "    elif mrr_score >= 0.3:\n",
    "        print(\"   MRR: Good first relevant result ranking 👍\")\n",
    "    elif mrr_score >= 0.2:\n",
    "        print(\"   MRR: Fair first relevant result ranking ⚠️\")\n",
    "    else:\n",
    "        print(\"   MRR: Poor first relevant result ranking ❌\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Print results\n",
    "print_detailed_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "if results:\n",
    "    output_file = \"tfidf_evaluation_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"💾 Results saved to {output_file}\")\n",
    "    \n",
    "    # Also save as a readable text file\n",
    "    output_text_file = \"tfidf_evaluation_results.txt\"\n",
    "    with open(output_text_file, 'w') as f:\n",
    "        f.write(\"TF-IDF EVALUATION RESULTS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"Evaluated on {results.get('num_queries_evaluated', 0)} queries\\n\")\n",
    "        f.write(\"\\nCORE METRICS:\\n\")\n",
    "        f.write(f\"Mean Average Precision (MAP): {results.get('map', 0.0):.4f}\\n\")\n",
    "        f.write(f\"Mean Reciprocal Rank (MRR):  {results.get('mrr', 0.0):.4f}\\n\")\n",
    "        f.write(f\"Precision@10:                {results.get('precision_at_10', 0.0):.4f}\\n\")\n",
    "        f.write(f\"Recall@10:                   {results.get('recall_at_10', 0.0):.4f}\\n\")\n",
    "    print(f\"📄 Human-readable results saved to {output_text_file}\")\n",
    "else:\n",
    "    print(\"⚠️ No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated the TF-IDF system using proper IR evaluation metrics:\n",
    "\n",
    "- **MAP (Mean Average Precision)**: Measures the average precision across all queries\n",
    "- **MRR (Mean Reciprocal Rank)**: Measures how well the system ranks the first relevant document\n",
    "- **Precision@10**: Measures precision at rank 10\n",
    "- **Recall@10**: Measures recall at rank 10\n",
    "\n",
    "The evaluation uses the ANTIQUE dataset and processes queries through the TF-IDF service with enhanced query processing (lemmatization, stemming, and spell checking)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

