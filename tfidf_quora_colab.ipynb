{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/custom-search-engine/blob/main/backend/tfidf_quora_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfidf-quora-header"
   },
   "source": [
    "# TF-IDF Training on Quora Dataset\n",
    "\n",
    "This notebook trains TF-IDF models on the Quora dataset for use in the custom search engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn numpy pandas matplotlib seaborn nltk joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading"
   },
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-quora-dataset"
   },
   "outputs": [],
   "source": [
    "# Load Quora dataset\n",
    "# Adjust the path based on where you upload your Quora dataset\n",
    "# Example formats: CSV, JSON, JSONL\n",
    "\n",
    "def load_quora_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load Quora dataset from various formats\n",
    "    Expected columns: id, text (or question/answer)\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.json'):\n",
    "        df = pd.read_json(file_path)\n",
    "    elif file_path.endswith('.jsonl'):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Upload your Quora dataset file to Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "file_name = list(uploaded.keys())[0]\n",
    "quora_df = load_quora_dataset(file_name)\n",
    "\n",
    "print(f\"Dataset shape: {quora_df.shape}\")\n",
    "print(\"\\nColumns:\", quora_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "quora_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-preprocessing"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and cleaning\n",
    "class QuoraTextProcessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove special characters, keep only alphanumeric and spaces\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_process(self, text):\n",
    "        \"\"\"Tokenize and apply lemmatization and stemming\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Filter out short tokens and non-alphanumeric\n",
    "            if len(token) < 2 or not token.isalnum():\n",
    "                continue\n",
    "            \n",
    "            # Remove stopwords\n",
    "            if token in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Apply lemmatization then stemming\n",
    "            lemmatized = self.lemmatizer.lemmatize(token)\n",
    "            stemmed = self.stemmer.stem(lemmatized)\n",
    "            processed_tokens.append(stemmed)\n",
    "        \n",
    "        return processed_tokens\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Complete text processing pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_process(cleaned)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Initialize processor\n",
    "processor = QuoraTextProcessor()\n",
    "\n",
    "# Identify text column(s)\n",
    "# Adjust these based on your dataset structure\n",
    "text_columns = ['text', 'question', 'answer', 'content']\n",
    "available_text_cols = [col for col in text_columns if col in quora_df.columns]\n",
    "\n",
    "if not available_text_cols:\n",
    "    print(\"Available columns:\", quora_df.columns.tolist())\n",
    "    text_col = input(\"Enter the name of the text column: \")\n",
    "else:\n",
    "    text_col = available_text_cols[0]\n",
    "    \n",
    "print(f\"Using text column: {text_col}\")\n",
    "\n",
    "# Process a subset for faster experimentation\n",
    "sample_size = min(10000, len(quora_df))  # Adjust as needed\n",
    "quora_sample = quora_df.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "print(f\"Processing {len(quora_sample)} documents...\")\n",
    "quora_sample['processed_text'] = quora_sample[text_col].apply(\n",
    "    lambda x: processor.process_text(x)\n",
    ")\n",
    "\n",
    "# Filter out empty processed texts\n",
    "quora_sample = quora_sample[quora_sample['processed_text'].str.len() > 0]\n",
    "print(f\"After filtering: {len(quora_sample)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfidf-training"
   },
   "source": [
    "## TF-IDF Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-tfidf-model"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer configuration\n",
    "tfidf_config = {\n",
    "    'max_features': 10000,      # Limit vocabulary size\n",
    "    'ngram_range': (1, 2),      # Use unigrams and bigrams\n",
    "    'min_df': 2,                # Ignore terms in less than 2 documents\n",
    "    'max_df': 0.8,              # Ignore terms in more than 80% of documents\n",
    "    'sublinear_tf': True,       # Apply sublinear TF scaling\n",
    "    'use_idf': True,            # Enable IDF\n",
    "    'smooth_idf': True,         # Smooth IDF weights\n",
    "    'norm': 'l2'                # L2 normalization\n",
    "}\n",
    "\n",
    "print(\"Training TF-IDF vectorizer...\")\n",
    "print(f\"Configuration: {tfidf_config}\")\n",
    "\n",
    "# Create and train vectorizer\n",
    "vectorizer = TfidfVectorizer(**tfidf_config)\n",
    "\n",
    "# Fit and transform the documents\n",
    "start_time = time.time()\n",
    "tfidf_matrix = vectorizer.fit_transform(quora_sample['processed_text'])\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Matrix density: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze-vocabulary"
   },
   "outputs": [],
   "source": [
    "# Analyze vocabulary and feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Create vocabulary analysis\n",
    "vocab_df = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'idf': idf_scores\n",
    "})\n",
    "\n",
    "# Sort by IDF score\n",
    "vocab_df = vocab_df.sort_values('idf', ascending=False)\n",
    "\n",
    "print(\"Top 20 terms by IDF score (most discriminative):\")\n",
    "print(vocab_df.head(20))\n",
    "\n",
    "print(\"\\nBottom 20 terms by IDF score (most common):\")\n",
    "print(vocab_df.tail(20))\n",
    "\n",
    "# Plot IDF distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(idf_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('IDF Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of IDF Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(idf_scores)\n",
    "plt.ylabel('IDF Score')\n",
    "plt.title('IDF Score Distribution (Box Plot)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIDF Statistics:\")\n",
    "print(f\"Mean: {np.mean(idf_scores):.3f}\")\n",
    "print(f\"Median: {np.median(idf_scores):.3f}\")\n",
    "print(f\"Std: {np.std(idf_scores):.3f}\")\n",
    "print(f\"Min: {np.min(idf_scores):.3f}\")\n",
    "print(f\"Max: {np.max(idf_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-evaluation"
   },
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-search-function"
   },
   "outputs": [],
   "source": [
    "# Create search function for testing\n",
    "def search_documents(query, top_k=10, return_scores=True):\n",
    "    \"\"\"\n",
    "    Search documents using the trained TF-IDF model\n",
    "    \"\"\"\n",
    "    # Process the query\n",
    "    processed_query = processor.process_text(query)\n",
    "    \n",
    "    # Transform query to TF-IDF vector\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if similarities[idx] > 0:  # Only include results with positive similarity\n",
    "            doc_data = quora_sample.iloc[idx]\n",
    "            result = {\n",
    "                'index': idx,\n",
    "                'score': similarities[idx],\n",
    "                'original_text': doc_data[text_col],\n",
    "                'processed_text': doc_data['processed_text']\n",
    "            }\n",
    "            \n",
    "            # Add any additional metadata\n",
    "            for col in quora_sample.columns:\n",
    "                if col not in ['processed_text', text_col]:\n",
    "                    result[col] = doc_data[col]\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the search function with sample queries\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"python programming\",\n",
    "    \"data science\",\n",
    "    \"artificial intelligence\",\n",
    "    \"web development\"\n",
    "]\n",
    "\n",
    "print(\"Testing search functionality with sample queries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = search_documents(query, top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. Score: {result['score']:.4f}\")\n",
    "            print(f\"   Text: {result['original_text'][:100]}...\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"   No relevant results found.\")\n",
    "    \n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate-model-statistics"
   },
   "outputs": [],
   "source": [
    "# Analyze model performance and statistics\n",
    "print(\"Model Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Document length statistics\n",
    "doc_lengths = [len(text.split()) for text in quora_sample['processed_text']]\n",
    "print(f\"Document lengths (words):\")\n",
    "print(f\"  Mean: {np.mean(doc_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(doc_lengths):.1f}\")\n",
    "print(f\"  Std: {np.std(doc_lengths):.1f}\")\n",
    "print(f\"  Min: {np.min(doc_lengths)}\")\n",
    "print(f\"  Max: {np.max(doc_lengths)}\")\n",
    "\n",
    "# TF-IDF matrix statistics\n",
    "print(f\"\\nTF-IDF Matrix Statistics:\")\n",
    "print(f\"  Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  Non-zero elements: {tfidf_matrix.nnz:,}\")\n",
    "print(f\"  Sparsity: {1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.4f}\")\n",
    "print(f\"  Memory usage: {tfidf_matrix.data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Vocabulary statistics\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"  Total terms: {len(vectorizer.vocabulary_):,}\")\n",
    "print(f\"  Unigrams: {sum(1 for term in feature_names if ' ' not in term):,}\")\n",
    "print(f\"  Bigrams: {sum(1 for term in feature_names if ' ' in term):,}\")\n",
    "\n",
    "# Plot document length distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(doc_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Document Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.boxplot(doc_lengths)\n",
    "plt.ylabel('Document Length (words)')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# TF-IDF score distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "sample_scores = tfidf_matrix.data[:10000]  # Sample for performance\n",
    "plt.hist(sample_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of TF-IDF Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature frequency\n",
    "plt.subplot(2, 2, 4)\n",
    "feature_doc_counts = np.array((tfidf_matrix > 0).sum(axis=0)).flatten()\n",
    "plt.hist(feature_doc_counts, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.title('Feature Document Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-saving"
   },
   "source": [
    "## Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-models"
   },
   "outputs": [],
   "source": [
    "# Prepare document metadata for saving\n",
    "document_metadata = []\n",
    "for idx, row in quora_sample.iterrows():\n",
    "    metadata = {\n",
    "        'doc_id': str(idx),\n",
    "        'raw_text': row[text_col],\n",
    "        'processed_text': row['processed_text'],\n",
    "        'length': len(row['processed_text'].split())\n",
    "    }\n",
    "    \n",
    "    # Add any additional metadata columns\n",
    "    for col in quora_sample.columns:\n",
    "        if col not in [text_col, 'processed_text']:\n",
    "            metadata[col] = row[col]\n",
    "    \n",
    "    document_metadata.append(metadata)\n",
    "\n",
    "print(f\"Prepared metadata for {len(document_metadata)} documents\")\n",
    "\n",
    "# Save all model components\n",
    "model_files = {\n",
    "    'quora_tfidf_vectorizer.joblib': vectorizer,\n",
    "    'quora_tfidf_matrix.joblib': tfidf_matrix,\n",
    "    'quora_document_metadata.joblib': document_metadata\n",
    "}\n",
    "\n",
    "print(\"\\nSaving model files...\")\n",
    "for filename, data in model_files.items():\n",
    "    try:\n",
    "        joblib.dump(data, filename)\n",
    "        file_size = os.path.getsize(filename) / 1024 / 1024  # MB\n",
    "        print(f\"‚úÖ Saved {filename} ({file_size:.2f} MB)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving {filename}: {e}\")\n",
    "\n",
    "# Create a summary report\n",
    "summary_report = {\n",
    "    'dataset_info': {\n",
    "        'total_documents': len(quora_sample),\n",
    "        'text_column': text_col,\n",
    "        'avg_doc_length': np.mean(doc_lengths),\n",
    "        'processing_time': training_time\n",
    "    },\n",
    "    'model_config': tfidf_config,\n",
    "    'model_stats': {\n",
    "        'vocabulary_size': len(vectorizer.vocabulary_),\n",
    "        'matrix_shape': tfidf_matrix.shape,\n",
    "        'sparsity': 1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])),\n",
    "        'memory_usage_mb': tfidf_matrix.data.nbytes / 1024 / 1024\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'test_queries': test_queries,\n",
    "        'avg_results_per_query': np.mean([len(search_documents(q, top_k=10)) for q in test_queries])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary report\n",
    "with open('quora_tfidf_training_report.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Summary report saved.\")\n",
    "print(\"\\nFiles created:\")\n",
    "for filename in model_files.keys():\n",
    "    print(f\"  - {filename}\")\n",
    "print(\"  - quora_tfidf_training_report.json\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Documents processed: {len(quora_sample):,}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Matrix sparsity: {1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.4f}\")\n",
    "print(f\"Memory usage: {tfidf_matrix.data.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-models"
   },
   "source": [
    "## Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "# Download all model files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading trained models...\")\n",
    "\n",
    "# Download model files\n",
    "for filename in model_files.keys():\n",
    "    try:\n",
    "        files.download(filename)\n",
    "        print(f\"‚úÖ Downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {filename}: {e}\")\n",
    "\n",
    "# Download summary report\n",
    "try:\n",
    "    files.download('quora_tfidf_training_report.json')\n",
    "    print(\"‚úÖ Downloaded training report\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading report: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Upload the .joblib files to your backend server\")\n",
    "print(\"2. Update the TF-IDF Quora service configuration\")\n",
    "print(\"3. Start the Quora TF-IDF service\")\n",
    "print(\"4. Test the search functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing-final"
   },
   "source": [
    "## Final Testing and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-test"
   },
   "outputs": [],
   "source": [
    "# Load saved models to verify they work correctly\n",
    "print(\"Verifying saved models...\")\n",
    "\n",
    "try:\n",
    "    # Load saved components\n",
    "    loaded_vectorizer = joblib.load('quora_tfidf_vectorizer.joblib')\n",
    "    loaded_matrix = joblib.load('quora_tfidf_matrix.joblib')\n",
    "    loaded_metadata = joblib.load('quora_document_metadata.joblib')\n",
    "    \n",
    "    print(\"‚úÖ All models loaded successfully\")\n",
    "    \n",
    "    # Test with loaded models\n",
    "    test_query = \"machine learning\"\n",
    "    processed_query = processor.process_text(test_query)\n",
    "    query_vector = loaded_vectorizer.transform([processed_query])\n",
    "    similarities = cosine_similarity(query_vector, loaded_matrix).flatten()\n",
    "    top_idx = np.argmax(similarities)\n",
    "    \n",
    "    print(f\"\\nTest query: '{test_query}'\")\n",
    "    print(f\"Top result score: {similarities[top_idx]:.4f}\")\n",
    "    print(f\"Top result text: {loaded_metadata[top_idx]['raw_text'][:100]}...\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model verification successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error verifying models: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TF-IDF QUORA MODEL TRAINING COMPLETED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYour models are ready for deployment in the search engine.\")\n",
    "print(\"Remember to update the service configuration with the correct file paths.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
