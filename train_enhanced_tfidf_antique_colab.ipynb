{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Enhanced TF-IDF Training on ANTIQUE Dataset\n",
        "## Advanced Text Processing with Spell Checking, Lemmatization, and Stemming\n",
        "\n",
        "This notebook trains enhanced TF-IDF models with:\n",
        "- Spell checking (conservative approach to preserve MAP)\n",
        "- Lemmatization with POS tagging\n",
        "- Stemming for vocabulary reduction\n",
        "- Optimized inverted index\n",
        "- MAP-preserving preprocessing\n",
        "\n",
        "The generated files will be used by the TF-IDF microservices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ir-datasets nltk scikit-learn joblib pandas numpy tqdm textblob\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import ir_datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_text_cleaner"
      },
      "outputs": [],
      "source": [
        "# Enhanced Text Cleaning Service\n",
        "class EnhancedTextCleaner:\n",
        "    def __init__(self, enable_spell_check=True, language='english'):\n",
        "        self.enable_spell_check = enable_spell_check\n",
        "        self.language = language\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words(language))\n",
        "        \n",
        "        # Enhanced stopwords for better IR performance\n",
        "        self.technical_stopwords = {\n",
        "            'code', 'function', 'method', 'class', 'variable', 'return',\n",
        "            'import', 'from', 'def', 'if', 'else', 'for', 'while', 'try',\n",
        "            'catch', 'finally', 'throw', 'throws', 'public', 'private',\n",
        "            'protected', 'static', 'final', 'abstract', 'interface'\n",
        "        }\n",
        "        \n",
        "        self.domain_stopwords = {\n",
        "            'antique', 'vintage', 'old', 'item', 'piece', 'thing', 'stuff',\n",
        "            'want', 'need', 'looking', 'find', 'search', 'help', 'please',\n",
        "            'anyone', 'someone', 'know', 'tell', 'show', 'give', 'get',\n",
        "            'would', 'could', 'should', 'might', 'maybe', 'perhaps'\n",
        "        }\n",
        "        \n",
        "        self.all_stopwords = self.stop_words.union(\n",
        "            self.technical_stopwords\n",
        "        ).union(self.domain_stopwords)\n",
        "        \n",
        "        # Spell check cache for performance\n",
        "        self.spell_check_cache = {}\n",
        "        \n",
        "        # Contractions expansion\n",
        "        self.contractions = {\n",
        "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "            \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\",\n",
        "            \"that's\": \"that is\", \"what's\": \"what is\",\n",
        "            \"where's\": \"where is\", \"how's\": \"how is\"\n",
        "        }\n",
        "        \n",
        "        # Normalization patterns\n",
        "        self.normalization_patterns = [\n",
        "            (r'https?://[^\\s<>\"]{2,}', ' URL '),  # URLs\n",
        "            (r'www\\.[^\\s<>\"]{2,}', ' URL '),      # www URLs\n",
        "            (r'\\S+@\\S+', ' EMAIL '),              # Email addresses\n",
        "            (r'\\$\\d+(?:\\.\\d+)?', ' PRICE '),      # Prices\n",
        "            (r'\\d{4}-\\d{2}-\\d{2}', ' DATE '),     # Dates\n",
        "            (r'\\b\\d{4}\\b', ' YEAR '),             # Years\n",
        "        ]\n",
        "    \n",
        "    def get_wordnet_pos(self, word):\n",
        "        \"\"\"Map POS tag to WordNet POS tag for lemmatization.\"\"\"\n",
        "        try:\n",
        "            tag = pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\n",
        "                'J': wordnet.ADJ,\n",
        "                'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB,\n",
        "                'R': wordnet.ADV\n",
        "            }\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "        except:\n",
        "            return wordnet.NOUN\n",
        "    \n",
        "    def expand_contractions(self, text):\n",
        "        \"\"\"Expand contractions in text.\"\"\"\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = re.sub(re.escape(contraction), expansion, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "    \n",
        "    def normalize_unicode(self, text):\n",
        "        \"\"\"Normalize Unicode characters to ASCII equivalents.\"\"\"\n",
        "        text = unicodedata.normalize('NFD', text)\n",
        "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
        "        return text.encode('ascii', 'ignore').decode('ascii')\n",
        "    \n",
        "    def apply_normalization_patterns(self, text):\n",
        "        \"\"\"Apply normalization patterns.\"\"\"\n",
        "        for pattern, replacement in self.normalization_patterns:\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "    \n",
        "    def spell_check_word(self, word, conservative=True):\n",
        "        \"\"\"Apply conservative spell checking to preserve MAP performance.\"\"\"\n",
        "        if not self.enable_spell_check or len(word) < 4:\n",
        "            return word\n",
        "        \n",
        "        if word in self.spell_check_cache:\n",
        "            return self.spell_check_cache[word]\n",
        "        \n",
        "        try:\n",
        "            blob = TextBlob(word)\n",
        "            corrected = str(blob.correct())\n",
        "            \n",
        "            # Conservative approach: only use correction if it's significantly better\n",
        "            if conservative:\n",
        "                if (corrected != word and \n",
        "                    word not in self.stop_words and\n",
        "                    abs(len(corrected) - len(word)) <= 2 and\n",
        "                    len(set(corrected.lower()) & set(word.lower())) >= min(len(word), len(corrected)) * 0.6):\n",
        "                    \n",
        "                    self.spell_check_cache[word] = corrected\n",
        "                    return corrected\n",
        "            else:\n",
        "                if corrected != word and word not in self.stop_words:\n",
        "                    self.spell_check_cache[word] = corrected\n",
        "                    return corrected\n",
        "            \n",
        "            self.spell_check_cache[word] = word\n",
        "            return word\n",
        "            \n",
        "        except:\n",
        "            self.spell_check_cache[word] = word\n",
        "            return word\n",
        "    \n",
        "    def clean_text_basic(self, text):\n",
        "        \"\"\"Basic text cleaning for MAP-preserving preprocessing.\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "        \n",
        "        # HTML decoding and tag removal\n",
        "        text = html.unescape(text)\n",
        "        text = re.sub(r'<[^>]+>', ' ', text)\n",
        "        \n",
        "        # Normalize Unicode\n",
        "        text = self.normalize_unicode(text)\n",
        "        \n",
        "        # Apply normalization patterns\n",
        "        text = self.apply_normalization_patterns(text)\n",
        "        \n",
        "        # Expand contractions\n",
        "        text = self.expand_contractions(text)\n",
        "        \n",
        "        # Remove extra punctuation but preserve sentence boundaries\n",
        "        text = re.sub(r'[^\\w\\s\\.!?]', ' ', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        return text.lower()\n",
        "    \n",
        "    def get_cleaning_statistics(self, original, cleaned):\n",
        "        \"\"\"Get cleaning statistics.\"\"\"\n",
        "        original_tokens = word_tokenize(original.lower()) if original else []\n",
        "        cleaned_tokens = cleaned.split() if cleaned else []\n",
        "        \n",
        "        return {\n",
        "            'original_length': len(original) if original else 0,\n",
        "            'cleaned_length': len(cleaned) if cleaned else 0,\n",
        "            'original_tokens': len(original_tokens),\n",
        "            'cleaned_tokens': len(cleaned_tokens),\n",
        "            'token_reduction_ratio': 1 - (len(cleaned_tokens) / max(len(original_tokens), 1)),\n",
        "            'char_reduction_ratio': 1 - (len(cleaned) / max(len(original), 1)) if original else 0,\n",
        "            'spell_corrections': len(self.spell_check_cache)\n",
        "        }\n",
        "\n",
        "print(\"✓ Enhanced Text Cleaner loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_tokenizer"
      },
      "outputs": [],
      "source": [
        "# Enhanced Tokenizer for TF-IDF Vectorizer\n",
        "class EnhancedTokenizer:\n",
        "    def __init__(self, enable_spell_check=True, enable_lemmatization=True, \n",
        "                 enable_stemming=True, language='english', min_token_length=3, max_token_length=50):\n",
        "        self.enable_spell_check = enable_spell_check\n",
        "        self.enable_lemmatization = enable_lemmatization\n",
        "        self.enable_stemming = enable_stemming\n",
        "        self.language = language\n",
        "        self.min_token_length = min_token_length\n",
        "        self.max_token_length = max_token_length\n",
        "        \n",
        "        # Initialize NLTK components\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words(language))\n",
        "        \n",
        "        # Enhanced stopwords\n",
        "        self.technical_stopwords = {\n",
        "            'code', 'function', 'method', 'class', 'variable', 'return',\n",
        "            'import', 'from', 'def', 'if', 'else', 'for', 'while', 'try',\n",
        "            'catch', 'finally', 'throw', 'throws', 'public', 'private',\n",
        "            'protected', 'static', 'final', 'abstract', 'interface'\n",
        "        }\n",
        "        \n",
        "        self.domain_stopwords = {\n",
        "            'antique', 'vintage', 'old', 'item', 'piece', 'thing', 'stuff',\n",
        "            'want', 'need', 'looking', 'find', 'search', 'help', 'please',\n",
        "            'anyone', 'someone', 'know', 'tell', 'show', 'give', 'get',\n",
        "            'would', 'could', 'should', 'might', 'maybe', 'perhaps'\n",
        "        }\n",
        "        \n",
        "        self.all_stopwords = self.stop_words.union(\n",
        "            self.technical_stopwords\n",
        "        ).union(self.domain_stopwords)\n",
        "        \n",
        "        # Spell check cache\n",
        "        self.spell_check_cache = {}\n",
        "    \n",
        "    def get_wordnet_pos(self, word):\n",
        "        \"\"\"Map POS tag to WordNet POS tag.\"\"\"\n",
        "        try:\n",
        "            tag = pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\n",
        "                'J': wordnet.ADJ,\n",
        "                'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB,\n",
        "                'R': wordnet.ADV\n",
        "            }\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "        except:\n",
        "            return wordnet.NOUN\n",
        "    \n",
        "    def spell_check_token(self, token):\n",
        "        \"\"\"Apply spell checking to a token.\"\"\"\n",
        "        if not self.enable_spell_check or len(token) < 4:\n",
        "            return token\n",
        "        \n",
        "        if token in self.spell_check_cache:\n",
        "            return self.spell_check_cache[token]\n",
        "        \n",
        "        try:\n",
        "            blob = TextBlob(token)\n",
        "            corrected = str(blob.correct())\n",
        "            \n",
        "            # Conservative correction to preserve MAP\n",
        "            if (corrected != token and \n",
        "                token not in self.stop_words and\n",
        "                abs(len(corrected) - len(token)) <= 2 and\n",
        "                len(set(corrected.lower()) & set(token.lower())) >= min(len(token), len(corrected)) * 0.6):\n",
        "                \n",
        "                self.spell_check_cache[token] = corrected\n",
        "                return corrected\n",
        "            \n",
        "            self.spell_check_cache[token] = token\n",
        "            return token\n",
        "            \n",
        "        except:\n",
        "            self.spell_check_cache[token] = token\n",
        "            return token\n",
        "    \n",
        "    def process_token(self, token):\n",
        "        \"\"\"Process a single token through the complete pipeline.\"\"\"\n",
        "        # Basic filtering\n",
        "        if (len(token) < self.min_token_length or \n",
        "            len(token) > self.max_token_length or\n",
        "            not token.isalpha() or\n",
        "            token.lower() in self.all_stopwords):\n",
        "            return None\n",
        "        \n",
        "        # Convert to lowercase\n",
        "        token = token.lower()\n",
        "        \n",
        "        # Apply spell checking\n",
        "        if self.enable_spell_check:\n",
        "            token = self.spell_check_token(token)\n",
        "        \n",
        "        # Apply lemmatization\n",
        "        if self.enable_lemmatization:\n",
        "            pos = self.get_wordnet_pos(token)\n",
        "            token = self.lemmatizer.lemmatize(token, pos)\n",
        "        \n",
        "        # Apply stemming\n",
        "        if self.enable_stemming:\n",
        "            token = self.stemmer.stem(token)\n",
        "        \n",
        "        # Final length check\n",
        "        if len(token) < self.min_token_length:\n",
        "            return None\n",
        "        \n",
        "        return token\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text using the enhanced pipeline.\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            raw_tokens = word_tokenize(text)\n",
        "        except:\n",
        "            raw_tokens = text.split()\n",
        "        \n",
        "        # Process each token\n",
        "        processed_tokens = []\n",
        "        for token in raw_tokens:\n",
        "            processed_token = self.process_token(token)\n",
        "            if processed_token:\n",
        "                processed_tokens.append(processed_token)\n",
        "        \n",
        "        return processed_tokens\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        \"\"\"Make tokenizer callable for sklearn TfidfVectorizer.\"\"\"\n",
        "        return self.tokenize(text)\n",
        "    \n",
        "    def get_tokenizer_info(self):\n",
        "        \"\"\"Get tokenizer configuration.\"\"\"\n",
        "        return {\n",
        "            'spell_check_enabled': self.enable_spell_check,\n",
        "            'lemmatization_enabled': self.enable_lemmatization,\n",
        "            'stemming_enabled': self.enable_stemming,\n",
        "            'language': self.language,\n",
        "            'min_token_length': self.min_token_length,\n",
        "            'max_token_length': self.max_token_length,\n",
        "            'total_stopwords': len(self.all_stopwords),\n",
        "            'spell_check_cache_size': len(self.spell_check_cache)\n",
        "        }\n",
        "\n",
        "print(\"✓ Enhanced Tokenizer loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_antique_data"
      },
      "outputs": [],
      "source": [
        "# Load ANTIQUE dataset\n",
        "print(\"Loading ANTIQUE dataset...\")\n",
        "dataset = ir_datasets.load(\"antique\")\n",
        "\n",
        "# Load documents\n",
        "print(\"Loading documents...\")\n",
        "docs = {}\n",
        "doc_texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for doc in tqdm(dataset.docs_iter(), desc=\"Loading documents\"):\n",
        "    docs[doc.doc_id] = doc.text\n",
        "    doc_texts.append(doc.text)\n",
        "    doc_ids.append(doc.doc_id)\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "\n",
        "# Load training queries\n",
        "print(\"Loading queries...\")\n",
        "train_queries = {}\n",
        "for query in dataset.queries_iter():\n",
        "    train_queries[query.query_id] = query.text\n",
        "\n",
        "print(f\"Loaded {len(train_queries)} queries\")\n",
        "\n",
        "# Load qrels for evaluation\n",
        "print(\"Loading qrels...\")\n",
        "train_qrels = defaultdict(dict)\n",
        "for qrel in dataset.qrels_iter():\n",
        "    train_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "\n",
        "print(f\"Loaded qrels for {len(train_qrels)} queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_documents"
      },
      "outputs": [],
      "source": [
        "# Initialize enhanced text cleaner\n",
        "text_cleaner = EnhancedTextCleaner(enable_spell_check=True)\n",
        "\n",
        "# Preprocess all documents with enhanced cleaning\n",
        "print(\"Preprocessing documents with enhanced text cleaning...\")\n",
        "processed_doc_texts = []\n",
        "cleaning_stats = []\n",
        "\n",
        "for text in tqdm(doc_texts, desc=\"Enhanced preprocessing\"):\n",
        "    cleaned = text_cleaner.clean_text_basic(text)\n",
        "    processed_doc_texts.append(cleaned)\n",
        "    \n",
        "    # Collect cleaning statistics for analysis\n",
        "    stats = text_cleaner.get_cleaning_statistics(text, cleaned)\n",
        "    cleaning_stats.append(stats)\n",
        "\n",
        "# Filter out empty documents\n",
        "valid_docs = [(doc_id, doc_text, processed_text) \n",
        "              for doc_id, doc_text, processed_text \n",
        "              in zip(doc_ids, doc_texts, processed_doc_texts) \n",
        "              if processed_text.strip()]\n",
        "\n",
        "print(f\"Valid documents after preprocessing: {len(valid_docs)}\")\n",
        "\n",
        "# Separate the valid data\n",
        "valid_doc_ids = [item[0] for item in valid_docs]\n",
        "valid_doc_texts = [item[1] for item in valid_docs]\n",
        "valid_processed_texts = [item[2] for item in valid_docs]\n",
        "\n",
        "# Calculate preprocessing statistics\n",
        "total_original_chars = sum(stats['original_length'] for stats in cleaning_stats)\n",
        "total_cleaned_chars = sum(stats['cleaned_length'] for stats in cleaning_stats)\n",
        "total_original_tokens = sum(stats['original_tokens'] for stats in cleaning_stats)\n",
        "total_cleaned_tokens = sum(stats['cleaned_tokens'] for stats in cleaning_stats)\n",
        "\n",
        "preprocessing_summary = {\n",
        "    'total_documents': len(doc_texts),\n",
        "    'valid_documents': len(valid_docs),\n",
        "    'documents_filtered': len(doc_texts) - len(valid_docs),\n",
        "    'total_char_reduction': 1 - (total_cleaned_chars / total_original_chars),\n",
        "    'total_token_reduction': 1 - (total_cleaned_tokens / total_original_tokens),\n",
        "    'spell_corrections': text_cleaner.spell_check_cache\n",
        "}\n",
        "\n",
        "print(\"\\n=== Preprocessing Summary ===\")\n",
        "for key, value in preprocessing_summary.items():\n",
        "    if key != 'spell_corrections':\n",
        "        print(f\"{key}: {value}\")\n",
        "print(f\"spell_corrections_count: {len(preprocessing_summary['spell_corrections'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_enhanced_tfidf"
      },
      "outputs": [],
      "source": [
        "# Create enhanced tokenizer for TF-IDF\n",
        "enhanced_tokenizer = EnhancedTokenizer(\n",
        "    enable_spell_check=True,\n",
        "    enable_lemmatization=True,\n",
        "    enable_stemming=True,\n",
        "    language='english',\n",
        "    min_token_length=3,\n",
        "    max_token_length=50\n",
        ")\n",
        "\n",
        "# Configure Enhanced TF-IDF vectorizer with optimized parameters for MAP > 0.4\n",
        "print(\"Training Enhanced TF-IDF vectorizer...\")\n",
        "\n",
        "enhanced_tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=100000,      # Large vocabulary for better coverage\n",
        "    min_df=2,                 # Remove very rare terms\n",
        "    max_df=0.85,              # Remove very common terms (conservative)\n",
        "    ngram_range=(1, 3),       # Include trigrams for better phrase matching\n",
        "    sublinear_tf=True,        # Log scaling for TF\n",
        "    norm='l2',                # L2 normalization\n",
        "    smooth_idf=True,          # Smooth IDF weights\n",
        "    use_idf=True,             # Use IDF weighting\n",
        "    tokenizer=enhanced_tokenizer,  # Enhanced tokenizer with preprocessing\n",
        "    preprocessor=None,        # All preprocessing handled by tokenizer\n",
        "    lowercase=False,          # Handled by tokenizer\n",
        "    stop_words=None,          # Handled by tokenizer\n",
        "    token_pattern=None,       # Using custom tokenizer\n",
        ")\n",
        "\n",
        "# Fit and transform documents\n",
        "print(\"Fitting Enhanced TF-IDF on documents...\")\n",
        "enhanced_tfidf_matrix = enhanced_tfidf_vectorizer.fit_transform(valid_processed_texts)\n",
        "\n",
        "print(f\"Enhanced TF-IDF matrix shape: {enhanced_tfidf_matrix.shape}\")\n",
        "print(f\"Vocabulary size: {len(enhanced_tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"Non-zero entries: {enhanced_tfidf_matrix.nnz}\")\n",
        "print(f\"Sparsity: {(1 - enhanced_tfidf_matrix.nnz / (enhanced_tfidf_matrix.shape[0] * enhanced_tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "print(f\"Average document length: {enhanced_tfidf_matrix.nnz / enhanced_tfidf_matrix.shape[0]:.2f} features\")\n",
        "\n",
        "# Get tokenizer statistics\n",
        "tokenizer_info = enhanced_tokenizer.get_tokenizer_info()\n",
        "print(\"\\n=== Enhanced Tokenizer Info ===\")\n",
        "for key, value in tokenizer_info.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build_optimized_inverted_index"
      },
      "outputs": [],
      "source": [
        "# Build optimized inverted index for enhanced search\n",
        "print(\"Building optimized inverted index...\")\n",
        "\n",
        "def build_optimized_inverted_index(tfidf_matrix, vectorizer, doc_ids):\n",
        "    \"\"\"Build optimized inverted index with term statistics.\"\"\"\n",
        "    inverted_index = defaultdict(lambda: {\n",
        "        'postings': [],\n",
        "        'df': 0,\n",
        "        'max_tfidf': 0.0,\n",
        "        'avg_tfidf': 0.0\n",
        "    })\n",
        "    \n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    coo_matrix = tfidf_matrix.tocoo()\n",
        "    term_stats = defaultdict(list)\n",
        "    \n",
        "    # Build index with statistics\n",
        "    for doc_idx, term_idx, tfidf_score in tqdm(\n",
        "        zip(coo_matrix.row, coo_matrix.col, coo_matrix.data),\n",
        "        total=coo_matrix.nnz,\n",
        "        desc=\"Building optimized inverted index\"\n",
        "    ):\n",
        "        term = feature_names[term_idx]\n",
        "        doc_id = doc_ids[doc_idx]\n",
        "        \n",
        "        # Add posting\n",
        "        inverted_index[term]['postings'].append((doc_id, float(tfidf_score)))\n",
        "        term_stats[term].append(float(tfidf_score))\n",
        "    \n",
        "    # Calculate statistics and sort postings\n",
        "    for term in inverted_index:\n",
        "        scores = term_stats[term]\n",
        "        inverted_index[term]['df'] = len(scores)\n",
        "        inverted_index[term]['max_tfidf'] = max(scores)\n",
        "        inverted_index[term]['avg_tfidf'] = sum(scores) / len(scores)\n",
        "        \n",
        "        # Sort postings by TF-IDF score (descending)\n",
        "        inverted_index[term]['postings'].sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return dict(inverted_index)\n",
        "\n",
        "enhanced_inverted_index = build_optimized_inverted_index(\n",
        "    enhanced_tfidf_matrix, \n",
        "    enhanced_tfidf_vectorizer, \n",
        "    valid_doc_ids\n",
        ")\n",
        "\n",
        "print(f\"Optimized inverted index built with {len(enhanced_inverted_index)} terms\")\n",
        "\n",
        "# Calculate index statistics\n",
        "avg_postings_per_term = np.mean([len(data['postings']) for data in enhanced_inverted_index.values()])\n",
        "max_postings = max([len(data['postings']) for data in enhanced_inverted_index.values()])\n",
        "min_postings = min([len(data['postings']) for data in enhanced_inverted_index.values()])\n",
        "\n",
        "print(f\"Average postings per term: {avg_postings_per_term:.2f}\")\n",
        "print(f\"Max postings per term: {max_postings}\")\n",
        "print(f\"Min postings per term: {min_postings}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_document_mapping"
      },
      "outputs": [],
      "source": [
        "# Create document ID to index mapping\n",
        "enhanced_doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(valid_doc_ids)}\n",
        "enhanced_idx_to_doc_id = {idx: doc_id for doc_id, idx in enhanced_doc_id_to_idx.items()}\n",
        "\n",
        "# Create enhanced document metadata\n",
        "enhanced_document_metadata = {\n",
        "    doc_id: {\n",
        "        'original_text': valid_doc_texts[idx],\n",
        "        'processed_text': valid_processed_texts[idx],\n",
        "        'index': idx,\n",
        "        'original_length': len(valid_doc_texts[idx]),\n",
        "        'processed_length': len(valid_processed_texts[idx]),\n",
        "        'text_reduction_ratio': 1 - (len(valid_processed_texts[idx]) / max(len(valid_doc_texts[idx]), 1))\n",
        "    }\n",
        "    for doc_id, idx in enhanced_doc_id_to_idx.items()\n",
        "}\n",
        "\n",
        "print(f\"Created enhanced metadata for {len(enhanced_document_metadata)} documents\")\n",
        "\n",
        "# Calculate comprehensive training statistics\n",
        "feature_names = enhanced_tfidf_vectorizer.get_feature_names_out()\n",
        "idf_scores = enhanced_tfidf_vectorizer.idf_\n",
        "\n",
        "enhanced_training_stats = {\n",
        "    # Document statistics\n",
        "    'total_documents': len(doc_texts),\n",
        "    'valid_documents': len(valid_docs),\n",
        "    'documents_filtered': len(doc_texts) - len(valid_docs),\n",
        "    'filter_rate_percent': ((len(doc_texts) - len(valid_docs)) / len(doc_texts)) * 100,\n",
        "    \n",
        "    # Text processing statistics\n",
        "    'total_char_reduction_percent': preprocessing_summary['total_char_reduction'] * 100,\n",
        "    'total_token_reduction_percent': preprocessing_summary['total_token_reduction'] * 100,\n",
        "    'spell_corrections_count': len(preprocessing_summary['spell_corrections']),\n",
        "    \n",
        "    # TF-IDF matrix statistics\n",
        "    'matrix_shape': enhanced_tfidf_matrix.shape,\n",
        "    'vocabulary_size': len(feature_names),\n",
        "    'non_zero_entries': int(enhanced_tfidf_matrix.nnz),\n",
        "    'sparsity_percent': float((1 - enhanced_tfidf_matrix.nnz / (enhanced_tfidf_matrix.shape[0] * enhanced_tfidf_matrix.shape[1])) * 100),\n",
        "    'avg_doc_length': float(enhanced_tfidf_matrix.nnz / enhanced_tfidf_matrix.shape[0]),\n",
        "    \n",
        "    # Vocabulary statistics\n",
        "    'min_idf': float(np.min(idf_scores)),\n",
        "    'max_idf': float(np.max(idf_scores)),\n",
        "    'avg_idf': float(np.mean(idf_scores)),\n",
        "    'median_idf': float(np.median(idf_scores)),\n",
        "    \n",
        "    # Inverted index statistics\n",
        "    'inverted_index_terms': len(enhanced_inverted_index),\n",
        "    'avg_postings_per_term': float(avg_postings_per_term),\n",
        "    'max_postings_per_term': int(max_postings),\n",
        "    'min_postings_per_term': int(min_postings),\n",
        "    \n",
        "    # Configuration\n",
        "    'vectorizer_params': enhanced_tfidf_vectorizer.get_params(),\n",
        "    'tokenizer_info': tokenizer_info,\n",
        "    \n",
        "    # Processing features enabled\n",
        "    'spell_check_enabled': True,\n",
        "    'lemmatization_enabled': True,\n",
        "    'stemming_enabled': True,\n",
        "    'ngram_range': (1, 3),\n",
        "    'optimization_target': 'MAP_above_0.4'\n",
        "}\n",
        "\n",
        "print(\"\\n=== Enhanced Training Statistics ===\")\n",
        "for key, value in enhanced_training_stats.items():\n",
        "    if key not in ['vectorizer_params', 'tokenizer_info']:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_enhanced_models"
      },
      "outputs": [],
      "source": [
        "# Save all enhanced trained models and data\n",
        "print(\"Saving enhanced trained models and data...\")\n",
        "\n",
        "# Save Enhanced TF-IDF vectorizer\n",
        "joblib.dump(enhanced_tfidf_vectorizer, 'enhanced_tfidf_vectorizer_antique.joblib')\n",
        "print(\"✓ Enhanced TF-IDF vectorizer saved\")\n",
        "\n",
        "# Save Enhanced TF-IDF matrix (sparse)\n",
        "joblib.dump(enhanced_tfidf_matrix, 'enhanced_tfidf_matrix_antique.joblib')\n",
        "print(\"✓ Enhanced TF-IDF matrix saved\")\n",
        "\n",
        "# Save optimized inverted index\n",
        "with open('enhanced_inverted_index_antique.pkl', 'wb') as f:\n",
        "    pickle.dump(enhanced_inverted_index, f)\n",
        "print(\"✓ Optimized inverted index saved\")\n",
        "\n",
        "# Save enhanced document mappings\n",
        "with open('enhanced_doc_mappings_antique.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'doc_id_to_idx': enhanced_doc_id_to_idx,\n",
        "        'idx_to_doc_id': enhanced_idx_to_doc_id\n",
        "    }, f)\n",
        "print(\"✓ Enhanced document mappings saved\")\n",
        "\n",
        "# Save enhanced document metadata\n",
        "with open('enhanced_document_metadata_antique.json', 'w') as f:\n",
        "    json.dump(enhanced_document_metadata, f)\n",
        "print(\"✓ Enhanced document metadata saved\")\n",
        "\n",
        "# Save enhanced training statistics\n",
        "with open('enhanced_training_statistics_antique.json', 'w') as f:\n",
        "    # Convert numpy types for JSON serialization\n",
        "    serializable_stats = {}\n",
        "    for key, value in enhanced_training_stats.items():\n",
        "        if isinstance(value, (np.int64, np.int32)):\n",
        "            serializable_stats[key] = int(value)\n",
        "        elif isinstance(value, (np.float64, np.float32)):\n",
        "            serializable_stats[key] = float(value)\n",
        "        elif isinstance(value, tuple):\n",
        "            serializable_stats[key] = list(value)\n",
        "        else:\n",
        "            serializable_stats[key] = value\n",
        "    \n",
        "    json.dump(serializable_stats, f, indent=2)\n",
        "print(\"✓ Enhanced training statistics saved\")\n",
        "\n",
        "# Save text cleaning methods and configurations\n",
        "cleaning_config = {\n",
        "    'spell_check_cache': dict(text_cleaner.spell_check_cache),\n",
        "    'tokenizer_spell_check_cache': dict(enhanced_tokenizer.spell_check_cache),\n",
        "    'preprocessing_summary': preprocessing_summary,\n",
        "    'text_cleaner_config': {\n",
        "        'enable_spell_check': text_cleaner.enable_spell_check,\n",
        "        'language': text_cleaner.language,\n",
        "        'total_stopwords': len(text_cleaner.all_stopwords),\n",
        "        'technical_stopwords_count': len(text_cleaner.technical_stopwords),\n",
        "        'domain_stopwords_count': len(text_cleaner.domain_stopwords)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('enhanced_text_cleaning_config_antique.json', 'w') as f:\n",
        "    json.dump(cleaning_config, f, indent=2)\n",
        "print(\"✓ Enhanced text cleaning configuration saved\")\n",
        "\n",
        "print(\"\\n=== Enhanced Training Complete ===\")\n",
        "print(f\"Documents processed: {enhanced_training_stats['valid_documents']}\")\n",
        "print(f\"Vocabulary size: {enhanced_training_stats['vocabulary_size']}\")\n",
        "print(f\"Matrix sparsity: {enhanced_training_stats['sparsity_percent']:.2f}%\")\n",
        "print(f\"Inverted index terms: {enhanced_training_stats['inverted_index_terms']}\")\n",
        "print(f\"Spell corrections applied: {enhanced_training_stats['spell_corrections_count']}\")\n",
        "print(f\"Text reduction: {enhanced_training_stats['total_char_reduction_percent']:.2f}%\")\n",
        "print(\"\\nOptimizations enabled:\")\n",
        "print(\"✓ Spell checking (conservative)\")\n",
        "print(\"✓ Lemmatization with POS tagging\")\n",
        "print(\"✓ Stemming for vocabulary reduction\")\n",
        "print(\"✓ Enhanced stopword filtering\")\n",
        "print(\"✓ Optimized inverted index with term statistics\")\n",
        "print(\"✓ MAP-preserving preprocessing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_enhanced_search"
      },
      "outputs": [],
      "source": [
        "# Test enhanced search functionality\n",
        "print(\"\\n=== Testing Enhanced Search ===\")\n",
        "\n",
        "def test_enhanced_search(query, top_k=5):\n",
        "    \"\"\"Test enhanced TF-IDF search with inverted index.\"\"\"\n",
        "    # Clean query using the same process\n",
        "    processed_query = text_cleaner.clean_text_basic(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Processed: {processed_query}\")\n",
        "    \n",
        "    if not processed_query.strip():\n",
        "        print(\"Empty query after preprocessing\")\n",
        "        return\n",
        "    \n",
        "    # Transform query to TF-IDF\n",
        "    query_vector = enhanced_tfidf_vectorizer.transform([processed_query])\n",
        "    \n",
        "    # Test 1: Full matrix search\n",
        "    similarities = cosine_similarity(query_vector, enhanced_tfidf_matrix).flatten()\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    \n",
        "    print(f\"\\nTop {top_k} results (Full Matrix Search):\")\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        doc_id = valid_doc_ids[idx]\n",
        "        score = similarities[idx]\n",
        "        doc_text = valid_doc_texts[idx][:150] + \"...\"\n",
        "        print(f\"{i+1}. Doc {doc_id} (Score: {score:.4f})\")\n",
        "        print(f\"   {doc_text}\\n\")\n",
        "    \n",
        "    # Test 2: Inverted index search simulation\n",
        "    query_terms = enhanced_tokenizer.tokenize(processed_query)\n",
        "    print(f\"Query terms after tokenization: {query_terms}\")\n",
        "    \n",
        "    # Get candidate documents from inverted index\n",
        "    candidate_docs = set()\n",
        "    term_doc_scores = defaultdict(float)\n",
        "    \n",
        "    for term in query_terms:\n",
        "        if term in enhanced_inverted_index:\n",
        "            term_data = enhanced_inverted_index[term]\n",
        "            print(f\"Term '{term}': {term_data['df']} documents, max_tfidf: {term_data['max_tfidf']:.4f}\")\n",
        "            \n",
        "            for doc_id, tfidf_score in term_data['postings'][:10]:  # Top 10 for this term\n",
        "                candidate_docs.add(doc_id)\n",
        "                term_doc_scores[doc_id] += tfidf_score\n",
        "    \n",
        "    print(f\"Found {len(candidate_docs)} candidate documents from inverted index\")\n",
        "\n",
        "# Test with sample queries\n",
        "test_queries = [\n",
        "    \"antique furniture restoration techniques\",\n",
        "    \"vintage jewelry appraisal value\",\n",
        "    \"old coins historical significance\"\n",
        "]\n",
        "\n",
        "for test_query in test_queries:\n",
        "    test_enhanced_search(test_query, top_k=3)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_enhanced_files"
      },
      "outputs": [],
      "source": [
        "# Download all enhanced trained files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading enhanced trained model files...\")\n",
        "\n",
        "try:\n",
        "    files.download('enhanced_tfidf_vectorizer_antique.joblib')\n",
        "    files.download('enhanced_tfidf_matrix_antique.joblib')\n",
        "    files.download('enhanced_inverted_index_antique.pkl')\n",
        "    files.download('enhanced_doc_mappings_antique.json')\n",
        "    files.download('enhanced_document_metadata_antique.json')\n",
        "    files.download('enhanced_training_statistics_antique.json')\n",
        "    files.download('enhanced_text_cleaning_config_antique.json')\n",
        "    print(\"✓ All enhanced files downloaded successfully!\")\n",
        "    \n",
        "    print(\"\\n=== Files Generated ===\")\n",
        "    print(\"1. enhanced_tfidf_vectorizer_antique.joblib - Enhanced TF-IDF vectorizer\")\n",
        "    print(\"2. enhanced_tfidf_matrix_antique.joblib - TF-IDF matrix with enhanced features\")\n",
        "    print(\"3. enhanced_inverted_index_antique.pkl - Optimized inverted index with statistics\")\n",
        "    print(\"4. enhanced_doc_mappings_antique.json - Document ID mappings\")\n",
        "    print(\"5. enhanced_document_metadata_antique.json - Document metadata with processing stats\")\n",
        "    print(\"6. enhanced_training_statistics_antique.json - Comprehensive training statistics\")\n",
        "    print(\"7. enhanced_text_cleaning_config_antique.json - Text cleaning configuration and caches\")\n",
        "    \n",
        "    print(\"\\n=== Usage Instructions ===\")\n",
        "    print(\"1. Upload these files to your backend/models/ directory\")\n",
        "    print(\"2. Use the Enhanced TF-IDF Service to load the pre-trained models\")\n",
        "    print(\"3. The service will automatically use the optimized inverted index\")\n",
        "    print(\"4. Text cleaning configuration includes spell-check caches for consistency\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Download error: {e}\")\n",
        "    print(\"Files are saved in Colab session and can be downloaded manually.\")\n",
        "    print(\"\\nAvailable files:\")\n",
        "    import os\n",
        "    for file in os.listdir('.'):\n",
        "        if 'enhanced' in file and (file.endswith('.joblib') or file.endswith('.pkl') or file.endswith('.json')):\n",
        "            print(f\"  - {file}\")"
      ]
    }
  ]
}
