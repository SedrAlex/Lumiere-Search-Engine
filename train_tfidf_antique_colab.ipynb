{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# TF-IDF Training on ANTIQUE Dataset\n",
        "## 3-Day Implementation - Day 1: TF-IDF Model Training\n",
        "\n",
        "This notebook trains TF-IDF models on ANTIQUE training data and saves them for use in the IR system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ir-datasets nltk scikit-learn joblib pandas numpy tqdm textblob\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import ir_datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import html\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_preprocessing_classes"
      },
      "outputs": [],
      "source": [
        "# Enhanced Text Cleaning Service\n",
        "import unicodedata\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class EnhancedTextCleaner:\n",
        "    def __init__(self, enable_spell_check=True):\n",
        "        self.enable_spell_check = enable_spell_check\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        \n",
        "        # Enhanced stopwords\n",
        "        self.technical_stopwords = {\n",
        "            'code', 'function', 'method', 'class', 'variable', 'return',\n",
        "            'import', 'from', 'def', 'if', 'else', 'for', 'while', 'try'\n",
        "        }\n",
        "        \n",
        "        self.domain_stopwords = {\n",
        "            'antique', 'vintage', 'old', 'item', 'piece', 'thing', 'stuff',\n",
        "            'want', 'need', 'looking', 'find', 'search', 'help', 'please'\n",
        "        }\n",
        "        \n",
        "        self.all_stopwords = self.stop_words.union(\n",
        "            self.technical_stopwords\n",
        "        ).union(self.domain_stopwords)\n",
        "        \n",
        "        # Spell check cache\n",
        "        self.spell_check_cache = {}\n",
        "        \n",
        "        # Contractions\n",
        "        self.contractions = {\n",
        "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "            \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\"\n",
        "        }\n",
        "    \n",
        "    def get_wordnet_pos(self, word):\n",
        "        tag = pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "    \n",
        "    def expand_contractions(self, text):\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = re.sub(re.escape(contraction), expansion, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "    \n",
        "    def normalize_unicode(self, text):\n",
        "        text = unicodedata.normalize('NFD', text)\n",
        "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
        "        return text.encode('ascii', 'ignore').decode('ascii')\n",
        "    \n",
        "    def spell_check_word(self, word):\n",
        "        if not self.enable_spell_check or len(word) < 4:\n",
        "            return word\n",
        "        \n",
        "        if word in self.spell_check_cache:\n",
        "            return self.spell_check_cache[word]\n",
        "        \n",
        "        try:\n",
        "            blob = TextBlob(word)\n",
        "            corrected = str(blob.correct())\n",
        "            \n",
        "            if (corrected != word and word not in self.stop_words and\n",
        "                abs(len(corrected) - len(word)) <= 2 and\n",
        "                len(set(corrected.lower()) & set(word.lower())) >= min(len(word), len(corrected)) * 0.6):\n",
        "                self.spell_check_cache[word] = corrected\n",
        "                return corrected\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.spell_check_cache[word] = word\n",
        "        return word\n",
        "    \n",
        "    def clean_text_basic(self, text):\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "        \n",
        "        # HTML decoding and tag removal\n",
        "        text = html.unescape(text)\n",
        "        text = re.sub(r'<[^>]+>', ' ', text)\n",
        "        \n",
        "        # Normalize Unicode\n",
        "        text = self.normalize_unicode(text)\n",
        "        \n",
        "        # URLs and emails\n",
        "        text = re.sub(r'https?://[^\\s<>\"]{2,}', ' URL ', text)\n",
        "        text = re.sub(r'\\S+@\\S+', ' EMAIL ', text)\n",
        "        \n",
        "        # Expand contractions\n",
        "        text = self.expand_contractions(text)\n",
        "        \n",
        "        # Remove extra punctuation but preserve sentence boundaries\n",
        "        text = re.sub(r'[^\\w\\s\\.\\!\\?]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def preprocess_for_tfidf(self, text):\n",
        "        # Basic cleaning\n",
        "        text = self.clean_text_basic(text)\n",
        "        \n",
        "        if not text.strip():\n",
        "            return \"\"\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "        \n",
        "        # Process tokens\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if len(token) <= 2 or not token.isalpha() or token in self.all_stopwords:\n",
        "                continue\n",
        "            \n",
        "            # Spell checking\n",
        "            if self.enable_spell_check:\n",
        "                token = self.spell_check_word(token)\n",
        "            \n",
        "            # Lemmatization with POS\n",
        "            pos = self.get_wordnet_pos(token)\n",
        "            lemmatized = self.lemmatizer.lemmatize(token, pos)\n",
        "            \n",
        "            # Stemming\n",
        "            stemmed = self.stemmer.stem(lemmatized)\n",
        "            \n",
        "            if len(stemmed) > 2:\n",
        "                processed_tokens.append(stemmed)\n",
        "        \n",
        "        return ' '.join(processed_tokens)\n",
        "    \n",
        "    def batch_preprocess(self, texts):\n",
        "        return [self.preprocess_for_tfidf(text) for text in tqdm(texts, desc=\"Enhanced Preprocessing\")]\n",
        "\n",
        "# Enhanced Tokenizer for TF-IDF Vectorizer\n",
        "class EnhancedTokenizer:\n",
        "    def __init__(self, enable_spell_check=True, enable_lemmatization=True, enable_stemming=True):\n",
        "        self.enable_spell_check = enable_spell_check\n",
        "        self.enable_lemmatization = enable_lemmatization\n",
        "        self.enable_stemming = enable_stemming\n",
        "        \n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        \n",
        "        # Enhanced stopwords\n",
        "        self.technical_stopwords = {\n",
        "            'code', 'function', 'method', 'class', 'variable', 'return',\n",
        "            'import', 'from', 'def', 'if', 'else', 'for', 'while', 'try'\n",
        "        }\n",
        "        \n",
        "        self.domain_stopwords = {\n",
        "            'antique', 'vintage', 'old', 'item', 'piece', 'thing', 'stuff',\n",
        "            'want', 'need', 'looking', 'find', 'search', 'help', 'please'\n",
        "        }\n",
        "        \n",
        "        self.all_stopwords = self.stop_words.union(\n",
        "            self.technical_stopwords\n",
        "        ).union(self.domain_stopwords)\n",
        "        \n",
        "        self.spell_check_cache = {}\n",
        "    \n",
        "    def get_wordnet_pos(self, word):\n",
        "        try:\n",
        "            tag = pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "        except:\n",
        "            return wordnet.NOUN\n",
        "    \n",
        "    def spell_check_token(self, token):\n",
        "        if not self.enable_spell_check or len(token) < 4:\n",
        "            return token\n",
        "        \n",
        "        if token in self.spell_check_cache:\n",
        "            return self.spell_check_cache[token]\n",
        "        \n",
        "        try:\n",
        "            blob = TextBlob(token)\n",
        "            corrected = str(blob.correct())\n",
        "            \n",
        "            if (corrected != token and token not in self.stop_words and\n",
        "                abs(len(corrected) - len(token)) <= 2 and\n",
        "                len(set(corrected.lower()) & set(token.lower())) >= min(len(token), len(corrected)) * 0.6):\n",
        "                self.spell_check_cache[token] = corrected\n",
        "                return corrected\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.spell_check_cache[token] = token\n",
        "        return token\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        if not text or not isinstance(text, str):\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except:\n",
        "            tokens = text.split()\n",
        "        \n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if (len(token) < 3 or len(token) > 50 or\n",
        "                not token.isalpha() or\n",
        "                token.lower() in self.all_stopwords):\n",
        "                continue\n",
        "            \n",
        "            token = token.lower()\n",
        "            \n",
        "            # Spell checking\n",
        "            if self.enable_spell_check:\n",
        "                token = self.spell_check_token(token)\n",
        "            \n",
        "            # Lemmatization\n",
        "            if self.enable_lemmatization:\n",
        "                pos = self.get_wordnet_pos(token)\n",
        "                token = self.lemmatizer.lemmatize(token, pos)\n",
        "            \n",
        "            # Stemming\n",
        "            if self.enable_stemming:\n",
        "                token = self.stemmer.stem(token)\n",
        "            \n",
        "            if len(token) >= 3:\n",
        "                processed_tokens.append(token)\n",
        "        \n",
        "        return processed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_antique_data"
      },
      "outputs": [],
      "source": [
        "# Load ANTIQUE dataset\n",
        "print(\"Loading ANTIQUE dataset...\")\n",
        "dataset = ir_datasets.load(\"antique\")\n",
        "\n",
        "# Load documents\n",
        "print(\"Loading documents...\")\n",
        "docs = {}\n",
        "doc_texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for doc in tqdm(dataset.docs_iter(), desc=\"Loading documents\"):\n",
        "    docs[doc.doc_id] = doc.text\n",
        "    doc_texts.append(doc.text)\n",
        "    doc_ids.append(doc.doc_id)\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "\n",
        "# Load training queries\n",
        "print(\"Loading queries...\")\n",
        "train_queries = {}\n",
        "for query in dataset.queries_iter():\n",
        "    train_queries[query.query_id] = query.text\n",
        "\n",
        "print(f\"Loaded {len(train_queries)} queries\")\n",
        "\n",
        "# Load qrels for evaluation\n",
        "print(\"Loading qrels...\")\n",
        "train_qrels = defaultdict(dict)\n",
        "for qrel in dataset.qrels_iter():\n",
        "    train_qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n",
        "\n",
        "print(f\"Loaded qrels for {len(train_qrels)} queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_documents"
      },
      "outputs": [],
      "source": [
        "# Initialize enhanced preprocessor\n",
        "preprocessor = EnhancedTextCleaner(enable_spell_check=True)\n",
        "enhanced_tokenizer = EnhancedTokenizer(\n",
        "    enable_spell_check=True,\n",
        "    enable_lemmatization=True,\n",
        "    enable_stemming=True\n",
        ")\n",
        "\n",
        "# Preprocess all documents with enhanced cleaning\n",
        "print(\"Preprocessing documents with enhanced text processing...\")\n",
        "processed_doc_texts = []\n",
        "for text in tqdm(doc_texts, desc=\"Enhanced preprocessing\"):\n",
        "    cleaned = preprocessor.clean_text_basic(text)\n",
        "    if cleaned.strip():\n",
        "        processed_doc_texts.append(cleaned)\n",
        "    else:\n",
        "        processed_doc_texts.append(\"\")\n",
        "\n",
        "# Filter out empty documents\n",
        "valid_docs = [(doc_id, doc_text, processed_text) \n",
        "              for doc_id, doc_text, processed_text \n",
        "              in zip(doc_ids, doc_texts, processed_doc_texts) \n",
        "              if processed_text.strip()]\n",
        "\n",
        "print(f\"Valid documents after preprocessing: {len(valid_docs)}\")\n",
        "\n",
        "# Separate the valid data\n",
        "valid_doc_ids = [item[0] for item in valid_docs]\n",
        "valid_doc_texts = [item[1] for item in valid_docs]\n",
        "valid_processed_texts = [item[2] for item in valid_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_enhanced_tfidf"
      },
      "outputs": [],
      "source": [
        "# Import Enhanced TF-IDF Service V2\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "# Enhanced TF-IDF Service V2 Implementation\n",
        "class EnhancedTFIDFServiceV2:\n",
        "    def __init__(self, enable_spell_check=True, enable_lemmatization=True, enable_stemming=True):\n",
        "        self.text_cleaner = EnhancedTextCleaner(enable_spell_check=enable_spell_check)\n",
        "        self.enhanced_tokenizer = EnhancedTokenizer(\n",
        "            enable_spell_check=enable_spell_check,\n",
        "            enable_lemmatization=enable_lemmatization,\n",
        "            enable_stemming=enable_stemming\n",
        "        )\n",
        "        self.vectorizer = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.inverted_index = None\n",
        "        self.doc_id_to_idx = None\n",
        "        self.idx_to_doc_id = None\n",
        "        self.document_metadata = None\n",
        "        self.training_stats = None\n",
        "        self.is_trained = False\n",
        "    \n",
        "    def create_optimized_vectorizer(self, **params):\n",
        "        default_params = {\n",
        "            'max_features': 100000,    # Large vocabulary for better coverage\n",
        "            'min_df': 2,               # Remove very rare terms\n",
        "            'max_df': 0.85,            # Remove very common terms\n",
        "            'ngram_range': (1, 3),     # Include trigrams for better matching\n",
        "            'sublinear_tf': True,      # Log scaling for TF\n",
        "            'norm': 'l2',              # L2 normalization\n",
        "            'smooth_idf': True,        # Smooth IDF weights\n",
        "            'use_idf': True,           # Use IDF weighting\n",
        "            'tokenizer': self.enhanced_tokenizer,\n",
        "            'preprocessor': None,      # All preprocessing handled by tokenizer\n",
        "            'lowercase': False,        # Handled by tokenizer\n",
        "            'stop_words': None,        # Handled by tokenizer\n",
        "            'token_pattern': None,     # Using custom tokenizer\n",
        "        }\n",
        "        default_params.update(params)\n",
        "        return TfidfVectorizer(**default_params)\n",
        "    \n",
        "    def train_enhanced_tfidf(self, documents, doc_ids, vectorizer_params=None, build_inverted_index=True):\n",
        "        print(f\"Training Enhanced TF-IDF V2 on {len(documents)} documents...\")\n",
        "        \n",
        "        # Step 1: Clean text using enhanced cleaner\n",
        "        print(\"Step 1: Enhanced text cleaning...\")\n",
        "        valid_docs = []\n",
        "        for doc_id, doc_text in zip(doc_ids, documents):\n",
        "            cleaned_text = self.text_cleaner.clean_text_basic(doc_text)\n",
        "            if cleaned_text.strip():\n",
        "                valid_docs.append((doc_id, doc_text, cleaned_text))\n",
        "        \n",
        "        print(f\"Valid documents after cleaning: {len(valid_docs)}\")\n",
        "        \n",
        "        # Step 2: Create enhanced vectorizer\n",
        "        vectorizer_params = vectorizer_params or {}\n",
        "        self.vectorizer = self.create_optimized_vectorizer(**vectorizer_params)\n",
        "        \n",
        "        # Step 3: Fit and transform with enhanced tokenization\n",
        "        print(\"Step 2: Fitting TF-IDF with enhanced tokenization...\")\n",
        "        training_texts = [item[2] for item in valid_docs]\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(training_texts)\n",
        "        \n",
        "        # Step 4: Create mappings\n",
        "        valid_doc_ids = [item[0] for item in valid_docs]\n",
        "        self.doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(valid_doc_ids)}\n",
        "        self.idx_to_doc_id = {idx: doc_id for doc_id, idx in self.doc_id_to_idx.items()}\n",
        "        \n",
        "        # Step 5: Create metadata\n",
        "        self.document_metadata = {\n",
        "            doc_id: {\n",
        "                'original_text': item[1],\n",
        "                'cleaned_text': item[2],\n",
        "                'index': idx\n",
        "            }\n",
        "            for doc_id, idx in self.doc_id_to_idx.items()\n",
        "            for item in valid_docs if item[0] == doc_id\n",
        "        }\n",
        "        \n",
        "        # Step 6: Build inverted index\n",
        "        if build_inverted_index:\n",
        "            print(\"Step 3: Building optimized inverted index...\")\n",
        "            self.inverted_index = self.build_optimized_inverted_index()\n",
        "        \n",
        "        # Step 7: Calculate statistics\n",
        "        self.training_stats = {\n",
        "            'total_documents': len(documents),\n",
        "            'valid_documents': len(valid_docs),\n",
        "            'vocabulary_size': len(self.vectorizer.vocabulary_),\n",
        "            'matrix_shape': self.tfidf_matrix.shape,\n",
        "            'non_zero_entries': int(self.tfidf_matrix.nnz),\n",
        "            'sparsity': float((1 - self.tfidf_matrix.nnz / (self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1])) * 100),\n",
        "            'inverted_index_terms': len(self.inverted_index) if self.inverted_index else 0,\n",
        "            'spell_check_enabled': self.enhanced_tokenizer.enable_spell_check,\n",
        "            'lemmatization_enabled': self.enhanced_tokenizer.enable_lemmatization,\n",
        "            'stemming_enabled': self.enhanced_tokenizer.enable_stemming\n",
        "        }\n",
        "        \n",
        "        self.is_trained = True\n",
        "        print(\"✓ Enhanced TF-IDF V2 training completed!\")\n",
        "        return self.training_stats\n",
        "    \n",
        "    def build_optimized_inverted_index(self):\n",
        "        inverted_index = defaultdict(lambda: {'postings': [], 'df': 0, 'max_tfidf': 0.0, 'avg_tfidf': 0.0})\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "        coo_matrix = self.tfidf_matrix.tocoo()\n",
        "        term_stats = defaultdict(list)\n",
        "        \n",
        "        for doc_idx, term_idx, tfidf_score in zip(coo_matrix.row, coo_matrix.col, coo_matrix.data):\n",
        "            term = feature_names[term_idx]\n",
        "            doc_id = self.idx_to_doc_id[doc_idx]\n",
        "            inverted_index[term]['postings'].append((doc_id, float(tfidf_score)))\n",
        "            term_stats[term].append(float(tfidf_score))\n",
        "        \n",
        "        for term in inverted_index:\n",
        "            scores = term_stats[term]\n",
        "            inverted_index[term]['df'] = len(scores)\n",
        "            inverted_index[term]['max_tfidf'] = max(scores)\n",
        "            inverted_index[term]['avg_tfidf'] = sum(scores) / len(scores)\n",
        "            inverted_index[term]['postings'].sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return dict(inverted_index)\n",
        "\n",
        "# Create and train enhanced TF-IDF service\n",
        "enhanced_tfidf_service = EnhancedTFIDFServiceV2(\n",
        "    enable_spell_check=True,\n",
        "    enable_lemmatization=True,\n",
        "    enable_stemming=True\n",
        ")\n",
        "\n",
        "# Train with enhanced processing\n",
        "training_stats = enhanced_tfidf_service.train_enhanced_tfidf(\n",
        "    documents=valid_doc_texts,\n",
        "    doc_ids=valid_doc_ids,\n",
        "    build_inverted_index=True\n",
        ")\n",
        "\n",
        "print(\"\\n=== Enhanced Training Statistics ===\")\n",
        "for key, value in training_stats.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Extract components for backward compatibility\n",
        "tfidf_vectorizer = enhanced_tfidf_service.vectorizer\n",
        "tfidf_matrix = enhanced_tfidf_service.tfidf_matrix\n",
        "inverted_index = enhanced_tfidf_service.inverted_index\n",
        "doc_id_to_idx = enhanced_tfidf_service.doc_id_to_idx\n",
        "idx_to_doc_id = enhanced_tfidf_service.idx_to_doc_id\n",
        "document_metadata = enhanced_tfidf_service.document_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_inverted_index"
      },
      "outputs": [],
      "source": [
        "# Create inverted index for efficient retrieval\n",
        "print(\"Building inverted index...\")\n",
        "\n",
        "def build_inverted_index(tfidf_matrix, vectorizer, doc_ids):\n",
        "    \"\"\"Build inverted index from TF-IDF matrix.\"\"\"\n",
        "    inverted_index = defaultdict(list)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Convert to COO format for efficient iteration\n",
        "    coo_matrix = tfidf_matrix.tocoo()\n",
        "    \n",
        "    # Build index: term -> [(doc_id, tfidf_score), ...]\n",
        "    for doc_idx, term_idx, tfidf_score in tqdm(\n",
        "        zip(coo_matrix.row, coo_matrix.col, coo_matrix.data),\n",
        "        total=coo_matrix.nnz,\n",
        "        desc=\"Building inverted index\"\n",
        "    ):\n",
        "        term = feature_names[term_idx]\n",
        "        doc_id = doc_ids[doc_idx]\n",
        "        inverted_index[term].append((doc_id, float(tfidf_score)))\n",
        "    \n",
        "    # Sort each posting list by TF-IDF score (descending)\n",
        "    for term in inverted_index:\n",
        "        inverted_index[term].sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return dict(inverted_index)\n",
        "\n",
        "inverted_index = build_inverted_index(tfidf_matrix, tfidf_vectorizer, valid_doc_ids)\n",
        "print(f\"Inverted index built with {len(inverted_index)} terms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_document_mapping"
      },
      "outputs": [],
      "source": [
        "# Create document ID to index mapping\n",
        "doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(valid_doc_ids)}\n",
        "idx_to_doc_id = {idx: doc_id for doc_id, idx in doc_id_to_idx.items()}\n",
        "\n",
        "# Create document metadata\n",
        "document_metadata = {\n",
        "    doc_id: {\n",
        "        'original_text': valid_doc_texts[idx],\n",
        "        'processed_text': valid_processed_texts[idx],\n",
        "        'index': idx\n",
        "    }\n",
        "    for doc_id, idx in doc_id_to_idx.items()\n",
        "}\n",
        "\n",
        "print(f\"Created metadata for {len(document_metadata)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_models"
      },
      "outputs": [],
      "source": [
        "# Save all trained models and data\n",
        "print(\"Saving trained models and data...\")\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_antique.joblib')\n",
        "print(\"✓ TF-IDF vectorizer saved\")\n",
        "\n",
        "# Save TF-IDF matrix (sparse)\n",
        "joblib.dump(tfidf_matrix, 'tfidf_matrix_antique.joblib')\n",
        "print(\"✓ TF-IDF matrix saved\")\n",
        "\n",
        "# Save inverted index\n",
        "with open('inverted_index_antique.pkl', 'wb') as f:\n",
        "    pickle.dump(inverted_index, f)\n",
        "print(\"✓ Inverted index saved\")\n",
        "\n",
        "# Save document mappings\n",
        "with open('doc_mappings_antique.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'doc_id_to_idx': doc_id_to_idx,\n",
        "        'idx_to_doc_id': idx_to_doc_id\n",
        "    }, f)\n",
        "print(\"✓ Document mappings saved\")\n",
        "\n",
        "# Save document metadata\n",
        "with open('document_metadata_antique.json', 'w') as f:\n",
        "    json.dump(document_metadata, f)\n",
        "print(\"✓ Document metadata saved\")\n",
        "\n",
        "# Save preprocessing statistics\n",
        "stats = {\n",
        "    'total_documents': len(doc_texts),\n",
        "    'valid_documents': len(valid_docs),\n",
        "    'vocabulary_size': len(tfidf_vectorizer.vocabulary_),\n",
        "    'matrix_shape': tfidf_matrix.shape,\n",
        "    'non_zero_entries': tfidf_matrix.nnz,\n",
        "    'sparsity': (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100,\n",
        "    'inverted_index_terms': len(inverted_index)\n",
        "}\n",
        "\n",
        "with open('training_statistics_antique.json', 'w') as f:\n",
        "    json.dump(stats, f, indent=2)\n",
        "print(\"✓ Training statistics saved\")\n",
        "\n",
        "print(\"\\n=== Training Complete ===\")\n",
        "print(f\"Documents processed: {stats['valid_documents']}\")\n",
        "print(f\"Vocabulary size: {stats['vocabulary_size']}\")\n",
        "print(f\"Matrix sparsity: {stats['sparsity']:.2f}%\")\n",
        "print(f\"Inverted index terms: {stats['inverted_index_terms']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quick_test"
      },
      "outputs": [],
      "source": [
        "# Quick test of the trained model\n",
        "print(\"\\n=== Quick Test ===\")\n",
        "\n",
        "def test_tfidf_search(query, top_k=5):\n",
        "    \"\"\"Test Enhanced TF-IDF search functionality.\"\"\"\n",
        "    # Preprocess query with enhanced cleaning\n",
        "    processed_query = preprocessor.clean_text_basic(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Processed: {processed_query}\")\n",
        "    \n",
        "    if not processed_query.strip():\n",
        "        print(\"Empty query after preprocessing\")\n",
        "        return\n",
        "    \n",
        "    # Transform query to TF-IDF\n",
        "    query_vector = tfidf_vectorizer.transform([processed_query])\n",
        "    \n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    \n",
        "    # Get top results\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    \n",
        "    print(f\"\\nTop {top_k} results:\")\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        doc_id = valid_doc_ids[idx]\n",
        "        score = similarities[idx]\n",
        "        doc_text = valid_doc_texts[idx][:150] + \"...\"\n",
        "        print(f\"{i+1}. Doc {doc_id} (Score: {score:.4f})\")\n",
        "        print(f\"   {doc_text}\\n\")\n",
        "\n",
        "# Test with sample queries\n",
        "test_queries = [\n",
        "    \"antique furniture restoration\",\n",
        "    \"vintage jewelry appraisal\",\n",
        "    \"old coins value\"\n",
        "]\n",
        "\n",
        "for test_query in test_queries:\n",
        "    test_tfidf_search(test_query, top_k=3)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_files"
      },
      "outputs": [],
      "source": [
        "# Download trained files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading trained model files...\")\n",
        "\n",
        "try:\n",
        "    files.download('tfidf_vectorizer_antique.joblib')\n",
        "    files.download('tfidf_matrix_antique.joblib')\n",
        "    files.download('inverted_index_antique.pkl')\n",
        "    files.download('doc_mappings_antique.json')\n",
        "    files.download('document_metadata_antique.json')\n",
        "    files.download('training_statistics_antique.json')\n",
        "    print(\"✓ All files downloaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Download error: {e}\")\n",
        "    print(\"Files are saved in Colab session and can be downloaded manually.\")"
      ]
    }
  ]
}
